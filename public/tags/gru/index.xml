<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GRU on My Favorite</title><link>https://wglog.net/tags/gru/</link><description>Recent content in GRU on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 18 Mar 2021 17:24:49 +0800</lastBuildDate><atom:link href="https://wglog.net/tags/gru/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding GRU Networks</title><link>https://wglog.net/archives/understanding-gru-networks/</link><pubDate>Thu, 18 Mar 2021 17:24:49 +0800</pubDate><guid>https://wglog.net/archives/understanding-gru-networks/</guid><description>In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network.</description></item><item><title>Understanding LSTM Networks</title><link>https://wglog.net/archives/understanding-lstm-networks/</link><pubDate>Mon, 08 Mar 2021 13:02:42 +0800</pubDate><guid>https://wglog.net/archives/understanding-lstm-networks/</guid><description>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp;amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</description></item><item><title>Illustrated Guide to LSTM's and GRU's: A step by step Explanation</title><link>https://wglog.net/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</link><pubDate>Sat, 06 Mar 2021 22:17:22 +0800</pubDate><guid>https://wglog.net/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</guid><description>In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.</description></item></channel></rss>