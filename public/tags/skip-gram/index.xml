<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>skip-gram on My Favorite</title><link>https://wglog.net/tags/skip-gram/</link><description>Recent content in skip-gram on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 16 Mar 2021 12:36:21 +0800</lastBuildDate><atom:link href="https://wglog.net/tags/skip-gram/index.xml" rel="self" type="application/rss+xml"/><item><title>An Introduction of Word2Vec and implementation using Tensorflow</title><link>https://wglog.net/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/</link><pubDate>Tue, 16 Mar 2021 12:36:21 +0800</pubDate><guid>https://wglog.net/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/</guid><description>Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.</description></item></channel></rss>