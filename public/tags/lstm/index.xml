<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LSTM on My Favorite</title><link>https://wglog.net/tags/lstm/</link><description>Recent content in LSTM on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 21 Apr 2021 13:09:01 +0800</lastBuildDate><atom:link href="https://wglog.net/tags/lstm/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring Spatiotemporal Features for Activity Classifications in Films</title><link>https://wglog.net/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/</link><pubDate>Wed, 21 Apr 2021 13:09:01 +0800</pubDate><guid>https://wglog.net/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/</guid><description>The post introduces several experiments about activity classification based on three main architectures: 3D CNN, ConvLSTM2D, and a pipeline of pre-trained CNN-LSTM.</description></item><item><title>Video Classification in Keras using ConvLSTM</title><link>https://wglog.net/archives/video-classification-keras-convlstm/</link><pubDate>Tue, 20 Apr 2021 19:37:11 +0800</pubDate><guid>https://wglog.net/archives/video-classification-keras-convlstm/</guid><description>This article will explain the Deep Learning based solution of the Video Classification task in Keras using ConvLSTM layers.</description></item><item><title>Implementing a LSTM From Scratch With Numpy</title><link>https://wglog.net/archives/implementing-a-lstm-from-scratch-with-numpy/</link><pubDate>Mon, 29 Mar 2021 15:25:04 +0800</pubDate><guid>https://wglog.net/archives/implementing-a-lstm-from-scratch-with-numpy/</guid><description>In this post, The author implement a simple character-level LSTM using Numpy. It is trained in batches with the Adam optimiser and learns basic words after just a few training iterations.</description></item><item><title>Deriving the Backpropagation Equations for a LSTM</title><link>https://wglog.net/archives/deriving-the-backpropagation-equations-for-a-lstm/</link><pubDate>Mon, 29 Mar 2021 15:22:46 +0800</pubDate><guid>https://wglog.net/archives/deriving-the-backpropagation-equations-for-a-lstm/</guid><description>In this post, The author derive the backpropagation equations for a LSTM cell in vectorised form.</description></item><item><title>A Simple Overview of RNN, LSTM and Attention Mechanism</title><link>https://wglog.net/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</link><pubDate>Mon, 22 Mar 2021 14:57:23 +0800</pubDate><guid>https://wglog.net/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</guid><description>Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained.</description></item><item><title>Understanding Input and Output Shapes in LSTM</title><link>https://wglog.net/archives/understanding-input-and-output-shapes-in-lstm/</link><pubDate>Wed, 17 Mar 2021 19:25:24 +0800</pubDate><guid>https://wglog.net/archives/understanding-input-and-output-shapes-in-lstm/</guid><description>Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM.</description></item><item><title>Understanding the Number of Parameter Used in LSTM Network</title><link>https://wglog.net/archives/understanding-the-number-of-parameter-used-in-lstm-network/</link><pubDate>Tue, 16 Mar 2021 17:57:54 +0800</pubDate><guid>https://wglog.net/archives/understanding-the-number-of-parameter-used-in-lstm-network/</guid><description>It explained clearly how to calculate the number of trainable parameters used in the LSTM network.</description></item><item><title>[Kaggle Summary] Quora Insincere Questions Classification</title><link>https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/</link><pubDate>Fri, 12 Mar 2021 17:13:34 +0800</pubDate><guid>https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/</guid><description>The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.</description></item><item><title>How do LSTM Networks Solve the Problem of Vanishing Gradients</title><link>https://wglog.net/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</link><pubDate>Mon, 08 Mar 2021 13:35:16 +0800</pubDate><guid>https://wglog.net/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</guid><description>LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate’s activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.</description></item><item><title>Understanding LSTM Networks</title><link>https://wglog.net/archives/understanding-lstm-networks/</link><pubDate>Mon, 08 Mar 2021 13:02:42 +0800</pubDate><guid>https://wglog.net/archives/understanding-lstm-networks/</guid><description>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp;amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</description></item><item><title>Illustrated Guide to LSTM's and GRU's: A step by step Explanation</title><link>https://wglog.net/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</link><pubDate>Sat, 06 Mar 2021 22:17:22 +0800</pubDate><guid>https://wglog.net/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</guid><description>In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.</description></item></channel></rss>