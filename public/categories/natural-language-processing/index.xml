<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on My Favorite</title><link>https://wglog.net/categories/natural-language-processing/</link><description>Recent content in Natural Language Processing on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 29 Mar 2021 15:25:04 +0800</lastBuildDate><atom:link href="https://wglog.net/categories/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Implementing a LSTM From Scratch With Numpy</title><link>https://wglog.net/archives/implementing-a-lstm-from-scratch-with-numpy/</link><pubDate>Mon, 29 Mar 2021 15:25:04 +0800</pubDate><guid>https://wglog.net/archives/implementing-a-lstm-from-scratch-with-numpy/</guid><description>In this post, The author implement a simple character-level LSTM using Numpy. It is trained in batches with the Adam optimiser and learns basic words after just a few training iterations.</description></item><item><title>Deriving the Backpropagation Equations for a LSTM</title><link>https://wglog.net/archives/deriving-the-backpropagation-equations-for-a-lstm/</link><pubDate>Mon, 29 Mar 2021 15:22:46 +0800</pubDate><guid>https://wglog.net/archives/deriving-the-backpropagation-equations-for-a-lstm/</guid><description>In this post, The author derive the backpropagation equations for a LSTM cell in vectorised form.</description></item><item><title>A Simple Overview of RNN, LSTM and Attention Mechanism</title><link>https://wglog.net/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</link><pubDate>Mon, 22 Mar 2021 14:57:23 +0800</pubDate><guid>https://wglog.net/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</guid><description>Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained.</description></item><item><title>Understanding GRU Networks</title><link>https://wglog.net/archives/understanding-gru-networks/</link><pubDate>Thu, 18 Mar 2021 17:24:49 +0800</pubDate><guid>https://wglog.net/archives/understanding-gru-networks/</guid><description>In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network.</description></item><item><title>Understanding Input and Output Shapes in LSTM</title><link>https://wglog.net/archives/understanding-input-and-output-shapes-in-lstm/</link><pubDate>Wed, 17 Mar 2021 19:25:24 +0800</pubDate><guid>https://wglog.net/archives/understanding-input-and-output-shapes-in-lstm/</guid><description>Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM.</description></item><item><title>Understanding the Number of Parameter Used in LSTM Network</title><link>https://wglog.net/archives/understanding-the-number-of-parameter-used-in-lstm-network/</link><pubDate>Tue, 16 Mar 2021 17:57:54 +0800</pubDate><guid>https://wglog.net/archives/understanding-the-number-of-parameter-used-in-lstm-network/</guid><description>It explained clearly how to calculate the number of trainable parameters used in the LSTM network.</description></item><item><title>An Introduction of Word2Vec and implementation using Tensorflow</title><link>https://wglog.net/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/</link><pubDate>Tue, 16 Mar 2021 12:36:21 +0800</pubDate><guid>https://wglog.net/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/</guid><description>Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.</description></item><item><title>[Kaggle Summary] Quora Insincere Questions Classification</title><link>https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/</link><pubDate>Fri, 12 Mar 2021 17:13:34 +0800</pubDate><guid>https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/</guid><description>The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.</description></item><item><title>How do LSTM Networks Solve the Problem of Vanishing Gradients</title><link>https://wglog.net/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</link><pubDate>Mon, 08 Mar 2021 13:35:16 +0800</pubDate><guid>https://wglog.net/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</guid><description>LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate’s activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.</description></item><item><title>Understanding LSTM Networks</title><link>https://wglog.net/archives/understanding-lstm-networks/</link><pubDate>Mon, 08 Mar 2021 13:02:42 +0800</pubDate><guid>https://wglog.net/archives/understanding-lstm-networks/</guid><description>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp;amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</description></item><item><title>Illustrated Guide to LSTM's and GRU's: A step by step Explanation</title><link>https://wglog.net/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</link><pubDate>Sat, 06 Mar 2021 22:17:22 +0800</pubDate><guid>https://wglog.net/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</guid><description>In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.</description></item><item><title>What is a Transformer?</title><link>https://wglog.net/archives/what-is-a-transformer/</link><pubDate>Sat, 06 Mar 2021 13:27:25 +0800</pubDate><guid>https://wglog.net/archives/what-is-a-transformer/</guid><description>An introduction to transformers and sequence-to-sequence learning for machine learning</description></item></channel></rss>