<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>My Favorite</title><link>https://oyzg.github.io/</link><description>Recent content on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 10 Apr 2022 18:06:13 +0800</lastBuildDate><atom:link href="https://oyzg.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Some Valuable Websites or Resources for Learning AI</title><link>https://oyzg.github.io/archives/some-useful-website-or-resources-for-learning-ai/</link><pubDate>Wed, 10 Mar 2021 16:22:38 +0800</pubDate><guid>https://oyzg.github.io/archives/some-useful-website-or-resources-for-learning-ai/</guid><description>I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.</description></item><item><title>计算机网络</title><link>https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</link><pubDate>Sun, 10 Apr 2022 18:06:13 +0800</pubDate><guid>https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/</guid><description>计算机网络面试相关内容.</description></item><item><title>All about Fourier Transform</title><link>https://oyzg.github.io/archives/all-about-fourier-transform/</link><pubDate>Thu, 13 May 2021 13:22:10 +0800</pubDate><guid>https://oyzg.github.io/archives/all-about-fourier-transform/</guid><description>This is a detailed tutorial about Fourier Transform and related topics. It intelligibly explained what Fourier Transform is and how it works.</description></item><item><title>Keras: Multiple outputs and multiple losses</title><link>https://oyzg.github.io/archives/keras-multiple-outputs-and-multiple-losses/</link><pubDate>Wed, 21 Apr 2021 14:31:58 +0800</pubDate><guid>https://oyzg.github.io/archives/keras-multiple-outputs-and-multiple-losses/</guid><description>A multiple outputs model has several fully connected layers for output. Each layer is responsible for performing a specific task and has a respective loss function and activation function.</description></item><item><title>Multi-label classification with Keras</title><link>https://oyzg.github.io/archives/multi-label-classification-with-keras/</link><pubDate>Wed, 21 Apr 2021 14:06:23 +0800</pubDate><guid>https://oyzg.github.io/archives/multi-label-classification-with-keras/</guid><description>The post introduces how to do multi-label classification with Keras and implements a simple model with custom SmallerVGGNet can predict both color and clothing type of input image.</description></item><item><title>Exploring Spatiotemporal Features for Activity Classifications in Films</title><link>https://oyzg.github.io/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/</link><pubDate>Wed, 21 Apr 2021 13:09:01 +0800</pubDate><guid>https://oyzg.github.io/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/</guid><description>The post introduces several experiments about activity classification based on three main architectures: 3D CNN, ConvLSTM2D, and a pipeline of pre-trained CNN-LSTM.</description></item><item><title>Video Classification in Keras using ConvLSTM</title><link>https://oyzg.github.io/archives/video-classification-keras-convlstm/</link><pubDate>Tue, 20 Apr 2021 19:37:11 +0800</pubDate><guid>https://oyzg.github.io/archives/video-classification-keras-convlstm/</guid><description>This article will explain the Deep Learning based solution of the Video Classification task in Keras using ConvLSTM layers.</description></item><item><title>Introduction to Video Classification and Human Activity Recognition</title><link>https://oyzg.github.io/archives/introduction-to-video-classification-and-human-activity-recognition/</link><pubDate>Fri, 16 Apr 2021 14:43:15 +0800</pubDate><guid>https://oyzg.github.io/archives/introduction-to-video-classification-and-human-activity-recognition/</guid><description>In this post, we will learn about Video Classification. We will go over a number of approaches to make a video classifier for Human Activity Recognition. Basically, you will learn video classification and human activity recognition.</description></item><item><title>PCA using Python (Scikit-Learn)</title><link>https://oyzg.github.io/archives/pca-using-python-scikit-learn/</link><pubDate>Fri, 09 Apr 2021 21:42:21 +0800</pubDate><guid>https://oyzg.github.io/archives/pca-using-python-scikit-learn/</guid><description>The post introduced the principal component analysis through the two most commonly used applications, speeding up a Machine Learning algorithm and data visualization.</description></item><item><title>mAP (mean Average Precision) for Object Detection</title><link>https://oyzg.github.io/archives/map-mean-average-precision-for-object-detection/</link><pubDate>Fri, 09 Apr 2021 14:43:12 +0800</pubDate><guid>https://oyzg.github.io/archives/map-mean-average-precision-for-object-detection/</guid><description>AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision computes the average precision value for recall value over 0 to 1.</description></item><item><title>Understanding PyTorch With an Example: A step by step Tutorial</title><link>https://oyzg.github.io/archives/understanding-pytorch-with-an-example-a-step-by-step-tutorial/</link><pubDate>Tue, 06 Apr 2021 21:19:40 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-pytorch-with-an-example-a-step-by-step-tutorial/</guid><description>In this post, The author will guide you through the main reasons why PyTorch makes it much easier and more intuitive to build a Deep Learning model in Python — autograd, dynamic computation graph, model classes and more - and He will also show you how to avoid some common pitfalls and errors along the way.</description></item><item><title>Implementing a LSTM From Scratch With Numpy</title><link>https://oyzg.github.io/archives/implementing-a-lstm-from-scratch-with-numpy/</link><pubDate>Mon, 29 Mar 2021 15:25:04 +0800</pubDate><guid>https://oyzg.github.io/archives/implementing-a-lstm-from-scratch-with-numpy/</guid><description>In this post, The author implement a simple character-level LSTM using Numpy. It is trained in batches with the Adam optimiser and learns basic words after just a few training iterations.</description></item><item><title>Deriving the Backpropagation Equations for a LSTM</title><link>https://oyzg.github.io/archives/deriving-the-backpropagation-equations-for-a-lstm/</link><pubDate>Mon, 29 Mar 2021 15:22:46 +0800</pubDate><guid>https://oyzg.github.io/archives/deriving-the-backpropagation-equations-for-a-lstm/</guid><description>In this post, The author derive the backpropagation equations for a LSTM cell in vectorised form.</description></item><item><title>The Illustrated Transformer</title><link>https://oyzg.github.io/archives/the-illustrated-transformer/</link><pubDate>Fri, 26 Mar 2021 10:47:26 +0800</pubDate><guid>https://oyzg.github.io/archives/the-illustrated-transformer/</guid><description>A pretty detailed illustration about the transformers: I recommend it for you if you are looking for an explained article about the paper Attention is all you need, or you want to learn the attention mechanism.</description></item><item><title>Attention? Attention!</title><link>https://oyzg.github.io/archives/attention-attention/</link><pubDate>Wed, 24 Mar 2021 13:14:48 +0800</pubDate><guid>https://oyzg.github.io/archives/attention-attention/</guid><description>Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.</description></item><item><title>A Simple Overview of RNN, LSTM and Attention Mechanism</title><link>https://oyzg.github.io/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</link><pubDate>Mon, 22 Mar 2021 14:57:23 +0800</pubDate><guid>https://oyzg.github.io/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</guid><description>Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained.</description></item><item><title>Understanding GRU Networks</title><link>https://oyzg.github.io/archives/understanding-gru-networks/</link><pubDate>Thu, 18 Mar 2021 17:24:49 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-gru-networks/</guid><description>In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network.</description></item><item><title>Understanding Input and Output Shapes in LSTM</title><link>https://oyzg.github.io/archives/understanding-input-and-output-shapes-in-lstm/</link><pubDate>Wed, 17 Mar 2021 19:25:24 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-input-and-output-shapes-in-lstm/</guid><description>Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM.</description></item><item><title>Understanding the Number of Parameter Used in LSTM Network</title><link>https://oyzg.github.io/archives/understanding-the-number-of-parameter-used-in-lstm-network/</link><pubDate>Tue, 16 Mar 2021 17:57:54 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-the-number-of-parameter-used-in-lstm-network/</guid><description>It explained clearly how to calculate the number of trainable parameters used in the LSTM network.</description></item><item><title>An Introduction of Word2Vec and implementation using Tensorflow</title><link>https://oyzg.github.io/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/</link><pubDate>Tue, 16 Mar 2021 12:36:21 +0800</pubDate><guid>https://oyzg.github.io/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/</guid><description>Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.</description></item><item><title>[Kaggle Summary] Quora Insincere Questions Classification</title><link>https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/</link><pubDate>Fri, 12 Mar 2021 17:13:34 +0800</pubDate><guid>https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/</guid><description>The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.</description></item><item><title>An Example of Classification on Imbalanced Data with Tensorflow</title><link>https://oyzg.github.io/archives/an-example-of-classification-on-imbalanced-data-with-tensorflow/</link><pubDate>Wed, 10 Mar 2021 14:57:23 +0800</pubDate><guid>https://oyzg.github.io/archives/an-example-of-classification-on-imbalanced-data-with-tensorflow/</guid><description>This tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the Credit Card Fraud Detection dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use Keras to define the model and class weights to help the model learn from the imbalanced data.</description></item><item><title>How do LSTM Networks Solve the Problem of Vanishing Gradients</title><link>https://oyzg.github.io/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</link><pubDate>Mon, 08 Mar 2021 13:35:16 +0800</pubDate><guid>https://oyzg.github.io/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</guid><description>LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate’s activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.</description></item><item><title>Understanding LSTM Networks</title><link>https://oyzg.github.io/archives/understanding-lstm-networks/</link><pubDate>Mon, 08 Mar 2021 13:02:42 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-lstm-networks/</guid><description>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp;amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</description></item><item><title>Illustrated Guide to LSTM's and GRU's: A step by step Explanation</title><link>https://oyzg.github.io/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</link><pubDate>Sat, 06 Mar 2021 22:17:22 +0800</pubDate><guid>https://oyzg.github.io/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</guid><description>In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.</description></item><item><title>What is a Transformer?</title><link>https://oyzg.github.io/archives/what-is-a-transformer/</link><pubDate>Sat, 06 Mar 2021 13:27:25 +0800</pubDate><guid>https://oyzg.github.io/archives/what-is-a-transformer/</guid><description>An introduction to transformers and sequence-to-sequence learning for machine learning</description></item><item><title>An Explanation of Bleu Score by Andrew Ng</title><link>https://oyzg.github.io/archives/an-explanation-of-bleu-score-by-andrew-ng/</link><pubDate>Fri, 05 Mar 2021 18:39:49 +0800</pubDate><guid>https://oyzg.github.io/archives/an-explanation-of-bleu-score-by-andrew-ng/</guid><description>BLEU: A method for automatic evaluation of machine translation</description></item><item><title>An Explanation about Three of the Most Important Metrics We Use: Accuracy Precision and Recall</title><link>https://oyzg.github.io/archives/an-explanation-about-three-of-the-most-important-metrics-we-use-accuracy-precision-and-recall/</link><pubDate>Tue, 02 Mar 2021 18:19:33 +0800</pubDate><guid>https://oyzg.github.io/archives/an-explanation-about-three-of-the-most-important-metrics-we-use-accuracy-precision-and-recall/</guid><description>More specifically, this article shows what happens when we focus on the wrong metric using an imbalanced classification problem.</description></item><item><title>Analyzing Handwritten Digit Model with Tensorflow Line by Line</title><link>https://oyzg.github.io/archives/analyzing-handwritten-digits-model-with-tensorflow-line-by-line/</link><pubDate>Tue, 02 Mar 2021 15:51:10 +0800</pubDate><guid>https://oyzg.github.io/archives/analyzing-handwritten-digits-model-with-tensorflow-line-by-line/</guid><description>Let&amp;rsquo;s do a line-by-line analysis of this deep learning model and truly understand what&amp;rsquo;s going on. This model identifies handwritten digits. It&amp;rsquo;s one of the classic examples of machine learning applied to computer vision</description></item><item><title>Archive</title><link>https://oyzg.github.io/archives/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://oyzg.github.io/archives/</guid><description>archives</description></item><item><title>Search</title><link>https://oyzg.github.io/search/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://oyzg.github.io/search/</guid><description>search</description></item></channel></rss>