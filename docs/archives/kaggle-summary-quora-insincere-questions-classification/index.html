<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[Kaggle Summary] Quora Insincere Questions Classification | My Favorite</title><meta name=keywords content="LSTM,kaggle,tensorflow,keras,imbalanced classification,class weight,bias initializer,tokenization,lemmatization,stemming,text vectorization,stop words,bag of words,TF-IDF,word2vec"><meta name=description content="The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec."><meta name=author content><link rel=canonical href=https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/><link href=/assets/css/stylesheet.min.d597aab94c71b49b163b6ac67c7958079589ea1a0b7300e8a0014e1d2023b95e.css integrity="sha256-1ZequUxxtJsWO2rGfHlYB5WJ6hoLcwDooAFOHSAjuV4=" rel="preload stylesheet" as=style><link rel=icon href=https://wglog.net/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wglog.net/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wglog.net/favicon-32x32.png><link rel=apple-touch-icon href=https://wglog.net/apple-touch-icon.png><link rel=mask-icon href=https://wglog.net/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.84.0"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js integrity=sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="[Kaggle Summary] Quora Insincere Questions Classification"><meta property="og:description" content="The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec."><meta property="og:type" content="article"><meta property="og:url" content="https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/"><meta property="article:published_time" content="2021-03-12T17:13:34+08:00"><meta property="article:modified_time" content="2021-03-12T17:13:34+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[Kaggle Summary] Quora Insincere Questions Classification"><meta name=twitter:description content="The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Archives","item":"https://wglog.net/archives/"},{"@type":"ListItem","position":2,"name":"[Kaggle Summary] Quora Insincere Questions Classification","item":"https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[Kaggle Summary] Quora Insincere Questions Classification","name":"[Kaggle Summary] Quora Insincere Questions Classification","description":"The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with …","keywords":["LSTM","kaggle","tensorflow","keras","imbalanced classification","class weight","bias initializer","tokenization","lemmatization","stemming","text vectorization","stop words","bag of words","TF-IDF","word2vec"],"articleBody":"Introduction This article is a summary of the notebook about Quora insincere questions classification in Kaggle.\nThe link of kaggle notebook is below.\n https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification\n The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.\n Important: The complete code of examples below can be found in Github or Colab.\n Key Points Text Preprocessing Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\nExamples of tokenization with NLTK:\nfrom nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize corpus = '''Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.''' print(sent_tokenize(corpus)) print(word_tokenize(corpus))  output:\n ['Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.', 'One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.'] ['Tokenization', 'is', 'the', 'process', 'of', 'tokenizing', 'or', 'splitting', 'a', 'string', ',', 'text', 'into', 'a', 'list', 'of', 'tokens', '.', 'One', 'can', 'think', 'of', 'token', 'as', 'parts', 'like', 'a', 'word', 'is', 'a', 'token', 'in', 'a', 'sentence', ',', 'and', 'a', 'sentence', 'is', 'a', 'token', 'in', 'a', 'paragraph', '.'] See also: https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\nLemmatization is a normalization technique in the field of natural language processing. There have a related technique called stemming. In some languages, the word has difference forms in difference contexts. The goal of both stemming and lemmatization is to reduce various forms to a common base form.\nExamples of lemmatization with NLTK:\nfrom nltk.stem import WordNetLemmatizer corpus = ['rocks', 'gone', 'better'] lemmatizer = WordNetLemmatizer() print([lemmatizer.lemmatize(w) for w in corpus])  output:\n ['rock', 'gone', 'better'] The result confused me. Why the output of gone and better isn’t the base form of them? Because we don’t provide the second parameter parts-of-speech to lemmatize. The parameter default value is n means a noun.\n Lemmatization needs the context to know what the parts-of-speech of the word is, a noun, verb or adjective so that can work well.\n Usage:\ndef lemmatize_sent(text): pos_dict = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'} word_list = [] for word, tag in pos_tag(word_tokenize(text)): pos = pos_dict[tag[0:2]] if tag[0:2] in pos_dict else 'n' word_list.append(lemmatizer.lemmatize(word, pos=pos)) return word_list sentence = 'He is walking to school' lemmatization_words = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)] print('lemmatize word by word: ', lemmatization_words) print('lemmatize with context: ', lemmatize_sent(sentence))  output:\n lemmatize word by word: ['He', 'is', 'walking', 'to', 'school'] lemmatize with context: ['He', 'be', 'walk', 'to', 'school'] Stemming shortens words with some rules and can be used without context, so it’s more efficient than Lemmatization.\nExamples of Stemming with NLTK:\nfrom nltk.stem import PorterStemmer corpus = ['rocks', 'going', 'history'] stemmer = PorterStemmer() print([stemmer.stem(w) for w in corpus])  output:\n ['rock', 'go', 'histori'] As you can see, the stemming outputs may not be actual words.\n Time-consuming Test:\n from nltk.corpus import gutenberg @timing def stemming(text): [stemmer.stem(w) for w in word_tokenize(sentence)] @timing def lemmatize(text): lemmatize_sent(text) @timing def lemmatize_without_context(text): [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)] book = gutenberg.raw(\"austen-sense.txt\") stemming(book) lemmatize(book) lemmatize_without_context(book)  output:\n stemming : 0.22 ms lemmatize : 5980.39 ms lemmatize_without_context : 0.17 ms See also: https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\nStop words usually refers to the most common words in a language, such as the, is, at, which and a in English. We remove the stop words before processing the text data because these words usually are no enough valuable.\nExamples of Stop words with NLTK:\nfrom nltk.corpus import stopwords corpus = ['I', 'am', 'a', 'boy'] print([w for w in corpus if w not in set(stopwords.words('english'))])  output:\n ['I', 'boy'] See also: https://www.nltk.org/book/ch02.html#stopwords_index_term\nText Vectorization is the process of converting text into a numerical vector that can be fed into a neural network or machine learning model. The most used methods to do that including Bag of Words, TF-IDF vectorization and Word2vec.\nExamples of Bag of Words with scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer() corpus = [ 'He is a teacher', 'I am student', 'She is also a student', ] X = vectorizer.fit_transform(corpus) print(vectorizer.get_feature_names()) print(X.toarray())  output:\n ['also', 'am', 'he', 'is', 'she', 'student', 'teacher'] [[0 0 1 1 0 0 1] [0 1 0 0 0 1 0] [1 0 0 1 1 1 0]] The default configuration tokenizes the string by extracting words of at least 2 letters. so the word a has been discarded. The vector representation table of words is below.\nV(also) : [1 0 0 0 0 0 0] V(am) : [0 1 0 0 0 0 0] V(he) : [0 0 1 0 0 0 0] V(is) : [0 0 0 1 0 0 0] V(she) : [0 0 0 0 1 0 0] V(student) : [0 0 0 0 0 1 0] V(teacher) : [0 0 0 0 0 0 1] V(He is a teacher) = V(He) + V(is) + V(teacher) = [0 0 1 1 0 0 1] ... TF means term-frequency. $$\\text{TF(t, d)}=\\frac{\\text{number of term t occurs in the document d}}{\\text{number of terms in the document d}}$$\nIDF means inverse document-frequency.\n$$\\text{IDF(t)}=\\log\\frac{\\text{number of documents in the corpus}}{\\text{number of document where the term t occurs}}$$\nTF-IDF means term-frequency times inverse document-frequency.\n$$\\text{TF-IDF(t, d)}=\\text{TF(t, d)} \\times \\text{IDF(t)}$$\nExamples of TF-IDF Verctorization with scikit-learn:\nfrom sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() corpus = [ 'He is a teacher', 'I am student', 'She is also a student', ] X = vectorizer.fit_transform(corpus) print(vectorizer.get_feature_names()) print(X.toarray())  output:\n ['also', 'am', 'he', 'is', 'she', 'student', 'teacher'] [[0. 0. 0.62276601 0.4736296 0. 0. 0.62276601] [0. 0.79596054 0. 0. 0. 0.60534851 0. ] [0.5628291 0. 0. 0.42804604 0.5628291 0.42804604 0. ]]  Note: The calculational formula of TF-IDF has some changes in the TfidfVectorizer.\n Word2vec is an algorithm that uses a shallow feedforward neural network to learn word embedding from a large corpus of text. There have two methods for producing a distributed representation of words.\n Continuous Bag-of-Words which was usually called CBOW predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). Continuous Skip-gram uses the current word to predict the surrounding window of context words.  See also: https://www.tensorflow.org/tutorials/text/word2vec\nExamples of Word2vec with gensim:\nimport gensim.downloader from gensim.models import Word2Vec word2vec = gensim.downloader.load('word2vec-google-news-300') print(word2vec.most_similar('car')) print(word2vec.word_vec('car'))  output:\n [('vehicle', 0.7821096181869507), ('cars', 0.7423830032348633), ('SUV', 0.7160962820053101), ('minivan', 0.6907036304473877), ('truck', 0.6735789775848389), ('Car', 0.6677608489990234), ('Ford_Focus', 0.667320191860199), ('Honda_Civic', 0.662684977054596), ('Jeep', 0.6511331796646118), ('pickup_truck', 0.64414381980896)] [ 0.13085938 0.00842285 0.03344727 -0.05883789 0.04003906 -0.14257812 0.04931641 -0.16894531 0.20898438 0.11962891 0.18066406 -0.25 ... 0.04248047 0.12792969 -0.27539062 0.28515625 -0.04736328 0.06494141 -0.11230469 -0.02575684 -0.04125977 0.22851562 -0.14941406 -0.15039062] See also: https://radimrehurek.com/gensim/models/word2vec.html\nLSTM network LSTM is an artificial recurrent neural network architecture. Its full name is long short-term memory, it is well-suited to classifying, processing and making predictions based on time series data.\nA common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\nSee also: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe following is an explanation of some parameters that were being used in the model.\nThe metrics parameter is used to evaluate the model during training and testing. It uses F1-score as the final evaluation metric in this task,. $$\\text{F1-score}=2\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}$$ Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.\nRecall is the number of true positive results divided by the number of all samples that should have been identified as positive.\nSee also: https://en.wikipedia.org/wiki/F-score\nThe bias_initializer parameter can influence the initial output of the layer. E.g. You have an imbalanced dataset of the ratio of positives to negatives is 1: 9. The model will predict 50% positive and 50% negative at initialization (PS: before training) if you don’t set the bias_initializer parameter. But if you have set the parameter properly, it will predict 10% positive and 90% negative at initialization, it is more reasonable. In addition, setting it correctly will speed up convergence.\nThe calculation of the bias_initializer parameter depends on what activation function the layer used. The correct bias to set can be derived from below if the activation function is sigmoid. $$p_0=\\frac{\\text{positive}}{\\text{positive}+\\text{negative}}=\\frac{1}{1+\\exp(-b_0)}$$ then $$b_0=-\\log(\\frac{1}{p_0}-1)=\\log(\\frac{\\text{positive}}{\\text{negative}})$$ See also: Classification on imbalanced data\nYou can refer to this question if the activation function is softmax.\nThe class weight parameter is also important for a model during training if the dataset is imbalanced. Giving a heavier weight to an under-represented class will cause the model to pay more attention to it.\nFinal Thoughts When I finish the article, I learned more than when I begin to write. I found some approaches more convenient to preprocess text, such as TextVectorization in tensorflow. I hava a more in-depth understand of Word2Vec. I just knew it was a distributed representation of words. Now I know there have two methods to implement it, CBOW and skip-gram. The skip-gram model uses the technique called negative sampling to maximize the probability of predicting context words when given the target word.\nReferences  Stemming and Lemmatization Basic NLP with NLTK NLP | How tokenizing text, sentence, words works Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text Word2Vec - Tensorflow Long short-term memory - Wikipedia  ","wordCount":"1560","inLanguage":"en","datePublished":"2021-03-12T17:13:34+08:00","dateModified":"2021-03-12T17:13:34+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://wglog.net/archives/kaggle-summary-quora-insincere-questions-classification/"},"publisher":{"@type":"Organization","name":"My Favorite","logo":{"@type":"ImageObject","url":"https://wglog.net/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://wglog.net/ accesskey=h title="My Favorite (Alt + H)">My Favorite</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://wglog.net/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://wglog.net/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://wglog.net/series/ title=Series><span>Series</span></a></li><li><a href=https://wglog.net/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://wglog.net/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[Kaggle Summary] Quora Insincere Questions Classification</h1><div class=post-meta>March 12, 2021&nbsp;·&nbsp;8 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#key-points aria-label="Key Points">Key Points</a><ul><li><a href=#text-preprocessing aria-label="Text Preprocessing">Text Preprocessing</a></li><li><a href=#lstm-network aria-label="LSTM network">LSTM network</a></li></ul></li><li><a href=#final-thoughts aria-label="Final Thoughts">Final Thoughts</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>This article is a summary of the notebook about <a href=https://www.kaggle.com/c/quora-insincere-questions-classification>Quora insincere questions classification</a> in Kaggle.</p><p>The link of kaggle notebook is below.</p><blockquote><p><a href=https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification>https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification</a></p></blockquote><p>The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.</p><blockquote><p><strong>Important</strong>: The complete code of examples below can be found in <a href=https://github.com/wgouyang/notebook/blob/master/kaggle-summary-quora-insincere-questions-classification.ipynb>Github</a> or <a href=https://colab.research.google.com/github/wgouyang/notebook/blob/main/kaggle-summary-quora-insincere-questions-classification.ipynb>Colab</a>.</p></blockquote><h2 id=key-points>Key Points<a hidden class=anchor aria-hidden=true href=#key-points>#</a></h2><h3 id=text-preprocessing>Text Preprocessing<a hidden class=anchor aria-hidden=true href=#text-preprocessing>#</a></h3><p><strong><code>Tokenization</code></strong> is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.</p><p>Examples of <code>tokenization</code> with NLTK:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> nltk.tokenize <span style=color:#f92672>import</span> word_tokenize
<span style=color:#f92672>from</span> nltk.tokenize <span style=color:#f92672>import</span> sent_tokenize

corpus <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;&#39;&#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.&#39;&#39;&#39;</span>

print(sent_tokenize(corpus))
print(word_tokenize(corpus))
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>['Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.', 'One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.']
['Tokenization', 'is', 'the', 'process', 'of', 'tokenizing', 'or', 'splitting', 'a', 'string', ',', 'text', 'into', 'a', 'list', 'of', 'tokens', '.', 'One', 'can', 'think', 'of', 'token', 'as', 'parts', 'like', 'a', 'word', 'is', 'a', 'token', 'in', 'a', 'sentence', ',', 'and', 'a', 'sentence', 'is', 'a', 'token', 'in', 'a', 'paragraph', '.']
</code></pre><p>See also: <a href=https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize>https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize</a></p><p><strong><code>Lemmatization</code></strong> is a normalization technique in the field of natural language processing. There have a related technique called <code>stemming</code>. In some languages, the word has difference forms in difference contexts. The goal of both <code>stemming</code> and <code>lemmatization</code> is to reduce various forms to a common base form.</p><p>Examples of <code>lemmatization</code> with NLTK:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> nltk.stem <span style=color:#f92672>import</span> WordNetLemmatizer

corpus <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;rocks&#39;</span>, <span style=color:#e6db74>&#39;gone&#39;</span>, <span style=color:#e6db74>&#39;better&#39;</span>]
lemmatizer <span style=color:#f92672>=</span> WordNetLemmatizer()

print([lemmatizer<span style=color:#f92672>.</span>lemmatize(w) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> corpus])
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>['rock', 'gone', 'better']
</code></pre><p>The result confused me. Why the output of <code>gone</code> and <code>better</code> isn&rsquo;t the base form of them? Because we don&rsquo;t provide the second parameter <code>parts-of-speech</code> to lemmatize. The parameter default value is <code>n</code> means a noun.</p><blockquote><p><code>Lemmatization</code> needs the context to know what the <code>parts-of-speech</code> of the word is, a noun, verb or adjective so that can work well.</p></blockquote><p>Usage:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lemmatize_sent</span>(text):
    pos_dict <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;NN&#39;</span>:<span style=color:#e6db74>&#39;n&#39;</span>, <span style=color:#e6db74>&#39;JJ&#39;</span>:<span style=color:#e6db74>&#39;a&#39;</span>, <span style=color:#e6db74>&#39;VB&#39;</span>:<span style=color:#e6db74>&#39;v&#39;</span>, <span style=color:#e6db74>&#39;RB&#39;</span>:<span style=color:#e6db74>&#39;r&#39;</span>}
    word_list <span style=color:#f92672>=</span> []
    <span style=color:#66d9ef>for</span> word, tag <span style=color:#f92672>in</span> pos_tag(word_tokenize(text)):
        pos <span style=color:#f92672>=</span> pos_dict[tag[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>2</span>]] <span style=color:#66d9ef>if</span> tag[<span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>2</span>] <span style=color:#f92672>in</span> pos_dict <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;n&#39;</span>
        word_list<span style=color:#f92672>.</span>append(lemmatizer<span style=color:#f92672>.</span>lemmatize(word, pos<span style=color:#f92672>=</span>pos))
    <span style=color:#66d9ef>return</span> word_list

sentence <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;He is walking to school&#39;</span>
lemmatization_words <span style=color:#f92672>=</span> [lemmatizer<span style=color:#f92672>.</span>lemmatize(w) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> word_tokenize(sentence)]
print(<span style=color:#e6db74>&#39;lemmatize word by word: &#39;</span>, lemmatization_words)
print(<span style=color:#e6db74>&#39;lemmatize with context: &#39;</span>, lemmatize_sent(sentence))
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>lemmatize word by word:  ['He', 'is', 'walking', 'to', 'school']
lemmatize with context:  ['He', 'be', 'walk', 'to', 'school']
</code></pre><p><strong><code>Stemming</code></strong> shortens words with some rules and can be used without context, so it&rsquo;s more efficient than <code>Lemmatization</code>.</p><p>Examples of <code>Stemming</code> with NLTK:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> nltk.stem <span style=color:#f92672>import</span> PorterStemmer

corpus <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;rocks&#39;</span>, <span style=color:#e6db74>&#39;going&#39;</span>, <span style=color:#e6db74>&#39;history&#39;</span>]
stemmer <span style=color:#f92672>=</span> PorterStemmer()
print([stemmer<span style=color:#f92672>.</span>stem(w) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> corpus])
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>['rock', 'go', 'histori']
</code></pre><p>As you can see, the <code>stemming</code> outputs may not be actual words.</p><blockquote><p>Time-consuming Test:</p></blockquote><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> gutenberg

<span style=color:#a6e22e>@timing</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>stemming</span>(text):
    [stemmer<span style=color:#f92672>.</span>stem(w) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> word_tokenize(sentence)]

<span style=color:#a6e22e>@timing</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lemmatize</span>(text):
    lemmatize_sent(text)

<span style=color:#a6e22e>@timing</span>
<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lemmatize_without_context</span>(text):
    [lemmatizer<span style=color:#f92672>.</span>lemmatize(w) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> word_tokenize(sentence)]

book <span style=color:#f92672>=</span> gutenberg<span style=color:#f92672>.</span>raw(<span style=color:#e6db74>&#34;austen-sense.txt&#34;</span>)

stemming(book)
lemmatize(book)
lemmatize_without_context(book)
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>stemming                      : 0.22    ms
lemmatize                     : 5980.39 ms
lemmatize_without_context     : 0.17    ms
</code></pre><p>See also: <a href=https://www.nltk.org/api/nltk.stem.html#module-nltk.stem>https://www.nltk.org/api/nltk.stem.html#module-nltk.stem</a></p><p><strong><code>Stop words</code></strong> usually refers to the most common words in a language, such as the, is, at, which and a in English. We remove the <code>stop words</code> before processing the text data because these words usually are no enough valuable.</p><p>Examples of <code>Stop words</code> with NLTK:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords

corpus <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;I&#39;</span>, <span style=color:#e6db74>&#39;am&#39;</span>, <span style=color:#e6db74>&#39;a&#39;</span>, <span style=color:#e6db74>&#39;boy&#39;</span>]
print([w <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> corpus <span style=color:#66d9ef>if</span> w <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> set(stopwords<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#39;english&#39;</span>))])
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>['I', 'boy']
</code></pre><p>See also: <a href=https://www.nltk.org/book/ch02.html#stopwords_index_term>https://www.nltk.org/book/ch02.html#stopwords_index_term</a></p><p><strong><code>Text Vectorization</code></strong> is the process of converting text into a numerical vector that can be fed into a neural network or machine learning model. The most used methods to do that including <code>Bag of Words</code>, <code>TF-IDF</code> vectorization and <code>Word2vec</code>.</p><p>Examples of <code>Bag of Words</code> with scikit-learn:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> CountVectorizer

vectorizer <span style=color:#f92672>=</span> CountVectorizer()
corpus <span style=color:#f92672>=</span> [
    <span style=color:#e6db74>&#39;He is a teacher&#39;</span>,
    <span style=color:#e6db74>&#39;I am student&#39;</span>,
    <span style=color:#e6db74>&#39;She is also a student&#39;</span>,
]
X <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>fit_transform(corpus)

print(vectorizer<span style=color:#f92672>.</span>get_feature_names())
print(X<span style=color:#f92672>.</span>toarray())
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>['also', 'am', 'he', 'is', 'she', 'student', 'teacher']
[[0 0 1 1 0 0 1]
 [0 1 0 0 0 1 0]
 [1 0 0 1 1 1 0]]
</code></pre><p>The default configuration tokenizes the string by extracting words of at least 2 letters. so the word a has been discarded. The vector representation table of words is below.</p><pre><code class=language-log data-lang=log>V(also)    : [1 0 0 0 0 0 0]
V(am)      : [0 1 0 0 0 0 0]
V(he)      : [0 0 1 0 0 0 0]
V(is)      : [0 0 0 1 0 0 0]
V(she)     : [0 0 0 0 1 0 0]
V(student) : [0 0 0 0 0 1 0]
V(teacher) : [0 0 0 0 0 0 1]

V(He is a teacher) = V(He) + V(is) + V(teacher) = [0 0 1 1 0 0 1]
...
</code></pre><p>TF means term-frequency.
$$\text{TF(t, d)}=\frac{\text{number of term t occurs in the document d}}{\text{number of terms in the document d}}$$</p><p>IDF means inverse document-frequency.<br>$$\text{IDF(t)}=\log\frac{\text{number of documents in the corpus}}{\text{number of document where the term t occurs}}$$</p><p>TF-IDF means term-frequency times inverse document-frequency.</p><p>$$\text{TF-IDF(t, d)}=\text{TF(t, d)} \times \text{IDF(t)}$$</p><p>Examples of <code>TF-IDF</code> Verctorization with scikit-learn:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> TfidfVectorizer

vectorizer <span style=color:#f92672>=</span> TfidfVectorizer()
corpus <span style=color:#f92672>=</span> [
    <span style=color:#e6db74>&#39;He is a teacher&#39;</span>,
    <span style=color:#e6db74>&#39;I am student&#39;</span>,
    <span style=color:#e6db74>&#39;She is also a student&#39;</span>,
]
X <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>fit_transform(corpus)

print(vectorizer<span style=color:#f92672>.</span>get_feature_names())
print(X<span style=color:#f92672>.</span>toarray())
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>['also', 'am', 'he', 'is', 'she', 'student', 'teacher']
[[0.         0.         0.62276601 0.4736296  0.         0.         0.62276601]
 [0.         0.79596054 0.         0.         0.         0.60534851 0.        ]
 [0.5628291  0.         0.         0.42804604 0.5628291  0.42804604 0.        ]]
</code></pre><blockquote><p>Note: The calculational formula of TF-IDF has some changes in the TfidfVectorizer.</p></blockquote><p><strong><code>Word2vec</code></strong> is an algorithm that uses a shallow feedforward neural network to learn word embedding from a large corpus of text. There have two methods for producing a distributed representation of words.</p><ul><li><code>Continuous Bag-of-Words</code> which was usually called <code>CBOW</code> predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption).</li><li><code>Continuous Skip-gram</code> uses the current word to predict the surrounding window of context words.</li></ul><p>See also: <a href=https://www.tensorflow.org/tutorials/text/word2vec>https://www.tensorflow.org/tutorials/text/word2vec</a></p><p>Examples of <code>Word2vec</code> with gensim:</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> gensim.downloader
<span style=color:#f92672>from</span> gensim.models <span style=color:#f92672>import</span> Word2Vec

word2vec <span style=color:#f92672>=</span> gensim<span style=color:#f92672>.</span>downloader<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#39;word2vec-google-news-300&#39;</span>)
print(word2vec<span style=color:#f92672>.</span>most_similar(<span style=color:#e6db74>&#39;car&#39;</span>))
print(word2vec<span style=color:#f92672>.</span>word_vec(<span style=color:#e6db74>&#39;car&#39;</span>))
</code></pre></div><blockquote><p>output:</p></blockquote><pre><code class=language-log data-lang=log>[('vehicle', 0.7821096181869507),
 ('cars', 0.7423830032348633),
 ('SUV', 0.7160962820053101),
 ('minivan', 0.6907036304473877),
 ('truck', 0.6735789775848389),
 ('Car', 0.6677608489990234),
 ('Ford_Focus', 0.667320191860199),
 ('Honda_Civic', 0.662684977054596),
 ('Jeep', 0.6511331796646118),
 ('pickup_truck', 0.64414381980896)]
 [ 0.13085938  0.00842285  0.03344727 -0.05883789  0.04003906 -0.14257812
  0.04931641 -0.16894531  0.20898438  0.11962891  0.18066406 -0.25
  ...
  0.04248047  0.12792969 -0.27539062  0.28515625 -0.04736328  0.06494141
 -0.11230469 -0.02575684 -0.04125977  0.22851562 -0.14941406 -0.15039062]
</code></pre><p>See also: <a href=https://radimrehurek.com/gensim/models/word2vec.html>https://radimrehurek.com/gensim/models/word2vec.html</a></p><h3 id=lstm-network>LSTM network<a hidden class=anchor aria-hidden=true href=#lstm-network>#</a></h3><p><strong><code>LSTM</code></strong> is an artificial recurrent neural network architecture. Its full name is long short-term memory, it is well-suited to classifying, processing and making predictions based on time series data.<br>A common <code>LSTM</code> unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.</p><p>See also: <a href=https://colah.github.io/posts/2015-08-Understanding-LSTMs/>https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>The following is an explanation of some parameters that were being used in the model.</p><p>The <code>metrics</code> parameter is used to evaluate the model during training and testing. It uses <code>F1-score</code> as the final evaluation metric in this task,.
$$\text{F1-score}=2\cdot\frac{\text{precision}\cdot\text{recall}}{\text{precision}+\text{recall}}$$
<code>Precision</code> is the number of true positive results divided by the number of all positive results, including those not identified correctly.<br><code>Recall</code> is the number of true positive results divided by the number of all samples that should have been identified as positive.</p><p>See also: <a href=https://en.wikipedia.org/wiki/F-score>https://en.wikipedia.org/wiki/F-score</a></p><p>The <code>bias_initializer</code> parameter can influence the initial output of the layer. E.g. You have an imbalanced dataset of the ratio of positives to negatives is 1: 9. The model will predict 50% positive and 50% negative at initialization (PS: before training) if you don&rsquo;t set the <code>bias_initializer</code> parameter.
But if you have set the parameter properly, it will predict 10% positive and 90% negative at initialization, it is more reasonable. In addition, setting it correctly will speed up convergence.</p><p>The calculation of the <code>bias_initializer</code> parameter depends on what activation function the layer used. The correct bias to set can be derived from below if the activation function is <code>sigmoid</code>.
$$p_0=\frac{\text{positive}}{\text{positive}+\text{negative}}=\frac{1}{1+\exp(-b_0)}$$
then
$$b_0=-\log(\frac{1}{p_0}-1)=\log(\frac{\text{positive}}{\text{negative}})$$
See also: <a href=https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#optional_set_the_correct_initial_bias>Classification on imbalanced data</a></p><p>You can refer to this <a href=https://stackoverflow.com/questions/60307239/setting-bias-for-multiclass-classification-python-tensorflow-keras>question</a> if the activation function is <code>softmax</code>.</p><p>The <code>class weight</code> parameter is also important for a model during training if the dataset is imbalanced. Giving a heavier weight to an under-represented class will cause the model to pay more attention to it.</p><h2 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h2><p>When I finish the article, I learned more than when I begin to write. I found some approaches more convenient to preprocess text, such as <code>TextVectorization</code> in tensorflow. I hava a more in-depth understand of <code>Word2Vec</code>. I just knew it was a distributed representation of words. Now I know there have two methods to implement it, <code>CBOW</code> and <code>skip-gram</code>. The <code>skip-gram</code> model uses the technique called negative sampling to maximize the probability of predicting context words when given the target word.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html>Stemming and Lemmatization</a></li><li><a href=https://www.kaggle.com/alvations/basic-nlp-with-nltk#Stemming-and-Lemmatization>Basic NLP with NLTK</a></li><li><a href=https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/>NLP | How tokenizing text, sentence, words works</a></li><li><a href=https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/>Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text</a></li><li><a href=https://www.tensorflow.org/tutorials/text/word2vec>Word2Vec - Tensorflow</a></li><li><a href=https://en.wikipedia.org/wiki/Long_short-term_memory>Long short-term memory - Wikipedia</a></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://wglog.net/tags/lstm/>LSTM</a></li><li><a href=https://wglog.net/tags/kaggle/>kaggle</a></li><li><a href=https://wglog.net/tags/tensorflow/>tensorflow</a></li><li><a href=https://wglog.net/tags/keras/>keras</a></li><li><a href=https://wglog.net/tags/imbalanced-classification/>imbalanced classification</a></li><li><a href=https://wglog.net/tags/class-weight/>class weight</a></li><li><a href=https://wglog.net/tags/bias-initializer/>bias initializer</a></li><li><a href=https://wglog.net/tags/tokenization/>tokenization</a></li><li><a href=https://wglog.net/tags/lemmatization/>lemmatization</a></li><li><a href=https://wglog.net/tags/stemming/>stemming</a></li><li><a href=https://wglog.net/tags/text-vectorization/>text vectorization</a></li><li><a href=https://wglog.net/tags/stop-words/>stop words</a></li><li><a href=https://wglog.net/tags/bag-of-words/>bag of words</a></li><li><a href=https://wglog.net/tags/tf-idf/>TF-IDF</a></li><li><a href=https://wglog.net/tags/word2vec/>word2vec</a></li></ul><nav class=paginav><a class=prev href=https://wglog.net/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/><span class=title>« Prev Page</span><br><span>An Introduction of Word2Vec and implementation using Tensorflow</span></a>
<a class=next href=https://wglog.net/archives/an-example-of-classification-on-imbalanced-data-with-tensorflow/><span class=title>Next Page »</span><br><span>An Example of Classification on Imbalanced Data with Tensorflow</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [Kaggle Summary] Quora Insincere Questions Classification on twitter" href="https://twitter.com/intent/tweet/?text=%5bKaggle%20Summary%5d%20Quora%20Insincere%20Questions%20Classification&url=https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f&hashtags=LSTM%2ckaggle%2ctensorflow%2ckeras%2cimbalancedclassification%2cclassweight%2cbiasinitializer%2ctokenization%2clemmatization%2cstemming%2ctextvectorization%2cstopwords%2cbagofwords%2cTF-IDF%2cword2vec"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [Kaggle Summary] Quora Insincere Questions Classification on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f&title=%5bKaggle%20Summary%5d%20Quora%20Insincere%20Questions%20Classification&summary=%5bKaggle%20Summary%5d%20Quora%20Insincere%20Questions%20Classification&source=https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [Kaggle Summary] Quora Insincere Questions Classification on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f&title=%5bKaggle%20Summary%5d%20Quora%20Insincere%20Questions%20Classification"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [Kaggle Summary] Quora Insincere Questions Classification on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [Kaggle Summary] Quora Insincere Questions Classification on whatsapp" href="https://api.whatsapp.com/send?text=%5bKaggle%20Summary%5d%20Quora%20Insincere%20Questions%20Classification%20-%20https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [Kaggle Summary] Quora Insincere Questions Classification on telegram" href="https://telegram.me/share/url?text=%5bKaggle%20Summary%5d%20Quora%20Insincere%20Questions%20Classification&url=https%3a%2f%2fwglog.net%2farchives%2fkaggle-summary-quora-insincere-questions-classification%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://wglog.net/>My Favorite</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script></body></html>