<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kubernetes | oyzg's note</title>
<meta name=keywords content="运维,Docker,K8s,Kubernetes,云原生"><meta name=description content="容器编排"><meta name=author content><link rel=canonical href=https://oyzg.github.io/archives/kubernetes/><link href=/assets/css/stylesheet.min.d597aab94c71b49b163b6ac67c7958079589ea1a0b7300e8a0014e1d2023b95e.css integrity="sha256-1ZequUxxtJsWO2rGfHlYB5WJ6hoLcwDooAFOHSAjuV4=" rel="preload stylesheet" as=style><link rel=icon href=https://oyzg.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://oyzg.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://oyzg.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://oyzg.github.io/apple-touch-icon.png><link rel=mask-icon href=https://oyzg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.147.4"><link rel=alternate hreflang=en href=https://oyzg.github.io/archives/kubernetes/><meta property="og:title" content="Kubernetes"><meta property="og:description" content="容器编排"><meta property="og:type" content="article"><meta property="og:url" content="https://oyzg.github.io/archives/kubernetes/"><meta property="article:published_time" content="2025-10-15T15:35:36+08:00"><meta property="article:modified_time" content="2025-10-15T15:35:36+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes"><meta name=twitter:description content="容器编排"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Kubernetes","item":"https://oyzg.github.io/archives/kubernetes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes","name":"Kubernetes","description":"容器编排","keywords":["运维","Docker","K8s","Kubernetes","云原生"],"articleBody":"容器 容器的本质其实就是一个特殊的进程。它通过Linux NameSpace、Linux CGroups、rootfs来实现对进程的隔离和约束。\nDocker的核心步骤： 1、启用 Linux Namespace 配置； 2、设置指定的 Cgroups 参数； 3、切换进程的根目录（Change Root）。\nNameSpace NameSpace是用来修改进程视图的方法，通过NameSpace技术可以让这个进程只能看到自己这个namespace中的空间，使用方式则是通过添加相关参数。\nint pid = clone(main_function, stack_size, SIGCHLD, NULL); # 添加CLONE_NEWPID后容器内看到的pid就是1了，除此外还有Mount、UTS、IPC、Network 和 User等Namespace int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); CGroups 在NameSpace修改视图后，相比于虚拟化技术还是隔离得不够彻底，多个容器间使用的还是同一个操作系统内核，而且还有很多资源和对象不能NameSpace化，比如时间，在容器内修改时间是对宿主机可见的。\nCGroups（Linux Control Group）技术是用来对进程进行资源限制的，限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等，从而防止一个进程把整个系统的资源吃光的情况。此外，还有对进程进行优先级设置、审计等作用。\n它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下，通过修改该路径下的文件即可实现对进程的资源限制。\nrootfs 在对进程进行隔离后，容器内看到的文件系统应该是完全独立的，让其不受宿主机和其他容器的影响。Mount Namespace 修改的是容器进程对文件系统“挂载点”的认知（对挂载点进行了隔离），即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样，只有在容器执行挂载后，才会看到与宿主机不同的文件系统。在Linux中有一个chroot命令可以改变进程的根目录到你指定的位置。\n但是我们一般需要在容器的根目录下挂载完整操作系统的文件系统，挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。\n需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。所有容器都共享宿主机的操作系统内核。\nDocker在制作镜像时，引入了层的概念，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。rootfs由三部分组成，如图所示。只读层包含操作系统的基础文件，可读写层包含你的程序、依赖包等，如果你删除了只读层的文件，就会在可读写层生成一个foo文件，Init层主要包含一些操作系统的配置信息，如/etc/hosts、/etc/resolv.conf，这些文件输入只读层，但是我们需要对这些配置文件进行修改，并且希望这些配置信息在docker commit时不包含在内。\nDocker Docker常用命令 数据拷贝：docker cp [源路径] [目标路径]，对于容器内的文件采用[容器名或容器ID]:[路径]的形式，如docker cp a.txt 062:/tmp 共享目录：通过-v参数挂载，docker run -v [宿主机路径]:[容器内路径] 网络模式 null：容器与宿主机之间不通信 host：容器与宿主机共享网络，docker run时使用–net=host参数开启 bridge：容器与宿主机之间通过docker0网桥进行通信，使用–net=bridge开启，一般不需要，因为是默认的 端口号映射需要bridge模式，在docker run 中指定-p参数，-p [宿主机端口]:[容器端口] Dockerfile 如前面rootfs部分所述，docker镜像是分成很多层的，可以用docker inspect [image name]来查看分层信息。\n如果需要自己构建镜像则可以通过docker build命令进行构建，-f参数指定Dockerfile文件，没有就是Dockerfile文件，需要目录下只有这一个Dockerfile文件\nDockerfile中的常用命令有：\nFROM:基础镜像 COPY:用于将本机的源码、配置文件等复制到镜像中，源文件必须在“构建上下文”路径中 RUN:用于执行SHELL命令，一行只能有一条命令，所以末尾用\\，命令之间使用\u0026\u0026相连，为了美观可以写一个sh文件，用COPY拷贝进来再RUN执行 变量:ARG和ENV，ARG变量只在镜像过程中可见，ENV变量在镜像构建和容器运行时均可见 TIP：Dockerfile每个指令都会生成一个镜像层，所以要精简\nKubernetes Kubernetes是一个容器编排工具。编排的意思就是能够按照用户意愿和系统规则，完全自动化的处理好容器之间的关系。调度是把容器按照某种规则，放在最佳节点上运行起来。\nKubunetes分为Master节点和note节点，Master节点由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。node节点最核心的部分是kubectl，用于和容器运行时打交道,此外还有kube-proxy用于管理容器的网络通信，ccontainer-runtime管理Pod的生命周期。整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。\n除此之外，还有kubectl，是Kubernetes的命令行工具，用于和集群进行交互。\nminikube minikube 是一个迷你版的Kubernetes集群，常用命令：\nminikube version： 查看minikube版本 minikube start：启动minikube集群，–kubernetes-version=指定版本 minikube stop：停止minikube集群 minikube status：查看minikube集群状态 minikube delete：删除minikube集群 minikube node list：查看minikube集群节点 kubectl kubectl是Kubernetes的命令行工具，用于和集群进行交互，常用命令：\nkubectl get pods|nodes|services|deployments|namespaces：查看所有pod｜node｜service｜deployment｜namespace信息 kubectl describe pods|nodes|services|deployments|namespaces：查看指定pod｜node｜service｜deployment｜namespace信息 kubectl create|delete|edit -f [文件名]：创建|删除|编辑 指定资源 kubectl apply -f [文件名]：使用文件创建资源 kubectl logs [pod名]：查看指定pod的日志 kubectl exec [pod名] [命令]：在指定pod中执行命令, kubectl exec [pod名] -it –/bin/bash进入pod kubectl run [pod名] –image=[镜像名]：运行一个pod kubectl get pod -n [namespace]：查看指定namespace下的所有pod kubectl api-resources：查看支持的所有API对象 kubectl explain [API对象]：查看API对象， –dry-run=client -o yaml可以生成yaml样板 API对象 由于Kubenetes的设计思路——“单一职责”和“组合优于继承”，所有对象都尽量只关注自己的职责。 Pod Pod是Kubernetes的最小运行单位，一个Pod中可以运行多个容器，每个容器之间是相互隔离的，但是可以共享同一个IP地址和端口。yaml示例：\napiVersion: v1 kind: Pod metadata: name: busy-pod labels: owner: chrono env: demo region: north tier: back spec: containers: - image: busybox:latest name: busy imagePullPolicy: IfNotPresent env: - name: os value: \"ubuntu\" - name: debug value: \"on\" command: - /bin/echo args: - \"$(os), $(debug)\" resources: # 限制使用资源 requests: # 要申请的资源 cpu: 10m # 1000m = 1CPU时间 memory: 100Mi limits: # 使用资源的上限 cpu: 20m memory: 200Mi 由apiVersion、kind、metadata、spec四个基本组成部分，metadata包含pod的名称、标签、注解等信息，spec包含pod的运行参数，包括容器镜像、环境变量、命令、参数、端口映射、卷挂载、资源限制、调度策略等等。\nKubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态：\nStartup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。如果Startup探针失败，会尝试反复重启 Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。如果容器异常也会重启容器 Readiness，就绪探针，用来检查应用是否已经准备好接收流量。如果失败会从Service负载均衡中移除，不再分配流量 应用程序先启动，加载完配置文件等基本的初始化数据就进入了 Startup 状态，之后如果没有什么异常就是 Liveness 存活状态，但可能有一些准备工作没有完成，还不一定能对外提供服务，只有到最后的 Readiness 状态才是一个容器最健康可用的状态\n要使用探针需要预留“检查口”，如下，在ConfigMap中使用/ready作为检查口。\napiVersion: v1 kind: ConfigMap metadata: name: ngx-conf data: default.conf: | server { listen 80; location = /ready { return 200 'I am ready'; } } 探针的具体定义：\napiVersion: v1 kind: Pod metadata: name: ngx-pod-probe spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol startupProbe: # 启动探针 periodSeconds: 1 # 执行探测动作的时间间隔 # timeoutSeconds字段 探测动作的超时时间 # successThreshold字段 连续几次探测成功才认为是正常 # failureThreshold字段 连续几次探测失败才认为是异常 # 三种探测方式： exec、TCP Socket、HTTP GET exec: # 执行一个Linux命令 command: [\"cat\", \"/var/run/nginx.pid\"] livenessProbe: # 存活探针 periodSeconds: 10 tcpSocket: # 使用 TCP 协议尝试连接容器的指定端口 port: 80 readinessProbe: # 就绪探针 periodSeconds: 5 httpGet: # 连接端口并发送 HTTP GET 请求 path: /ready port: 80 Job和CronJob 在线业务：长时间运行的，如nginx 离线业务：短时间运行的，如定时任务 Job和CronJob都是用来处理离线业务的，Job是处理临时任务，CronJob是处理定时任务。 Job的yaml示例： apiVersion: batch/v1 kind: Job metadata: name: echo-job spec: activeDeadlineSeconds: 15 backoffLimit: 2 completions: 4 parallelism: 2 template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-job imagePullPolicy: IfNotPresent command: [\"/bin/echo\"] args: [\"hello\", \"world\"] template定义了一个应用模版，里面用来嵌入Pod。 几个重要字段：\nactiveDeadlineSeconds，设置 Pod 运行的超时时间。 backoffLimit，设置 Pod 的失败重试次数。 completions，Job 完成需要运行多少个 Pod，默认是 1 个。 parallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源 CronJob的yaml示例：\napiVersion: batch/v1 kind: CronJob metadata: name: echo-cj spec: # 这个spec是CronJob的配置 schedule: '*/* * * * *' # 定时，每分钟执行一次 jobTemplate: # 这个下面嵌套Job spec: template: # 这个下面嵌套Pod spec: restartPolicy: OnFailure containers: - image: busybox name: echo-cj imagePullPolicy: IfNotPresent command: [\"/bin/echo\"] args: [\"hello\", \"world\"] ConfigMap和Secret ConfigMap和Secret用于保存配置信息，ConfigMap保存的是非敏感信息，如数据库连接信息，Secret保存的是敏感信息，如密码。这些信息都保存在etcd中。\nConfigMap的yaml示例：\napiVersion: v1 kind: ConfigMap metadata: name: info data: # 用来存储数据，kv结构 count: '10' debug: 'on' path: '/etc/systemd' greeting: | say hello to kubernetes. Secret的yaml示例：\napiVersion: v1 kind: Secret metadata: name: user data: # 这些都是base64编码的， name: cm9vdA== # root pwd: MTIzNDU2 # 123456 db: bXlzY2Fw # mysql # 自己手动base64编码，-n是删除隐含的换行符 echo -n \"123456\" | base64 MTIzNDU2 使用方法： 1、环境变量的方式：利用pod.spec.containers.env.valueFrom以环境变量的形式注入Pod\napiVersion: v1 kind: Pod metadata: name: env-pod spec: containers: - env: - name: COUNT valueFrom: configMapKeyRef: #ConfigMap对象 name: info key: count - name: USERNAME valueFrom: secretKeyRef: #Secret对象 name: user key: name 2、Volume的方式：类似docker run -v，在Pod中挂载一个Volume，然后把Volume中的内容映射到容器中。\napiVersion: v1 kind: Pod metadata: name: vol-pod spec: # 定义两个 Volume，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol volumes: - name: cm-vol configMap: name: info - name: sec-vol secret: secretName: user # 然后在容器中挂载 Volume containers: - volumeMounts: - mountPath: /tmp/cm-items name: cm-vol - mountPath: /tmp/sec-items name: sec-vol image: busybox name: busy imagePullPolicy: IfNotPresent command: [\"/bin/sleep\", \"300\"] Deployment Deployment是用来部署应用程序的，用来处理在线业务，让应用永不宕机。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: ngx-dep name: ngx-dep spec: replicas: 2 # 副本数，运行的Pod数量 selector: matchLabels: app: ngx-dep # 匹配标签，和下面的template中的标签一致 # 通过这种labels，解除了Deployment和Pod的强绑定，把组合关系变成了“弱引用” template: metadata: labels: app: ngx-dep spec: containers: - image: nginx:alpine name: nginx 通过scale命令来扩容： kubectl scale –replicas=5 deploy ngx-dep\nDaemonset 用于在集群的每个节点上运行且仅运行一个Pod，主要用于日志收集、数据采集等场景。\napiVersion: apps/v1 kind: DaemonSet metadata: name: redis-ds labels: app: redis-ds spec: selector: matchLabels: name: redis-ds template: metadata: labels: name: redis-ds spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 污点（taint）和容忍度（toleration\nMaster默认不跑应用，需要配置taint和toleration来配置才能在master上运行pod。 方法一：去掉master节点的taint，末尾的-就是去除 kubectl taint node master node-role.kubernetes.io/master:NoSchedule-\n方法二：在创建节点的时候，添加toleration\ntolerations: - key: node-role.kubernetes.io/master effect: NoSchedule operator: Exists 静态Pod 默认在/etc/kubernetes/manifests下，Kubernetes 的 4 个核心组件 apiserver、etcd、scheduler、controller-manager都以静态 Pod 的形式存在的\nService 用于服务发现和负载均衡，通过iptables实现。\n生成yaml文件命令：kubectl expose deploy ngx-dep –port=80 –target-port=80 –dry-run=client -o yaml\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: selector: app: ngx-dep ports: - port: 80 # 外部端口 targetPort: 80 # 内部端口 protocol: TCP # 协议 type：ClusterIP # 还有“ExternalName”、“LoadBalancer”、\"NodePort\"，“NodePort”类型会创建一个专用映射端口，外部可访问 namespace\nkubectl get ns查看所有的namespace，默认情况下所有API对象都在defalult namespace下。\n基于 DNS 插件，可以以域名形式访问Service，Service对象的域名形式为：”对象. 名字空间.svc.cluster.local“，或者“对象. 名字空间”、“对象名”\nIngress Ingress是流量的总入口，统管集群的进出口数据，以及负载均衡（七层，Service是四层负载均衡）\nIngress Controller Ingress Controller是Ingress资源的实际执行者，负责监听Ingress资源的变化并根据规则配置负载均衡器。 常用的Ingress Controller实现有Nginx Ingress Controller。\n工作原理：\n用户创建Ingress资源定义路由规则 Ingress Controller持续监听API Server中的Ingress资源变化 当Ingress资源发生变化时，Controller更新其内部的负载均衡配置 外部流量通过Ingress Controller访问集群内的服务 IngreeClass 用于定义 Ingress 控制器的类型和配置，解决了集群中存在多个 Ingress 控制器时的路由冲突问题。\nPersistentVolume 用于管理存储资源，属于系统资源，与Node平级，Pod只拥有其使用权\nPersistentVolumeClaim，简称 PVC，用来向Kubernetes申请存储资源，由Pod使用，代表Pod向系统申请PV，申请成功后会将PV和PVC绑定（bind） StorageClass，在PV和PVC之间，帮助PVC找到合适的PV。 PV的yaml：\napiVersion: v1 kind: PersistentVolume metadata: name: host-10m-pv spec: storageClassName: host-test # 对应StorageClass # 访问模式，ReadWriteOnce（可读可写，只能被一个Pod使用）、ReadOnlyMany（只可读，可以被多个Pod使用）、ReadWriteMany（可读可写，可以被多个Pod使用） accessModes: - ReadWriteOnce capacity: # 存储容量 storage: 10Mi hostPath: # 存储卷的本地路径，在节点上创建的目录 path: /tmp/host-10m-pv/ PVC的yaml：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: host-5m-pvc spec: storageClassName: host-test accessModes: - ReadWriteOnce resources: requests: storage: 5Mi # 请求的存储容量 在Pod中使用PVC：\napiVersion: v1 kind: Pod metadata: name: host-pvc-pod spec: volumes: - name: host-pvc-vol persistentVolumeClaim: claimName: host-5m-pvc # 绑定PVC containers: - name: ngx-pvc-pod image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: host-pvc-vol mountPath: /tmp HostPath 是最简单的一种 PV，数据存储在节点本地，速度快但不能跟随 Pod 迁移\n要实现跨节点数据共享，需要使用NFS或者Ceph等网络存储系统\n# NFS PV apiVersion: v1 kind: PersistentVolume metadata: name: nfs-1g-pv spec: storageClassName: nfs accessModes: - ReadWriteMany capacity: storage: 1Gi nfs: path: /tmp/nfs/1g-pv # 需事先创建好 server: 192.168.10.208 以上PV都是静态存储卷，需要手动创建，而Provisioner可以自动管理、创建PV，是动态存储卷。Provisioner以Pod形式运行，部署Provisioner有三个yaml文件，rbac.yaml、class.yaml和deployment.yaml。\n使用Provisioner时就不需要定义PV了，只需要在PVC中指定StorageClass，StorageClass再关联到Provisioner。 StorageClass的yaml：\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # Provisioner名称 parameters: archiveOnDelete: \"false\" # \"false\"为自动回收存储空间 StatefulSet Deployment只能管理无状态的应用，StatefulSet管理有状态的，可以看作Deployment的特例。\nStatefulSet启动的Pod是有顺序的，名字为XXXX-0, XXXX-1等，可以根据编号来决定依赖关系，\nStatefulSet的yaml：\napiVersion: apps/v1 kind: StatefulSet metadata: name: redis-sts spec: serviceName: redis-svc # 和Service中metadata.name一致 volumeClaimTemplates: # PVC，为每个Pod自动创建PVC - metadata: name: redis-100m-pvc spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 100Mi replicas: 2 selector: matchLabels: app: redis-sts template: metadata: labels: app: redis-sts spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 volumeMounts: - name: redis-100m-pvc mountPath: /data 写 Service 对象的时候要小心一些，metadata.name 必须和StatefulSet 里的 serviceName 相同，selector 里的标签也必须和 StatefulSet 里的一致。StatefulSet创建的Pod的域名和其他Pod不同，格式为“Pod名.服务名.名字空间.svc.cluster.local”，简写”Pod名.服务名“。\nnamespace namespace是Kubernetes中的一个概念，可以将集群切分成多个区域，同时namespace也是一种API对象，简称ns。Kubernetes中默认有四个namespace：default、kube-system、kube-public、kube-node-lease。要将API对象放入指定namespace中可以在metadata里添加一个namespace字段。\nResourceQuota 用于为namespace进行资源配额，yaml文件示例：\napiVersion: v1 kind: ResourceQuota metadata: name: dev-qt namespace: dev-ns spec: hard: # 硬性全局限制 requests.cpu: 10 # 10个CPU requests.memory: 10Gi limits.cpu: 10 limits.memory: 20Gi requests.storage: 100Gi persistentvolumeclaims: 100 pods: 100 configmaps: 100 secrets: 100 services: 10 count/jobs.batch: 1 count/cronjobs.batch: 1 count/deployments.apps: 1 在添加资源配额后，要求所有在里面运行的 Pod 都必须用字段 resources 声明资源需求，否则就无法创建。可以用LimitRange自动为Pod加上资源限制，LimitRange的yaml：\napiVersion: v1 kind: LimitRange metadata: name: dev-limits namespace: dev-ns spec: limits: - type: Container defaultRequest: cpu: 200m memory: 50Mi default: cpu: 500m memory: 100Mi - type: Pod max: cpu: 800m memory: 200Mi HorizontalPodAutoscaler 基于Metrics Server，它从 Metrics Server 获取当前应用的运行指标，主要是 CPU 使用率，再依据预定的策略增加或者减少 Pod 的数量 yaml:\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: ngx-hpa spec: maxReplicas: 10 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: ngx-hpa-dep targetCPUUtilizationPercentage: 5 在Deployment的spec下一定要写resources字段，否则无法实现自动扩缩容\nMetrics Server Metrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具，它定时从所有节点的 kubelet 里采集信息,安装Metrics Server后可以使用kubectl top查看资源使用情况\nPrometheus 相对于Metrics Server，Prometheus的指标更全一些。Prometheus 是云原生监控领域的“事实标准”，用 PromQL 语言来查询数据，配合 Grafana可以展示直观的图形界面，方便监控。\nkubuadm——一键部署 使用方法：\n# 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join Kubernetes在部署时，它的每一个组件都是一个需要被执行的、单独的二进制文件。所以部署十分麻烦，kubeadm的方案是把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。\n滚动更新 Kubernetes中rsion使用Pod模版的hash值来作为版本号。 要实现滚动更新，主要有以下几个命令：\nkubectl apply -f xxx.yaml,修改yaml文件后应用即可，Kubernetes会进行滚动更新（逐步的创建新版本Pod，删除旧版本Pod，最后只剩新版本Pod，旧版本Pod全部删除） kubectl rollout pause暂停更新 kubectl rollout resume恢复更新 kubectl rollout history查看版本历史，加上参数–revision x，查看指定版本详细信息 kubectl rollout undo，回滚到上一个版本，–to-revision参数指定版本 Deployment 的 metadata添加字段annotations类似label，但是label用于外部对象，annotations用于内部对象，其中kubernetes.io/change-cause是更新版本说明。 网络模型 Docker有三种网络模式：null、host 和 bridge，但是只适用于单机，Kubernetes提出了自己的网络模型IP-per-pod：\n集群里的每个 Pod 都会有唯一的一个 IP 地址。 Pod 里的所有容器共享这个 IP 地址。 集群里的所有 Pod 都属于同一个网段。 Pod直接可以基于IP地址访问另一个Pod，不需要做网络地址转换（NAT） CNI（Container Networking Interface） CNI 为网络插件定义了一系列通用接口，CNI插件大致分为三种：\nOverlay：它构建了一个工作在真实底层网络之上的“逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。 Route：在底层网络之上工作，但它没有封包和拆包，而是使用系统内置的路由功能来实现Pod 跨主机通信。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了 Underlay： 直接用底层网络来实现 CNI，也就是说 Pod 和宿主机都在一个网络里，Pod和宿主机是平等的。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。 常用的CNI插件：\nFlannel：最早是Overlay模式，后来用 Host-Gateway 技术支持了 Route 模式，简单易用、但是性能不是很好 Calico：Route模式使用BGP 协议来维护路由信息，性能要比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能 Cilium：同时支持 Overlay 模式和 Route 模式，它的特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，所以性能很高，可以灵活实现各种功能 参考资料： 《Kubernetes入门实战课》— 罗剑锋（极客时间）\n","wordCount":"1227","inLanguage":"en","datePublished":"2025-10-15T15:35:36+08:00","dateModified":"2025-10-15T15:35:36+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://oyzg.github.io/archives/kubernetes/"},"publisher":{"@type":"Organization","name":"oyzg's note","logo":{"@type":"ImageObject","url":"https://oyzg.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://oyzg.github.io/ accesskey=h title="oyzg's note (Alt + H)">oyzg's note</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://oyzg.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://oyzg.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://oyzg.github.io/series/ title=Series><span>Series</span></a></li><li><a href=https://oyzg.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://oyzg.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Kubernetes</h1><div class=post-meta>October 15, 2025 · 6 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#%e5%ae%b9%e5%99%a8 aria-label=容器>容器</a><ul><ul><ul><li><a href=#namespace aria-label=NameSpace>NameSpace</a></li><li><a href=#cgroups aria-label=CGroups>CGroups</a></li><li><a href=#rootfs aria-label=rootfs>rootfs</a></li></ul></ul></ul></li><li><a href=#docker aria-label=Docker>Docker</a><ul><li><a href=#docker%e5%b8%b8%e7%94%a8%e5%91%bd%e4%bb%a4 aria-label=Docker常用命令>Docker常用命令</a></li><li><a href=#dockerfile aria-label=Dockerfile>Dockerfile</a></li></ul></li><li><a href=#kubernetes aria-label=Kubernetes>Kubernetes</a><ul><ul><li><a href=#minikube aria-label=minikube>minikube</a></li><li><a href=#kubectl aria-label=kubectl>kubectl</a></li></ul><li><a href=#api%e5%af%b9%e8%b1%a1 aria-label=API对象>API对象</a><ul><li><a href=#pod aria-label=Pod>Pod</a></li><li><a href=#job%e5%92%8ccronjob aria-label=Job和CronJob>Job和CronJob</a></li><li><a href=#configmap%e5%92%8csecret aria-label=ConfigMap和Secret>ConfigMap和Secret</a></li><li><a href=#deployment aria-label=Deployment>Deployment</a></li><li><a href=#daemonset aria-label=Daemonset>Daemonset</a><ul><li><a href=#%e9%9d%99%e6%80%81pod aria-label=静态Pod>静态Pod</a></li></ul></li><li><a href=#service aria-label=Service>Service</a></li><li><a href=#ingress aria-label=Ingress>Ingress</a><ul><li><a href=#ingress-controller aria-label="Ingress Controller">Ingress Controller</a></li><li><a href=#ingreeclass aria-label=IngreeClass>IngreeClass</a></li></ul></li><li><a href=#persistentvolume aria-label=PersistentVolume>PersistentVolume</a></li><li><a href=#statefulset aria-label=StatefulSet>StatefulSet</a></li><li><a href=#namespace-1 aria-label=namespace>namespace</a></li><li><a href=#resourcequota aria-label=ResourceQuota>ResourceQuota</a></li><li><a href=#horizontalpodautoscaler aria-label=HorizontalPodAutoscaler>HorizontalPodAutoscaler</a><ul><li><a href=#metrics-server aria-label="Metrics Server">Metrics Server</a></li><li><a href=#prometheus aria-label=Prometheus>Prometheus</a></li></ul></li></ul></li><li><a href=#kubuadm%e4%b8%80%e9%94%ae%e9%83%a8%e7%bd%b2 aria-label=kubuadm——一键部署>kubuadm——一键部署</a></li><li><a href=#%e6%bb%9a%e5%8a%a8%e6%9b%b4%e6%96%b0 aria-label=滚动更新>滚动更新</a></li><li><a href=#%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b aria-label=网络模型>网络模型</a><ul><li><a href=#cnicontainer-networking-interface aria-label="CNI（Container Networking Interface）">CNI（Container Networking Interface）</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=容器>容器<a hidden class=anchor aria-hidden=true href=#容器>#</a></h1><p>容器的本质其实就是一个特殊的进程。它通过Linux NameSpace、Linux CGroups、rootfs来实现对进程的隔离和约束。</p><p>Docker的核心步骤：
1、启用 Linux Namespace 配置；
2、设置指定的 Cgroups 参数；
3、切换进程的根目录（Change Root）。</p><h4 id=namespace>NameSpace<a hidden class=anchor aria-hidden=true href=#namespace>#</a></h4><p>NameSpace是用来修改进程视图的方法，通过NameSpace技术可以让这个进程只能看到自己这个namespace中的空间，使用方式则是通过添加相关参数。</p><pre tabindex=0><code>int pid = clone(main_function, stack_size, SIGCHLD, NULL); 
# 添加CLONE_NEWPID后容器内看到的pid就是1了，除此外还有Mount、UTS、IPC、Network 和 User等Namespace
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 
</code></pre><h4 id=cgroups>CGroups<a hidden class=anchor aria-hidden=true href=#cgroups>#</a></h4><p>在NameSpace修改视图后，相比于虚拟化技术还是隔离得不够彻底，多个容器间使用的还是同一个操作系统内核，而且还有很多资源和对象不能NameSpace化，比如时间，在容器内修改时间是对宿主机可见的。</p><p>CGroups（Linux Control Group）技术是用来对进程进行资源限制的，限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等，从而防止一个进程把整个系统的资源吃光的情况。此外，还有对进程进行优先级设置、审计等作用。</p><p>它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下，通过修改该路径下的文件即可实现对进程的资源限制。</p><h4 id=rootfs>rootfs<a hidden class=anchor aria-hidden=true href=#rootfs>#</a></h4><p>在对进程进行隔离后，容器内看到的文件系统应该是完全独立的，让其不受宿主机和其他容器的影响。Mount Namespace 修改的是容器进程对文件系统“挂载点”的认知（对挂载点进行了隔离），即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样，只有在容器执行挂载后，才会看到与宿主机不同的文件系统。在Linux中有一个chroot命令可以改变进程的根目录到你指定的位置。</p><p>但是我们一般需要在容器的根目录下挂载完整操作系统的文件系统，挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。</p><p>需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。所有容器都共享宿主机的操作系统内核。</p><p>Docker在制作镜像时，引入了层的概念，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。rootfs由三部分组成，如图所示。只读层包含操作系统的基础文件，可读写层包含你的程序、依赖包等，如果你删除了只读层的文件，就会在可读写层生成一个foo文件，Init层主要包含一些操作系统的配置信息，如/etc/hosts、/etc/resolv.conf，这些文件输入只读层，但是我们需要对这些配置文件进行修改，并且希望这些配置信息在docker commit时不包含在内。</p><p><img src=/images/kubernetes/rootfs.png alt=rootfs></p><h1 id=docker>Docker<a hidden class=anchor aria-hidden=true href=#docker>#</a></h1><h2 id=docker常用命令>Docker常用命令<a hidden class=anchor aria-hidden=true href=#docker常用命令>#</a></h2><p><img src=/images/kubernetes/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.png alt=docker常用命令></p><ul><li>数据拷贝：docker cp [源路径] [目标路径]，对于容器内的文件采用[容器名或容器ID]:[路径]的形式，如docker cp a.txt 062:/tmp</li><li>共享目录：通过-v参数挂载，docker run -v [宿主机路径]:[容器内路径]</li><li>网络模式<ul><li>null：容器与宿主机之间不通信</li><li>host：容器与宿主机共享网络，docker run时使用&ndash;net=host参数开启</li><li>bridge：容器与宿主机之间通过docker0网桥进行通信，使用&ndash;net=bridge开启，一般不需要，因为是默认的</li><li>端口号映射需要bridge模式，在docker run 中指定-p参数，-p [宿主机端口]:[容器端口]</li></ul></li></ul><h2 id=dockerfile>Dockerfile<a hidden class=anchor aria-hidden=true href=#dockerfile>#</a></h2><p>如前面rootfs部分所述，docker镜像是分成很多层的，可以用docker inspect [image name]来查看分层信息。</p><p>如果需要自己构建镜像则可以通过docker build命令进行构建，-f参数指定Dockerfile文件，没有就是Dockerfile文件，需要目录下只有这一个Dockerfile文件</p><p>Dockerfile中的常用命令有：</p><ul><li>FROM:基础镜像</li><li>COPY:用于将本机的源码、配置文件等复制到镜像中，源文件必须在“构建上下文”路径中</li><li>RUN:用于执行SHELL命令，一行只能有一条命令，所以末尾用\，命令之间使用&&相连，为了美观可以写一个sh文件，用COPY拷贝进来再RUN执行</li><li>变量:ARG和ENV，ARG变量只在镜像过程中可见，ENV变量在镜像构建和容器运行时均可见</li></ul><p>TIP：Dockerfile每个指令都会生成一个镜像层，所以要精简</p><h1 id=kubernetes>Kubernetes<a hidden class=anchor aria-hidden=true href=#kubernetes>#</a></h1><p>Kubernetes是一个容器编排工具。编排的意思就是能够按照用户意愿和系统规则，完全自动化的处理好容器之间的关系。调度是把容器按照某种规则，放在最佳节点上运行起来。</p><p>Kubunetes分为Master节点和note节点，Master节点由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。node节点最核心的部分是kubectl，用于和容器运行时打交道,此外还有kube-proxy用于管理容器的网络通信，ccontainer-runtime管理Pod的生命周期。整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。</p><p><img src=/images/kubernetes/rootfs.png alt=rootfs></p><p>除此之外，还有kubectl，是Kubernetes的命令行工具，用于和集群进行交互。</p><h3 id=minikube>minikube<a hidden class=anchor aria-hidden=true href=#minikube>#</a></h3><p>minikube 是一个迷你版的Kubernetes集群，常用命令：</p><ul><li>minikube version： 查看minikube版本</li><li>minikube start：启动minikube集群，&ndash;kubernetes-version=指定版本</li><li>minikube stop：停止minikube集群</li><li>minikube status：查看minikube集群状态</li><li>minikube delete：删除minikube集群</li><li>minikube node list：查看minikube集群节点</li></ul><h3 id=kubectl>kubectl<a hidden class=anchor aria-hidden=true href=#kubectl>#</a></h3><p>kubectl是Kubernetes的命令行工具，用于和集群进行交互，常用命令：</p><ul><li>kubectl get pods|nodes|services|deployments|namespaces：查看所有pod｜node｜service｜deployment｜namespace信息</li><li>kubectl describe pods|nodes|services|deployments|namespaces：查看指定pod｜node｜service｜deployment｜namespace信息</li><li>kubectl create|delete|edit -f [文件名]：创建|删除|编辑 指定资源</li><li>kubectl apply -f [文件名]：使用文件创建资源</li><li>kubectl logs [pod名]：查看指定pod的日志</li><li>kubectl exec [pod名] [命令]：在指定pod中执行命令, kubectl exec [pod名] -it &ndash;/bin/bash进入pod</li><li>kubectl run [pod名] &ndash;image=[镜像名]：运行一个pod</li><li>kubectl get pod -n [namespace]：查看指定namespace下的所有pod</li><li>kubectl api-resources：查看支持的所有API对象</li><li>kubectl explain [API对象]：查看API对象， &ndash;dry-run=client -o yaml可以生成yaml样板</li></ul><h2 id=api对象>API对象<a hidden class=anchor aria-hidden=true href=#api对象>#</a></h2><p>由于Kubenetes的设计思路——“单一职责”和“组合优于继承”，所有对象都尽量只关注自己的职责。
<img src=/images/kubernetes/Pod.png alt=Pod></p><h3 id=pod>Pod<a hidden class=anchor aria-hidden=true href=#pod>#</a></h3><p>Pod是Kubernetes的最小运行单位，一个Pod中可以运行多个容器，每个容器之间是相互隔离的，但是可以共享同一个IP地址和端口。yaml示例：</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: busy-pod
  labels:
    owner: chrono
    env: demo
    region: north
    tier: back
spec:
  containers:
    - image: busybox:latest
      name: busy
      imagePullPolicy: IfNotPresent
      env:
        - name: os
          value: &#34;ubuntu&#34;
        - name: debug
          value: &#34;on&#34;
      command:
        - /bin/echo
      args:
        - &#34;$(os), $(debug)&#34;
      
      resources: # 限制使用资源
        requests: # 要申请的资源
          cpu: 10m # 1000m = 1CPU时间
          memory: 100Mi
        limits: # 使用资源的上限
          cpu: 20m
          memory: 200Mi
</code></pre><p>由apiVersion、kind、metadata、spec四个基本组成部分，metadata包含pod的名称、标签、注解等信息，spec包含pod的运行参数，包括容器镜像、环境变量、命令、参数、端口映射、卷挂载、资源限制、调度策略等等。</p><p>Kubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态：</p><ul><li>Startup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。如果Startup探针失败，会尝试反复重启</li><li>Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。如果容器异常也会重启容器</li><li>Readiness，就绪探针，用来检查应用是否已经准备好接收流量。如果失败会从Service负载均衡中移除，不再分配流量</li></ul><p>应用程序先启动，加载完配置文件等基本的初始化数据就进入了 Startup 状态，之后如果没有什么异常就是 Liveness 存活状态，但可能有一些准备工作没有完成，还不一定能对外提供服务，只有到最后的 Readiness 状态才是一个容器最健康可用的状态</p><p>要使用探针需要预留“检查口”，如下，在ConfigMap中使用/ready作为检查口。</p><pre tabindex=0><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: ngx-conf
data:
  default.conf: |
    server {
      listen 80;
      location = /ready {
        return 200 &#39;I am ready&#39;;
      }
    }
</code></pre><p>探针的具体定义：</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: ngx-pod-probe
spec:
  volumes:
    - name: ngx-conf-vol
      configMap:
        name: ngx-conf
  containers:
    - image: nginx:alpine
      name: ngx
      ports:
        - containerPort: 80
      volumeMounts:
        - mountPath: /etc/nginx/conf.d
          name: ngx-conf-vol
      startupProbe: # 启动探针
        periodSeconds: 1 # 执行探测动作的时间间隔
        # timeoutSeconds字段 探测动作的超时时间
        # successThreshold字段 连续几次探测成功才认为是正常
        # failureThreshold字段 连续几次探测失败才认为是异常
        # 三种探测方式： exec、TCP Socket、HTTP GET
        exec: # 执行一个Linux命令
          command: [&#34;cat&#34;, &#34;/var/run/nginx.pid&#34;]
      livenessProbe: # 存活探针
        periodSeconds: 10
        tcpSocket: # 使用 TCP 协议尝试连接容器的指定端口
          port: 80
      readinessProbe: # 就绪探针
        periodSeconds: 5 
        httpGet: # 连接端口并发送 HTTP GET 请求
          path: /ready
          port: 80
</code></pre><h3 id=job和cronjob>Job和CronJob<a hidden class=anchor aria-hidden=true href=#job和cronjob>#</a></h3><ul><li>在线业务：长时间运行的，如nginx</li><li>离线业务：短时间运行的，如定时任务
Job和CronJob都是用来处理离线业务的，Job是处理临时任务，CronJob是处理定时任务。
Job的yaml示例：</li></ul><pre tabindex=0><code>apiVersion: batch/v1
kind: Job
metadata:
  name: echo-job

spec:
  activeDeadlineSeconds: 15
  backoffLimit: 2
  completions: 4
  parallelism: 2

  template:
    spec:
      restartPolicy: OnFailure
      containers:
      - image: busybox
        name: echo-job
        imagePullPolicy: IfNotPresent
        command: [&#34;/bin/echo&#34;]
        args: [&#34;hello&#34;, &#34;world&#34;]
</code></pre><p>template定义了一个应用模版，里面用来嵌入Pod。
几个重要字段：</p><ul><li>activeDeadlineSeconds，设置 Pod 运行的超时时间。</li><li>backoffLimit，设置 Pod 的失败重试次数。</li><li>completions，Job 完成需要运行多少个 Pod，默认是 1 个。</li><li>parallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源</li></ul><p>CronJob的yaml示例：</p><pre tabindex=0><code>apiVersion: batch/v1
kind: CronJob
metadata:
  name: echo-cj

spec: # 这个spec是CronJob的配置
  schedule: &#39;*/* * * * *&#39; # 定时，每分钟执行一次
  jobTemplate: # 这个下面嵌套Job
    spec:
      template: # 这个下面嵌套Pod
        spec:
          restartPolicy: OnFailure
          containers:
          - image: busybox
            name: echo-cj
            imagePullPolicy: IfNotPresent
            command: [&#34;/bin/echo&#34;]
            args: [&#34;hello&#34;, &#34;world&#34;]
</code></pre><h3 id=configmap和secret>ConfigMap和Secret<a hidden class=anchor aria-hidden=true href=#configmap和secret>#</a></h3><p>ConfigMap和Secret用于保存配置信息，ConfigMap保存的是非敏感信息，如数据库连接信息，Secret保存的是敏感信息，如密码。这些信息都保存在etcd中。</p><p>ConfigMap的yaml示例：</p><pre tabindex=0><code>apiVersion: v1
kind: ConfigMap
metadata:
  name: info

data: # 用来存储数据，kv结构
  count: &#39;10&#39;
  debug: &#39;on&#39;
  path: &#39;/etc/systemd&#39;
  greeting: |
    say hello to kubernetes.
</code></pre><p>Secret的yaml示例：</p><pre tabindex=0><code>apiVersion: v1
kind: Secret
metadata:
  name: user

data: # 这些都是base64编码的，
  name: cm9vdA==  # root
  pwd: MTIzNDU2  # 123456
  db: bXlzY2Fw  # mysql
</code></pre><pre tabindex=0><code># 自己手动base64编码，-n是删除隐含的换行符
echo -n &#34;123456&#34; | base64
MTIzNDU2
</code></pre><p>使用方法：
1、环境变量的方式：利用pod.spec.containers.env.valueFrom以环境变量的形式注入Pod</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: env-pod

spec:
  containers:
  - env:
    - name: COUNT
      valueFrom:
        configMapKeyRef: #ConfigMap对象
          name: info
          key: count
    - name: USERNAME
      valueFrom:
        secretKeyRef: #Secret对象
          name: user
          key: name
</code></pre><p>2、Volume的方式：类似docker run -v，在Pod中挂载一个Volume，然后把Volume中的内容映射到容器中。</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: vol-pod

spec:
  # 定义两个 Volume，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol
  volumes:
    - name: cm-vol
      configMap:
        name: info
    - name: sec-vol
      secret:
        secretName: user

  # 然后在容器中挂载 Volume
  containers:
    - volumeMounts:
        - mountPath: /tmp/cm-items
          name: cm-vol
        - mountPath: /tmp/sec-items
          name: sec-vol
      image: busybox
      name: busy
      imagePullPolicy: IfNotPresent
      command: [&#34;/bin/sleep&#34;, &#34;300&#34;]
</code></pre><h3 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h3><p>Deployment是用来部署应用程序的，用来处理在线业务，让应用永不宕机。</p><pre tabindex=0><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ngx-dep
  name: ngx-dep
spec:
  replicas: 2  # 副本数，运行的Pod数量
  selector:
    matchLabels:
      app: ngx-dep   # 匹配标签，和下面的template中的标签一致
      # 通过这种labels，解除了Deployment和Pod的强绑定，把组合关系变成了“弱引用”
  template:
    metadata:
      labels:
        app: ngx-dep
    spec:
      containers:
        - image: nginx:alpine
          name: nginx
</code></pre><p>通过scale命令来扩容：
kubectl scale &ndash;replicas=5 deploy ngx-dep</p><h3 id=daemonset>Daemonset<a hidden class=anchor aria-hidden=true href=#daemonset>#</a></h3><p>用于在集群的每个节点上运行且仅运行一个Pod，主要用于日志收集、数据采集等场景。</p><pre tabindex=0><code>apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: redis-ds
  labels:
    app: redis-ds

spec:
  selector:
    matchLabels:
      name: redis-ds

  template:
    metadata:
      labels:
        name: redis-ds
    spec:
      containers:
        - image: redis:5-alpine
          name: redis
          ports:
            - containerPort: 6379
</code></pre><p><strong>污点（taint）和容忍度（toleration</strong></p><p>Master默认不跑应用，需要配置taint和toleration来配置才能在master上运行pod。
方法一：去掉master节点的taint，末尾的-就是去除
kubectl taint node master node-role.kubernetes.io/master:NoSchedule-</p><p>方法二：在创建节点的时候，添加toleration</p><pre tabindex=0><code>tolerations:
- key: node-role.kubernetes.io/master
  effect: NoSchedule
  operator: Exists
</code></pre><h4 id=静态pod>静态Pod<a hidden class=anchor aria-hidden=true href=#静态pod>#</a></h4><p>默认在/etc/kubernetes/manifests下，Kubernetes 的 4 个核心组件 apiserver、etcd、scheduler、controller-manager都以静态 Pod 的形式存在的</p><h3 id=service>Service<a hidden class=anchor aria-hidden=true href=#service>#</a></h3><p>用于服务发现和负载均衡，通过iptables实现。</p><p>生成yaml文件命令：kubectl expose deploy ngx-dep &ndash;port=80 &ndash;target-port=80 &ndash;dry-run=client -o yaml</p><pre tabindex=0><code>apiVersion: v1
kind: Service
metadata:
  name: ngx-svc

spec:
  selector:
    app: ngx-dep

  ports:
    - port: 80 # 外部端口
      targetPort: 80 # 内部端口
      protocol: TCP # 协议
  
  type：ClusterIP # 还有“ExternalName”、“LoadBalancer”、&#34;NodePort&#34;，“NodePort”类型会创建一个专用映射端口，外部可访问
</code></pre><p><strong>namespace</strong></p><p>kubectl get ns查看所有的namespace，默认情况下所有API对象都在defalult namespace下。</p><p>基于 DNS 插件，可以以域名形式访问Service，Service对象的域名形式为：”对象. 名字空间.svc.cluster.local“，或者“对象. 名字空间”、“对象名”</p><h3 id=ingress>Ingress<a hidden class=anchor aria-hidden=true href=#ingress>#</a></h3><p>Ingress是流量的总入口，统管集群的进出口数据，以及负载均衡（七层，Service是四层负载均衡）</p><h4 id=ingress-controller>Ingress Controller<a hidden class=anchor aria-hidden=true href=#ingress-controller>#</a></h4><p>Ingress Controller是Ingress资源的实际执行者，负责监听Ingress资源的变化并根据规则配置负载均衡器。
常用的Ingress Controller实现有Nginx Ingress Controller。</p><p>工作原理：</p><ol><li>用户创建Ingress资源定义路由规则</li><li>Ingress Controller持续监听API Server中的Ingress资源变化</li><li>当Ingress资源发生变化时，Controller更新其内部的负载均衡配置</li><li>外部流量通过Ingress Controller访问集群内的服务</li></ol><h4 id=ingreeclass>IngreeClass<a hidden class=anchor aria-hidden=true href=#ingreeclass>#</a></h4><p>用于定义 Ingress 控制器的类型和配置，解决了集群中存在多个 Ingress 控制器时的路由冲突问题。</p><p><img src=/images/kubernetes/Ingress.png alt=Ingress></p><h3 id=persistentvolume>PersistentVolume<a hidden class=anchor aria-hidden=true href=#persistentvolume>#</a></h3><p>用于管理存储资源，属于系统资源，与Node平级，Pod只拥有其使用权</p><ul><li>PersistentVolumeClaim，简称 PVC，用来向Kubernetes申请存储资源，由Pod使用，代表Pod向系统申请PV，申请成功后会将PV和PVC绑定（bind）</li><li>StorageClass，在PV和PVC之间，帮助PVC找到合适的PV。</li></ul><p>PV的yaml：</p><pre tabindex=0><code>apiVersion: v1
kind: PersistentVolume
metadata:
  name: host-10m-pv
spec:
  storageClassName: host-test # 对应StorageClass
  # 访问模式，ReadWriteOnce（可读可写，只能被一个Pod使用）、ReadOnlyMany（只可读，可以被多个Pod使用）、ReadWriteMany（可读可写，可以被多个Pod使用）
  accessModes: 
    - ReadWriteOnce
  capacity: # 存储容量
    storage: 10Mi
  hostPath: # 存储卷的本地路径，在节点上创建的目录
    path: /tmp/host-10m-pv/
</code></pre><p>PVC的yaml：</p><pre tabindex=0><code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: host-5m-pvc
spec:
  storageClassName: host-test
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Mi # 请求的存储容量
</code></pre><p>在Pod中使用PVC：</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: host-pvc-pod
spec:
  volumes:
    - name: host-pvc-vol
      persistentVolumeClaim:
        claimName: host-5m-pvc # 绑定PVC
  containers:
    - name: ngx-pvc-pod
      image: nginx:alpine
      ports:
        - containerPort: 80
      volumeMounts:
        - name: host-pvc-vol
          mountPath: /tmp
</code></pre><p>HostPath 是最简单的一种 PV，数据存储在节点本地，速度快但不能跟随 Pod 迁移</p><p>要实现跨节点数据共享，需要使用NFS或者Ceph等网络存储系统</p><pre tabindex=0><code># NFS PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-1g-pv
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  capacity:
    storage: 1Gi
  nfs:
    path: /tmp/nfs/1g-pv # 需事先创建好
    server: 192.168.10.208
</code></pre><p>以上PV都是静态存储卷，需要手动创建，而Provisioner可以自动管理、创建PV，是动态存储卷。Provisioner以Pod形式运行，部署Provisioner有三个yaml文件，rbac.yaml、class.yaml和deployment.yaml。</p><p>使用Provisioner时就不需要定义PV了，只需要在PVC中指定StorageClass，StorageClass再关联到Provisioner。
StorageClass的yaml：</p><pre tabindex=0><code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-client
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # Provisioner名称
parameters:
  archiveOnDelete: &#34;false&#34; # &#34;false&#34;为自动回收存储空间
</code></pre><h3 id=statefulset>StatefulSet<a hidden class=anchor aria-hidden=true href=#statefulset>#</a></h3><p>Deployment只能管理无状态的应用，StatefulSet管理有状态的，可以看作Deployment的特例。</p><p>StatefulSet启动的Pod是有顺序的，名字为XXXX-0, XXXX-1等，可以根据编号来决定依赖关系，</p><p>StatefulSet的yaml：</p><pre tabindex=0><code>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-sts
spec:
  serviceName: redis-svc # 和Service中metadata.name一致

  volumeClaimTemplates: # PVC，为每个Pod自动创建PVC
  - metadata:
      name: redis-100m-pvc
    spec:
      storageClassName: nfs-client
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 100Mi

  replicas: 2
  selector:
    matchLabels:
      app: redis-sts
  template:
    metadata:
      labels:
        app: redis-sts
    spec:
      containers:
        - image: redis:5-alpine
          name: redis
          ports:
          - containerPort: 6379

          volumeMounts:
          - name: redis-100m-pvc
            mountPath: /data
</code></pre><p>写 Service 对象的时候要小心一些，metadata.name 必须和StatefulSet 里的 serviceName 相同，selector 里的标签也必须和 StatefulSet 里的一致。StatefulSet创建的Pod的域名和其他Pod不同，格式为“Pod名.服务名.名字空间.svc.cluster.local”，简写”Pod名.服务名“。</p><h3 id=namespace-1>namespace<a hidden class=anchor aria-hidden=true href=#namespace-1>#</a></h3><p>namespace是Kubernetes中的一个概念，可以将集群切分成多个区域，同时namespace也是一种API对象，简称ns。Kubernetes中默认有四个namespace：default、kube-system、kube-public、kube-node-lease。要将API对象放入指定namespace中可以在metadata里添加一个namespace字段。</p><h3 id=resourcequota>ResourceQuota<a hidden class=anchor aria-hidden=true href=#resourcequota>#</a></h3><p>用于为namespace进行资源配额，yaml文件示例：</p><pre tabindex=0><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-qt
  namespace: dev-ns
spec:
  hard: # 硬性全局限制
    requests.cpu: 10 # 10个CPU
    requests.memory: 10Gi
    limits.cpu: 10
    limits.memory: 20Gi
    requests.storage: 100Gi
    persistentvolumeclaims: 100
    pods: 100
    configmaps: 100
    secrets: 100
    services: 10
    count/jobs.batch: 1
    count/cronjobs.batch: 1
    count/deployments.apps: 1
</code></pre><p>在添加资源配额后，要求所有在里面运行的 Pod 都必须用字段 resources 声明资源需求，否则就无法创建。可以用LimitRange自动为Pod加上资源限制，LimitRange的yaml：</p><pre tabindex=0><code>apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: dev-ns
spec:
  limits:
    - type: Container
      defaultRequest:
        cpu: 200m
        memory: 50Mi
      default:
        cpu: 500m
        memory: 100Mi
    - type: Pod
      max:
        cpu: 800m
        memory: 200Mi
</code></pre><h3 id=horizontalpodautoscaler>HorizontalPodAutoscaler<a hidden class=anchor aria-hidden=true href=#horizontalpodautoscaler>#</a></h3><p>基于Metrics Server，它从 Metrics Server 获取当前应用的运行指标，主要是 CPU 使用率，再依据预定的策略增加或者减少 Pod 的数量
yaml:</p><pre tabindex=0><code>apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: ngx-hpa
spec:
  maxReplicas: 10
  minReplicas: 2
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ngx-hpa-dep
  targetCPUUtilizationPercentage: 5
</code></pre><p>在Deployment的spec下一定要写resources字段，否则无法实现自动扩缩容</p><h4 id=metrics-server>Metrics Server<a hidden class=anchor aria-hidden=true href=#metrics-server>#</a></h4><p>Metrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具，它定时从所有节点的 kubelet 里采集信息,安装Metrics Server后可以使用kubectl top查看资源使用情况</p><h4 id=prometheus>Prometheus<a hidden class=anchor aria-hidden=true href=#prometheus>#</a></h4><p>相对于Metrics Server，Prometheus的指标更全一些。Prometheus 是云原生监控领域的“事实标准”，用 PromQL 语言来查询数据，配合 Grafana可以展示直观的图形界面，方便监控。</p><h2 id=kubuadm一键部署>kubuadm——一键部署<a hidden class=anchor aria-hidden=true href=#kubuadm一键部署>#</a></h2><p>使用方法：</p><pre tabindex=0><code># 创建一个 Master 节点
$ kubeadm init
 
# 将一个 Node 节点加入到当前集群中
$ kubeadm join &lt;Master 节点的 IP 和端口 &gt;
</code></pre><p>Kubernetes在部署时，它的每一个组件都是一个需要被执行的、单独的二进制文件。所以部署十分麻烦，kubeadm的方案是把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。</p><h2 id=滚动更新>滚动更新<a hidden class=anchor aria-hidden=true href=#滚动更新>#</a></h2><p>Kubernetes中rsion使用Pod模版的hash值来作为版本号。
要实现滚动更新，主要有以下几个命令：</p><ul><li>kubectl apply -f xxx.yaml,修改yaml文件后应用即可，Kubernetes会进行滚动更新（逐步的创建新版本Pod，删除旧版本Pod，最后只剩新版本Pod，旧版本Pod全部删除）</li><li>kubectl rollout pause暂停更新</li><li>kubectl rollout resume恢复更新</li><li>kubectl rollout history查看版本历史，加上参数&ndash;revision x，查看指定版本详细信息</li><li>kubectl rollout undo，回滚到上一个版本，&ndash;to-revision参数指定版本</li><li>Deployment 的 metadata添加字段annotations类似label，但是label用于外部对象，annotations用于内部对象，其中kubernetes.io/change-cause是更新版本说明。</li></ul><h2 id=网络模型>网络模型<a hidden class=anchor aria-hidden=true href=#网络模型>#</a></h2><p>Docker有三种网络模式：null、host 和 bridge，但是只适用于单机，Kubernetes提出了自己的网络模型IP-per-pod：</p><ul><li>集群里的每个 Pod 都会有唯一的一个 IP 地址。</li><li>Pod 里的所有容器共享这个 IP 地址。</li><li>集群里的所有 Pod 都属于同一个网段。</li><li>Pod直接可以基于IP地址访问另一个Pod，不需要做网络地址转换（NAT）</li></ul><h3 id=cnicontainer-networking-interface>CNI（Container Networking Interface）<a hidden class=anchor aria-hidden=true href=#cnicontainer-networking-interface>#</a></h3><p>CNI 为网络插件定义了一系列通用接口，CNI插件大致分为三种：</p><ul><li>Overlay：它构建了一个工作在真实底层网络之上的“逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。</li><li>Route：在底层网络之上工作，但它没有封包和拆包，而是使用系统内置的路由功能来实现Pod 跨主机通信。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了</li><li>Underlay： 直接用底层网络来实现 CNI，也就是说 Pod 和宿主机都在一个网络里，Pod和宿主机是平等的。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。</li></ul><p>常用的CNI插件：</p><ul><li>Flannel：最早是Overlay模式，后来用 Host-Gateway 技术支持了 Route 模式，简单易用、但是性能不是很好</li><li>Calico：Route模式使用BGP 协议来维护路由信息，性能要比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能</li><li>Cilium：同时支持 Overlay 模式和 Route 模式，它的特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，所以性能很高，可以灵活实现各种功能</li></ul><p>参考资料：
《Kubernetes入门实战课》— 罗剑锋（极客时间）</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://oyzg.github.io/tags/%E8%BF%90%E7%BB%B4/>运维</a></li><li><a href=https://oyzg.github.io/tags/docker/>Docker</a></li><li><a href=https://oyzg.github.io/tags/k8s/>K8s</a></li><li><a href=https://oyzg.github.io/tags/kubernetes/>Kubernetes</a></li><li><a href=https://oyzg.github.io/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/>云原生</a></li></ul><nav class=paginav><a class=next href=https://oyzg.github.io/archives/%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92/><span class=title>Next Page »</span><br><span>《认知觉醒》</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on twitter" href="https://twitter.com/intent/tweet/?text=Kubernetes&amp;url=https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f&amp;hashtags=%e8%bf%90%e7%bb%b4%2cDocker%2cK8s%2cKubernetes%2c%e4%ba%91%e5%8e%9f%e7%94%9f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f&amp;title=Kubernetes&amp;summary=Kubernetes&amp;source=https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on reddit" href="https://reddit.com/submit?url=https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f&title=Kubernetes"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on whatsapp" href="https://api.whatsapp.com/send?text=Kubernetes%20-%20https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on telegram" href="https://telegram.me/share/url?text=Kubernetes&amp;url=https%3a%2f%2foyzg.github.io%2farchives%2fkubernetes%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://oyzg.github.io/>oyzg's note</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g>
<svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a><script defer src=/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY=" onload=hljs.initHighlightingOnLoad()></script><script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>