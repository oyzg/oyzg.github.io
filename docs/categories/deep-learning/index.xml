<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on My Favorite</title><link>https://oyzg.github.io/categories/deep-learning/</link><description>Recent content in Deep Learning on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 21 Apr 2021 14:31:58 +0800</lastBuildDate><atom:link href="https://oyzg.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Keras: Multiple outputs and multiple losses</title><link>https://oyzg.github.io/archives/keras-multiple-outputs-and-multiple-losses/</link><pubDate>Wed, 21 Apr 2021 14:31:58 +0800</pubDate><guid>https://oyzg.github.io/archives/keras-multiple-outputs-and-multiple-losses/</guid><description>A multiple outputs model has several fully connected layers for output. Each layer is responsible for performing a specific task and has a respective loss function and activation function.</description></item><item><title>Multi-label classification with Keras</title><link>https://oyzg.github.io/archives/multi-label-classification-with-keras/</link><pubDate>Wed, 21 Apr 2021 14:06:23 +0800</pubDate><guid>https://oyzg.github.io/archives/multi-label-classification-with-keras/</guid><description>The post introduces how to do multi-label classification with Keras and implements a simple model with custom SmallerVGGNet can predict both color and clothing type of input image.</description></item><item><title>Exploring Spatiotemporal Features for Activity Classifications in Films</title><link>https://oyzg.github.io/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/</link><pubDate>Wed, 21 Apr 2021 13:09:01 +0800</pubDate><guid>https://oyzg.github.io/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/</guid><description>The post introduces several experiments about activity classification based on three main architectures: 3D CNN, ConvLSTM2D, and a pipeline of pre-trained CNN-LSTM.</description></item><item><title>Video Classification in Keras using ConvLSTM</title><link>https://oyzg.github.io/archives/video-classification-keras-convlstm/</link><pubDate>Tue, 20 Apr 2021 19:37:11 +0800</pubDate><guid>https://oyzg.github.io/archives/video-classification-keras-convlstm/</guid><description>This article will explain the Deep Learning based solution of the Video Classification task in Keras using ConvLSTM layers.</description></item><item><title>Introduction to Video Classification and Human Activity Recognition</title><link>https://oyzg.github.io/archives/introduction-to-video-classification-and-human-activity-recognition/</link><pubDate>Fri, 16 Apr 2021 14:43:15 +0800</pubDate><guid>https://oyzg.github.io/archives/introduction-to-video-classification-and-human-activity-recognition/</guid><description>In this post, we will learn about Video Classification. We will go over a number of approaches to make a video classifier for Human Activity Recognition. Basically, you will learn video classification and human activity recognition.</description></item><item><title>Understanding PyTorch With an Example: A step by step Tutorial</title><link>https://oyzg.github.io/archives/understanding-pytorch-with-an-example-a-step-by-step-tutorial/</link><pubDate>Tue, 06 Apr 2021 21:19:40 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-pytorch-with-an-example-a-step-by-step-tutorial/</guid><description>In this post, The author will guide you through the main reasons why PyTorch makes it much easier and more intuitive to build a Deep Learning model in Python — autograd, dynamic computation graph, model classes and more - and He will also show you how to avoid some common pitfalls and errors along the way.</description></item><item><title>Implementing a LSTM From Scratch With Numpy</title><link>https://oyzg.github.io/archives/implementing-a-lstm-from-scratch-with-numpy/</link><pubDate>Mon, 29 Mar 2021 15:25:04 +0800</pubDate><guid>https://oyzg.github.io/archives/implementing-a-lstm-from-scratch-with-numpy/</guid><description>In this post, The author implement a simple character-level LSTM using Numpy. It is trained in batches with the Adam optimiser and learns basic words after just a few training iterations.</description></item><item><title>Deriving the Backpropagation Equations for a LSTM</title><link>https://oyzg.github.io/archives/deriving-the-backpropagation-equations-for-a-lstm/</link><pubDate>Mon, 29 Mar 2021 15:22:46 +0800</pubDate><guid>https://oyzg.github.io/archives/deriving-the-backpropagation-equations-for-a-lstm/</guid><description>In this post, The author derive the backpropagation equations for a LSTM cell in vectorised form.</description></item><item><title>The Illustrated Transformer</title><link>https://oyzg.github.io/archives/the-illustrated-transformer/</link><pubDate>Fri, 26 Mar 2021 10:47:26 +0800</pubDate><guid>https://oyzg.github.io/archives/the-illustrated-transformer/</guid><description>A pretty detailed illustration about the transformers: I recommend it for you if you are looking for an explained article about the paper Attention is all you need, or you want to learn the attention mechanism.</description></item><item><title>Attention? Attention!</title><link>https://oyzg.github.io/archives/attention-attention/</link><pubDate>Wed, 24 Mar 2021 13:14:48 +0800</pubDate><guid>https://oyzg.github.io/archives/attention-attention/</guid><description>Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.</description></item><item><title>A Simple Overview of RNN, LSTM and Attention Mechanism</title><link>https://oyzg.github.io/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</link><pubDate>Mon, 22 Mar 2021 14:57:23 +0800</pubDate><guid>https://oyzg.github.io/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</guid><description>Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained.</description></item><item><title>Understanding GRU Networks</title><link>https://oyzg.github.io/archives/understanding-gru-networks/</link><pubDate>Thu, 18 Mar 2021 17:24:49 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-gru-networks/</guid><description>In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network.</description></item><item><title>Understanding Input and Output Shapes in LSTM</title><link>https://oyzg.github.io/archives/understanding-input-and-output-shapes-in-lstm/</link><pubDate>Wed, 17 Mar 2021 19:25:24 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-input-and-output-shapes-in-lstm/</guid><description>Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM.</description></item><item><title>Understanding the Number of Parameter Used in LSTM Network</title><link>https://oyzg.github.io/archives/understanding-the-number-of-parameter-used-in-lstm-network/</link><pubDate>Tue, 16 Mar 2021 17:57:54 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-the-number-of-parameter-used-in-lstm-network/</guid><description>It explained clearly how to calculate the number of trainable parameters used in the LSTM network.</description></item><item><title>[Kaggle Summary] Quora Insincere Questions Classification</title><link>https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/</link><pubDate>Fri, 12 Mar 2021 17:13:34 +0800</pubDate><guid>https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/</guid><description>The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.</description></item><item><title>An Example of Classification on Imbalanced Data with Tensorflow</title><link>https://oyzg.github.io/archives/an-example-of-classification-on-imbalanced-data-with-tensorflow/</link><pubDate>Wed, 10 Mar 2021 14:57:23 +0800</pubDate><guid>https://oyzg.github.io/archives/an-example-of-classification-on-imbalanced-data-with-tensorflow/</guid><description>This tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the Credit Card Fraud Detection dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use Keras to define the model and class weights to help the model learn from the imbalanced data.</description></item><item><title>How do LSTM Networks Solve the Problem of Vanishing Gradients</title><link>https://oyzg.github.io/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</link><pubDate>Mon, 08 Mar 2021 13:35:16 +0800</pubDate><guid>https://oyzg.github.io/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/</guid><description>LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate’s activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.</description></item><item><title>Understanding LSTM Networks</title><link>https://oyzg.github.io/archives/understanding-lstm-networks/</link><pubDate>Mon, 08 Mar 2021 13:02:42 +0800</pubDate><guid>https://oyzg.github.io/archives/understanding-lstm-networks/</guid><description>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp;amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</description></item><item><title>Illustrated Guide to LSTM's and GRU's: A step by step Explanation</title><link>https://oyzg.github.io/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</link><pubDate>Sat, 06 Mar 2021 22:17:22 +0800</pubDate><guid>https://oyzg.github.io/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/</guid><description>In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.</description></item><item><title>What is a Transformer?</title><link>https://oyzg.github.io/archives/what-is-a-transformer/</link><pubDate>Sat, 06 Mar 2021 13:27:25 +0800</pubDate><guid>https://oyzg.github.io/archives/what-is-a-transformer/</guid><description>An introduction to transformers and sequence-to-sequence learning for machine learning</description></item><item><title>Analyzing Handwritten Digit Model with Tensorflow Line by Line</title><link>https://oyzg.github.io/archives/analyzing-handwritten-digits-model-with-tensorflow-line-by-line/</link><pubDate>Tue, 02 Mar 2021 15:51:10 +0800</pubDate><guid>https://oyzg.github.io/archives/analyzing-handwritten-digits-model-with-tensorflow-line-by-line/</guid><description>Let&amp;rsquo;s do a line-by-line analysis of this deep learning model and truly understand what&amp;rsquo;s going on. This model identifies handwritten digits. It&amp;rsquo;s one of the classic examples of machine learning applied to computer vision</description></item></channel></rss>