<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Natural Language Processing | My Favorite</title><meta name=keywords content><meta name=description content><meta name=author content><link rel=canonical href=https://oyzg.github.io/categories/natural-language-processing/><link href=/assets/css/stylesheet.min.d597aab94c71b49b163b6ac67c7958079589ea1a0b7300e8a0014e1d2023b95e.css integrity="sha256-1ZequUxxtJsWO2rGfHlYB5WJ6hoLcwDooAFOHSAjuV4=" rel="preload stylesheet" as=style><link rel=icon href=https://oyzg.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://oyzg.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://oyzg.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://oyzg.github.io/apple-touch-icon.png><link rel=mask-icon href=https://oyzg.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.96.0"><link rel=alternate type=application/rss+xml href=https://oyzg.github.io/categories/natural-language-processing/index.xml><meta property="og:title" content="Natural Language Processing"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://oyzg.github.io/categories/natural-language-processing/"><meta property="og:updated_time" content="2021-03-29T15:25:04+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Natural Language Processing"><meta name=twitter:description content></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://oyzg.github.io/ accesskey=h title="My Favorite (Alt + H)">My Favorite</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://oyzg.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://oyzg.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://oyzg.github.io/series/ title=Series><span>Series</span></a></li><li><a href=https://oyzg.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://oyzg.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><header class=page-header><h1>Natural Language Processing</h1></header><article class="post-entry tag-entry"><header class=entry-header><h2>Implementing a LSTM From Scratch With Numpy</h2></header><section class=entry-content><p>In this post, The author implement a simple character-level LSTM using Numpy. It is trained in batches with the Adam optimiser and learns basic words after just a few training iterations....</p></section><footer class=entry-footer>March 29, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to Implementing a LSTM From Scratch With Numpy" href=https://christinakouridi.blog/2019/06/20/vanilla-lstm-numpy/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Deriving the Backpropagation Equations for a LSTM</h2></header><section class=entry-content><p>In this post, The author derive the backpropagation equations for a LSTM cell in vectorised form....</p></section><footer class=entry-footer>March 29, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to Deriving the Backpropagation Equations for a LSTM" href=https://christinakouridi.blog/2019/06/19/backpropagation-lstm/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>A Simple Overview of RNN, LSTM and Attention Mechanism</h2></header><section class=entry-content><p>Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained....</p></section><footer class=entry-footer>March 22, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to A Simple Overview of RNN, LSTM and Attention Mechanism" href=https://medium.com/swlh/a-simple-overview-of-rnn-lstm-and-attention-mechanism-9e844763d07b></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Understanding GRU Networks</h2></header><section class=entry-content><p>In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network....</p></section><footer class=entry-footer>March 18, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to Understanding GRU Networks" href=https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Understanding Input and Output Shapes in LSTM</h2></header><section class=entry-content><p>Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM....</p></section><footer class=entry-footer>March 17, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to Understanding Input and Output Shapes in LSTM" href=https://shiva-verma.medium.com/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Understanding the Number of Parameter Used in LSTM Network</h2></header><section class=entry-content><p>It explained clearly how to calculate the number of trainable parameters used in the LSTM network....</p></section><footer class=entry-footer>March 16, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to Understanding the Number of Parameter Used in LSTM Network" href=https://medium.com/deep-learning-with-keras/lstm-understanding-the-number-of-parameters-c4e087575756></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>An Introduction of Word2Vec and implementation using Tensorflow</h2></header><section class=entry-content><p>Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks....</p></section><footer class=entry-footer>March 16, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to An Introduction of Word2Vec and implementation using Tensorflow" href=https://www.tensorflow.org/tutorials/text/word2vec></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>[Kaggle Summary] Quora Insincere Questions Classification</h2></header><section class=entry-content><p>The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec....</p></section><footer class=entry-footer>March 12, 2021&nbsp;·&nbsp;8 min</footer><a class=entry-link aria-label="post link to [Kaggle Summary] Quora Insincere Questions Classification" href=https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>How do LSTM Networks Solve the Problem of Vanishing Gradients</h2></header><section class=entry-content><p>LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate’s activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process....</p></section><footer class=entry-footer>March 8, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to How do LSTM Networks Solve the Problem of Vanishing Gradients" href=https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577></a></article><article class="post-entry tag-entry"><header class=entry-header><h2>Understanding LSTM Networks</h2></header><section class=entry-content><p>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used....</p></section><footer class=entry-footer>March 8, 2021
&nbsp;&nbsp;↗</footer><a target=_blank class=entry-link aria-label="post link to Understanding LSTM Networks" href=https://colah.github.io/posts/2015-08-Understanding-LSTMs/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://oyzg.github.io/categories/natural-language-processing/page/2/>Next Page »</a></nav></footer></main><footer class=footer><span>&copy; 2022 <a href=https://oyzg.github.io/>My Favorite</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>