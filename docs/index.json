[{"content":"Redis 数据结构 Redis中值是用redisObject保存的,定义如下：\ntypedef struct redisObject { unsigned type:4; // 数据类型，包括String、List等 unsigned encoding:4; // 数据编码方式，实现数据类型所用的数据结构 unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits access time). */ int refcount; // 引用计数 void *ptr; // 指向实际数据 } robj; SDS SDS结构： // sds本质还是char* typedef char *sds; // 除此之外还有sdshdr16、sdshdr32、sdshdr64，通过设置不同的结构头，来灵活保存不同大小的字符串，节省内存空间 // struct __attribute__ ((__packed__))的作用是告诉编译器，不要使用字节对齐的方式，而用紧凑的方式分配内存 struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* 字符串现有长度 */ uint8_t alloc; /* 字符数组分配的空间长度 */ unsigned char flags; /* SDS类型 */ char buf[]; /* 字符数组 */ }; // 新建sds时，会根据长度创建相应的结构体，并返回结构体的指针。sds指向buf部分 sds sdsnewlen(const void *init, size_t initlen) { void *sh; // 指向结构体头部的指针 sds s; // sds类型变量，指向buf的指针 // 新建SDS结构，并分配内存：头部 + 数据 + null终止符 sh = s_malloc(hdrlen + initlen + 1); // s 指向buf部分 s = (char*)sh + hdrlen; // 关键：s指向buf开始位置 // ... 设置结构体字段 ... if (initlen \u0026amp;\u0026amp; init) memcpy(s, init, initlen); //将要传⼊的字符串拷⻉给sds变量s s[initlen] = \u0026#39;\\0\u0026#39;; // 变量s末尾增加\\0，表⽰字符串结束 return s; // 返回指向buf的指针 } // 追加字符串 sds sdscatlen(sds s, const void *t, size_t len) { // 获取目标字符串s的当前长度 size_t curlen = sdslength(s); // 检查+扩容：根据要追加的长度len和目标字符串s的现有长度，判断是否要增加新的空间 s = sdsMakeRoomFor(s, len); if (s == NULL) return NULL; // 将源字符串t中len长度的数据拷贝到目标字符串结尾 memcpy(s + curlen, t, len); // 设置目标字符串的最新长度：拷贝前长度curlen加上拷贝长度 sdsssetlen(s, curlen + len); // 拷贝后，在目标字符串结尾加上\\0 s[curlen + len] = \u0026#39;\\0\u0026#39;; return s; } // 检查+扩容 sds sdsMakeRoomFor(sds s, size_t addlen) { void *sh, *newsh; // 结构体指针 size_t avail = sdsavail(s); // 当前可用空间 size_t len, newlen; // 当前长度和新长度 char type, oldtype = s[-1] \u0026amp; SDS_TYPE_MASK; // 新类型和旧类型 int hdrlen; // 头部长度 /* Return ASAP if there is enough space left. */ if (avail \u0026gt;= addlen) return s; // 扩容 len = sdslen(s); sh = (char*)s-sdsHdrSize(oldtype); newlen = (len+addlen); if (newlen \u0026lt; SDS_MAX_PREALLOC) newlen *= 2; // 小于1MB，翻倍扩容 else newlen += SDS_MAX_PREALLOC; // 大于1MB，增加1MB type = sdsReqType(newlen); /* Don\u0026#39;t use type 5: the user is appending to the string and type 5 is * not able to remember empty space, so sdsMakeRoomFor() must be called * at every appending operation. */ if (type == SDS_TYPE_5) type = SDS_TYPE_8; hdrlen = sdsHdrSize(type); if (oldtype==type) { // 类型不变 newsh = s_realloc(sh, hdrlen+newlen+1); //原地扩容 if (newsh == NULL) return NULL; s = (char*)newsh+hdrlen; } else { // 类型改变，重新分配内存 /* Since the header size changes, need to move the string forward, * and can\u0026#39;t use realloc */ newsh = s_malloc(hdrlen+newlen+1); if (newsh == NULL) return NULL; memcpy((char*)newsh+hdrlen, s, len+1); s_free(sh); s = (char*)newsh+hdrlen; s[-1] = type; sdssetlen(s, len); } sdssetalloc(s, newlen); return s; } #define SDS_HDR(T,s) ((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T)))) #define SDS_TYPE_5_LEN(f) ((f)\u0026gt;\u0026gt;SDS_TYPE_BITS) // 获取长度，通过宏获取结构体指针，访问len变量 static inline size_t sdslen(const sds s) { unsigned char flags = s[-1]; switch(flags\u0026amp;SDS_TYPE_MASK) { case SDS_TYPE_5: return SDS_TYPE_5_LEN(flags); case SDS_TYPE_8: return SDS_HDR(8,s)-\u0026gt;len; case SDS_TYPE_16: return SDS_HDR(16,s)-\u0026gt;len; case SDS_TYPE_32: return SDS_HDR(32,s)-\u0026gt;len; case SDS_TYPE_64: return SDS_HDR(64,s)-\u0026gt;len; } return 0; } Hash Hash是一个kv结构，具有O(1)的查询复杂度，但是当数据量增加，性能会受到哈希冲突和rehash开销的影响。 怎么解决这两个问题？\n哈希冲突：Redis采用链式哈希来解决 rehash开销：Redis采用了渐进式的hash设计，来缓解对系统的性能影响 Hash结构：\n// 链表节点，用来实现链式哈希 typedef struct dictEntry { void *key; union { // 联合体中所有成员都共享这同一块8字节内存 void *val; // 指向实际值 uint64_t u64; int64_t s64; double d; } v; // 当值为整数或双精度浮点数时，就不需要指针指向了，直接存储在结构体中，从而节省内存空间 struct dictEntry *next; } dictEntry; // 哈希表结构 typedef struct dictht { dictEntry **table; // 桶数组，每个元素是一个dictEntry *，所以实际上是一维数组 unsigned long size; // 哈希表的大小 unsigned long sizemask; // 大小掩码，等于size-1，用于快速计算哈希索引：hash \u0026amp; sizemask unsigned long used; // 已经使用的元素个数 } dictht; // Hash结构 typedef struct dict { dictType *type; void *privdata; dictht ht[2]; // 两个哈希表，用于实现渐进式rehash // 正常服务阶段所有kv都写入ht[0]，rehash时kv被迁移到ht[1] // 迁移完成后ht[0]被释放 ，并把ht[1]的地址赋值给ht[0]，ht[1]的表⼤⼩设置为0 long rehashidx; // rehash索引，-1表示没有进行rehash unsigned long iterators; /* number of iterators currently running */ } dict; // 检查是否需要进行rehash // _dictExpandIfNeeded -\u0026gt; _dictKeyIndex -\u0026gt; dictAddRaw -\u0026gt; （dictAdd、 dictRelace、 dicAddorFind） // _dictExpandIfNeeded最后被以上三个方法调用 // dictAdd：⽤来往Hash表中添加⼀个键值对 // dictRelace：⽤来往Hash表中添加⼀个键值对，或者键值对存在时，修改键值对 // dicAddorFind：直接调用dictAddRaw // 所以当添加或者修改键值对都会判断是否需要进行rehash static int _dictExpandIfNeeded(dict *d) { /* Incremental rehashing already in progress. Return. */ if (dictIsRehashing(d)) return DICT_OK; /* 如果容量为0，则进行初始化 */ if (d-\u0026gt;ht[0].size == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE); /* 如果元素个数已经达到h[0]的大小 并且可以进行扩容 或者 负载因子（元素个数/h[0]大小）大于dict_force_resize_ratio（默认为5）*/ /* 简单就是（1）负载因子\u0026gt;=1并且可以扩容（2）负载因子\u0026gt;5满足其一就进行rehash */ if (d-\u0026gt;ht[0].used \u0026gt;= d-\u0026gt;ht[0].size \u0026amp;\u0026amp; (dict_can_resize || d-\u0026gt;ht[0].used/d-\u0026gt;ht[0].size \u0026gt; dict_force_resize_ratio)) { return dictExpand(d, d-\u0026gt;ht[0].used*2); //扩容为原来的两倍 } return DICT_OK; } // server.c，如果处于RDB或者AOF子进程，则不允许进行扩容 void updateDictResizePolicy(void) { if (server.rdb_child_pid == -1 \u0026amp;\u0026amp; server.aof_child_pid == -1) dictEnableResize(); else dictDisableResize(); } /* 扩容大小，会保持2的倍数 */ static unsigned long _dictNextPower(unsigned long size) { unsigned long i = DICT_HT_INITIAL_SIZE; # 默认值：4 if (size \u0026gt;= LONG_MAX) return LONG_MAX + 1LU; while(1) { if (i \u0026gt;= size) return i; i *= 2; } } // rehash过程，n为需要拷贝的桶数量 int dictRehash(dict *d, int n) { int empty_visits = n*10; /* 最大空桶数，如果检查了empty_visits个空桶，就不继续进行rehashl */ if (!dictIsRehashing(d)) return 0; // 进行n个桶的拷贝 while(n-- \u0026amp;\u0026amp; d-\u0026gt;ht[0].used != 0) { dictEntry *de, *nextde; /* Note that rehashidx can\u0026#39;t overflow as we are sure there are more * elements because ht[0].used != 0 */ assert(d-\u0026gt;ht[0].size \u0026gt; (unsigned long)d-\u0026gt;rehashidx); while(d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx] == NULL) { d-\u0026gt;rehashidx++; if (--empty_visits == 0) return 1; } de = d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx]; /* Move all the keys in this bucket from the old to the new hash HT */ while(de) { uint64_t h; nextde = de-\u0026gt;next; // 获取同一个桶中的下一个元素 /* 计算新表中的butket位置 */ h = dictHashKey(d, de-\u0026gt;key) \u0026amp; d-\u0026gt;ht[1].sizemask; de-\u0026gt;next = d-\u0026gt;ht[1].table[h]; d-\u0026gt;ht[1].table[h] = de; d-\u0026gt;ht[0].used--; // 旧表元素个数减1 d-\u0026gt;ht[1].used++; // 新表元素个数加1 de = nextde; // 指向下一个元素 } d-\u0026gt;ht[0].table[d-\u0026gt;rehashidx] = NULL; d-\u0026gt;rehashidx++; } /* 检查整个表是否被迁移完 */ if (d-\u0026gt;ht[0].used == 0) { zfree(d-\u0026gt;ht[0].table); // 释放ht[0] d-\u0026gt;ht[0] = d-\u0026gt;ht[1]; // ht[1]赋给ht[0] _dictReset(\u0026amp;d-\u0026gt;ht[1]); // 重置ht[1] d-\u0026gt;rehashidx = -1; // 重置rehash索引 return 0; } /* More to rehash... */ return 1; } // 每次只迁移一个桶，被dictAddRaw，dictGenericDelete，dictFind，dictGetRandomKey，dictGetSomeKeys调用 // dictAddRaw是添加键值对，dictGenericDelete是删除键值对 // dictFind，dictGetRandomKey，dictGetSomeKeys都是查询键值对 static void _dictRehashStep(dict *d) { if (d-\u0026gt;iterators == 0) dictRehash(d,1); } // 使用的hash函数 SipHash，速度快，安全 uint64_t dictGenHashFunction(const void *key, int len) { return siphash(key,len,dict_hash_function_seed); } intset // 结构体定义如下 typedef struct intset { uint32_t encoding; // 编码方式 uint32_t length; // 元素个数 int8_t contents[]; // 整数数组 } intset; // 数组元素的类型取决于encoding属性的值 #define INTSET_ENC_INT16 (sizeof(int16_t)) #define INTSET_ENC_INT32 (sizeof(int32_t)) #define INTSET_ENC_INT64 (sizeof(int64_t)) /* Return the required encoding for the provided value. */ static uint8_t _intsetValueEncoding(int64_t v) { if (v \u0026lt; INT32_MIN || v \u0026gt; INT32_MAX) return INTSET_ENC_INT64; else if (v \u0026lt; INT16_MIN || v \u0026gt; INT16_MAX) return INTSET_ENC_INT32; else return INTSET_ENC_INT16; } 整数集合会有一个升级规则，就是当我们将一个新元素加入到整数集合里面，如果新元素的类型（int32_t）比整数集合现有所有元素的类型（int16_t）都要长时，整数集合需要先进行升级，也就是按新元素的类型（int32_t）扩展 contents 数组的空间大小，然后才能将新元素加入到整数集合里，当然升级的过程中，也要维持整数集合的有序性。\n整数集合升级的过程不会重新分配一个新类型的数组，而是在原本的数组上扩展空间，然后在将每个元素按间隔类型大小分割，如果 encoding 属性值为 INTSET_ENC_INT16，则每个元素的间隔就是 16 位。\n// 添加元素 intset *intsetAdd(intset *is, int64_t value, uint8_t *success) { uint8_t valenc = _intsetValueEncoding(value); uint32_t pos; if (success) *success = 1; /* Upgrade encoding if necessary. If we need to upgrade, we know that * this value should be either appended (if \u0026gt; 0) or prepended (if \u0026lt; 0), * because it lies outside the range of existing values. */ if (valenc \u0026gt; intrev32ifbe(is-\u0026gt;encoding)) { // 如果新添加的元素类型比encoding大，就需要升级 /* This always succeeds, so we don\u0026#39;t need to curry *success. */ return intsetUpgradeAndAdd(is,value); } else { /* Abort if the value is already present in the set. * This call will populate \u0026#34;pos\u0026#34; with the right position to insert * the value when it cannot be found. */ if (intsetSearch(is,value,\u0026amp;pos)) { if (success) *success = 0; return is; } is = intsetResize(is,intrev32ifbe(is-\u0026gt;length)+1); if (pos \u0026lt; intrev32ifbe(is-\u0026gt;length)) intsetMoveTail(is,pos,pos+1); } _intsetSet(is,pos,value); is-\u0026gt;length = intrev32ifbe(intrev32ifbe(is-\u0026gt;length)+1); return is; } // 升级操作 /* Upgrades the intset to a larger encoding and inserts the given integer. */ static intset *intsetUpgradeAndAdd(intset *is, int64_t value) { uint8_t curenc = intrev32ifbe(is-\u0026gt;encoding); uint8_t newenc = _intsetValueEncoding(value); // 新元素的类型 int length = intrev32ifbe(is-\u0026gt;length); int prepend = value \u0026lt; 0 ? 1 : 0; /* First set new encoding and resize */ is-\u0026gt;encoding = intrev32ifbe(newenc); is = intsetResize(is,intrev32ifbe(is-\u0026gt;length)+1); // 扩容 /* Upgrade back-to-front so we don\u0026#39;t overwrite values. * Note that the \u0026#34;prepend\u0026#34; variable is used to make sure we have an empty * space at either the beginning or the end of the intset. */ while(length--) // 从后往前迁移，避免前向覆盖 _intsetSet(is,length+prepend,_intsetGetEncoded(is,length,curenc)); /* Set the value at the beginning or the end. */ // 插入新元素 if (prepend) _intsetSet(is,0,value); else _intsetSet(is,intrev32ifbe(is-\u0026gt;length),value); is-\u0026gt;length = intrev32ifbe(intrev32ifbe(is-\u0026gt;length)+1); return is; } // 扩容，会先尝试原地扩展，如果无法扩展才会分配新内存，拷贝旧数据 static intset *intsetResize(intset *is, uint32_t len) { uint32_t size = len*intrev32ifbe(is-\u0026gt;encoding); is = zrealloc(is,sizeof(intset)+size); return is; } skiplist 在Redis中只有Zset用到了跳表。跳表是在链表基础上改进的，是一种多层链表，支持O(logN)的查询时间复杂度，结构如下： // 跳表节点 typedef struct zskiplistNode { sds ele; // 值 double score; // 权重 struct zskiplistNode *backward; // 指向前一个节点 struct zskiplistLevel { // level数组 struct zskiplistNode *forward; // 指向下一个节点 unsigned long span; // 跨度，当前节点到下一个节点的间隔，用于计算这个节点在跳表中的排位，因为Zset是有序的 } level[]; } zskiplistNode; // 跳表结构 typedef struct zskiplist { struct zskiplistNode *header, *tail; // 头尾节点 unsigned long length; // 元素个数 int level; // 最大层数 } zskiplist; // 随机层数 int zslRandomLevel(void) { int level = 1; while ((random()\u0026amp;0xFFFF) \u0026lt; (ZSKIPLIST_P * 0xFFFF)) // 随机生成一个数，如果小于0.25，就加一层 level += 1; return (level\u0026lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL; // 最大层数64 } // 插入元素 zskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) { zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; // 记录各层插入点的前驱 unsigned int rank[ZSKIPLIST_MAXLEVEL]; // 记录到达各层当前位置时的跨度 int i, level; serverAssert(!isnan(score)); x = zsl-\u0026gt;header; // 从头节点开始 for (i = zsl-\u0026gt;level-1; i \u0026gt;= 0; i--) { // 自顶向下遍历层 /* store rank that is crossed to reach the insert position */ rank[i] = i == (zsl-\u0026gt;level-1) ? 0 : rank[i+1]; // 顶层的 rank 初始化为 0，其它层继承上一层的 rank while (x-\u0026gt;level[i].forward \u0026amp;\u0026amp; // 在第i层向前走 (x-\u0026gt;level[i].forward-\u0026gt;score \u0026lt; score || (x-\u0026gt;level[i].forward-\u0026gt;score == score \u0026amp;\u0026amp; sdscmp(x-\u0026gt;level[i].forward-\u0026gt;ele,ele) \u0026lt; 0))) { rank[i] += x-\u0026gt;level[i].span; // 层 i 的跨度加上 1 x = x-\u0026gt;level[i].forward; } update[i] = x; } /* we assume the element is not already inside, since we allow duplicated * scores, reinserting the same element should never happen since the * caller of zslInsert() should test in the hash table if the element is * already inside or not. */ level = zslRandomLevel(); // 随机层数 if (level \u0026gt; zsl-\u0026gt;level) { // 若新层高超出现有最大层高，则需要扩层 for (i = zsl-\u0026gt;level; i \u0026lt; level; i++) { // 对新增的每一层做初始化 rank[i] = 0; update[i] = zsl-\u0026gt;header; update[i]-\u0026gt;level[i].span = zsl-\u0026gt;length; } zsl-\u0026gt;level = level; } x = zslCreateNode(level,score,ele); // 创建节点 for (i = 0; i \u0026lt; level; i++) { // 插入节点 x-\u0026gt;level[i].forward = update[i]-\u0026gt;level[i].forward; update[i]-\u0026gt;level[i].forward = x; /* update span covered by update[i] as x is inserted here */ x-\u0026gt;level[i].span = update[i]-\u0026gt;level[i].span - (rank[0] - rank[i]); update[i]-\u0026gt;level[i].span = (rank[0] - rank[i]) + 1; } /* increment span for untouched levels */ for (i = level; i \u0026lt; zsl-\u0026gt;level; i++) { // 对新节点未覆盖的更高层，只需前驱跨度加 1 update[i]-\u0026gt;level[i].span++; } x-\u0026gt;backward = (update[0] == zsl-\u0026gt;header) ? NULL : update[0]; // 设置 backward 指针 if (x-\u0026gt;level[0].forward) x-\u0026gt;level[0].forward-\u0026gt;backward = x; else zsl-\u0026gt;tail = x; zsl-\u0026gt;length++; return x; } ziplist ZipList没有结构体定义，因为它是一块连续的内存空间，使用不同的编码来保存数据。 #define ZIPLIST_HEADER_SIZE (sizeof(uint32_t)*2+sizeof(uint16_t)) // 列表头大小 存储列表长度(32bits)、列表尾偏移量(32bits)、列表元素个数(16bits) #define ZIPLIST_END_SIZE (sizeof(uint8_t)) // 列表尾大小 #define ZIP_END 255 /* Special \u0026#34;end of ziplist\u0026#34; entry. */ // 列表尾字节内容 255(8bits) /* 创建一个空的ziplist. */ unsigned char *ziplistNew(void) { unsigned int bytes = ZIPLIST_HEADER_SIZE+ZIPLIST_END_SIZE; unsigned char *zl = zmalloc(bytes); ZIPLIST_BYTES(zl) = intrev32ifbe(bytes); ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE); ZIPLIST_LENGTH(zl) = 0; zl[bytes-1] = ZIP_END; return zl; } // 对prelen进行编码，这里根据254进行划分的原因是255被用作结束标记，254是除255外最大的 unsigned int zipStorePrevEntryLength(unsigned char *p, unsigned int len) { if (p == NULL) { return (len \u0026lt; ZIP_BIG_PREVLEN) ? 1 : sizeof(len)+1; } else { if (len \u0026lt; ZIP_BIG_PREVLEN) { // ZIP_BIG_PREVLEN 为254 p[0] = len; // 如果小于254 则使用1字节编码 return 1; } else { return zipStorePrevEntryLengthLarge(p,len); // 使用5字节编码 } } } int zipStorePrevEntryLengthLarge(unsigned char *p, unsigned int len) { if (p != NULL) { p[0] = ZIP_BIG_PREVLEN; // p[0]设置为254，后面4个字节存储前一个值的长度 memcpy(p+1,\u0026amp;len,sizeof(len)); memrev32ifbe(p+1); } return 1+sizeof(len); } // 对encoding进行编码，针对整数和字符串采用不同的编码方式 unsigned int zipStoreEntryEncoding(unsigned char *p, unsigned char encoding, unsigned int rawlen) { unsigned char len = 1, buf[5]; if (ZIP_IS_STR(encoding)) { // 如果是字符串 /* Although encoding is given it may not be set for strings, * so we determine it here using the raw length. */ if (rawlen \u0026lt;= 0x3f) { // 如果小于等于63字节 if (!p) return len; buf[0] = ZIP_STR_06B | rawlen; // 编码结果是1字节 } else if (rawlen \u0026lt;= 0x3fff) { // 如果小于等于16383字节 len += 1; if (!p) return len; buf[0] = ZIP_STR_14B | ((rawlen \u0026gt;\u0026gt; 8) \u0026amp; 0x3f); buf[1] = rawlen \u0026amp; 0xff; // 编码结果是2字节 } else { // 如果大于16383字节 len += 4; // 编码结果是5字节 if (!p) return len; buf[0] = ZIP_STR_32B; buf[1] = (rawlen \u0026gt;\u0026gt; 24) \u0026amp; 0xff; buf[2] = (rawlen \u0026gt;\u0026gt; 16) \u0026amp; 0xff; buf[3] = (rawlen \u0026gt;\u0026gt; 8) \u0026amp; 0xff; buf[4] = rawlen \u0026amp; 0xff; } } else { // 如果是整数，则编码结果是1字节 /* Implies integer encoding, so length is always 1. */ if (!p) return len; buf[0] = encoding; } /* Store this length at p. */ memcpy(p,buf,len); return len; } ziplist的弊端：\n不能保存过多的元素，否则访问性能会降低（查询需要遍历） 不能保存过⼤的元素，否则容易导致内存重新分配，甚⾄可能引发连锁更新的问题 连锁反应：压缩列表新增某个元素或修改某个元素时，如果空间不不够，压缩列表占用的内存空间就需要重新分配。而当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起「连锁更新」问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降 。\nquicklist quicklist结合了链表和ziplist的优势，quicklist是一个链表，其中每个节点都是一个ziplist。\n// quicklist节点结构体 typedef struct quicklistNode { struct quicklistNode *prev; // 前驱节点 struct quicklistNode *next; // 后驱节点 unsigned char *zl; // 该节点指向的ziplist unsigned int sz; // ziplist的字节大小 unsigned int count : 16; // ziplist的元素个数 unsigned int encoding : 2; /* RAW==1（原生字节数组） or LZF==2（压缩存储） */ unsigned int container : 2; /* 存储方式 NONE==1 or ZIPLIST==2 */ unsigned int recompress : 1; /* 数据是否被压缩 */ unsigned int attempted_compress : 1; /*数据能否被压缩 */ unsigned int extra : 10; /* 预留的bit位 */ } quicklistNode; // quicklist结构体 typedef struct quicklist { quicklistNode *head; // 头节点 quicklistNode *tail; // 尾节点 unsigned long count; /* 所有ziplist中的总元素个数 */ unsigned long len; /* 节点个数 */ int fill : 16; /* fill factor for individual nodes */ unsigned int compress : 16; /* depth of end nodes not to compress;0=off */ } quicklist; // 在插入新元素前，会先通过_quicklistNodeAllowInsert()函数检查插入位置的ziplist是否可以容纳 // 单个ziplist是否不超过8KB，或是单个ziplist⾥的元素个数是否满⾜要求，如果满足一个就在当前节点插入新元素，否则新建一个quicklistNode // 通过控制每个节点的大小或元素个数，能有效减少在ziplist中发生连锁更新的情况 REDIS_STATIC int _quicklistNodeAllowInsert(const quicklistNode *node, const int fill, const size_t sz) { if (unlikely(!node)) return 0; int ziplist_overhead; /* size of previous offset */ if (sz \u0026lt; 254) ziplist_overhead = 1; else ziplist_overhead = 5; /* size of forward offset */ if (sz \u0026lt; 64) ziplist_overhead += 1; else if (likely(sz \u0026lt; 16384)) ziplist_overhead += 2; else ziplist_overhead += 5; /* new_sz overestimates if \u0026#39;sz\u0026#39; encodes to an integer type */ unsigned int new_sz = node-\u0026gt;sz + sz + ziplist_overhead; if (likely(_quicklistNodeSizeMeetsOptimizationRequirement(new_sz, fill))) return 1; else if (!sizeMeetsSafetyLimit(new_sz)) return 0; else if ((int)node-\u0026gt;count \u0026lt; fill) return 1; else return 0; } listpack 也叫紧凑列表，用一段连续的内存空间来紧凑的保存数据，同时为了节省内存空间，listpack列表项使⽤了多种编码⽅式，来表⽰不同⻓度的数据listpack列表项使⽤了多种编码⽅式，来表⽰不同⻓度的数据，这些数据包括整数和字符串。 不同于ziplist，listpack不再存储前一个节点的长度，而是记录自身encoding和data的长度。 对于不同长度的整数和字符串，listpack通过宏定义了很多种编码类型，对于整数有LP_ENCODING_7BIT_UINT、LP_ENCODING_13BIT_INT、LP_ENCODING_16BIT_INT、LP_ENCODING_24BIT_INT、LP_ENCODING_32BIT_INT和LP_ENCODING_64BIT_INT，字符串有LP_ENCODING_6BIT_STR、LP_ENCODING_12BIT_STR和LP_ENCODING_32BIT_STR三种， listpack的从左到右正向查询过程： #define LP_HDR_SIZE 6 // 其中四个字节记录listpack的总字节数，2个字节记录元素个数 #define LP_EOF 0xFF // 结束符 // 创建一个新的listpack unsigned char *lpNew(void) { unsigned char *lp = lp_malloc(LP_HDR_SIZE+1); if (lp == NULL) return NULL; lpSetTotalBytes(lp,LP_HDR_SIZE+1); lpSetNumElements(lp,0); lp[LP_HDR_SIZE] = LP_EOF; return lp; } // listpack列表项的编码方式 #define LP_ENCODING_7BIT_UINT 0 #define LP_ENCODING_7BIT_UINT_MASK 0x80 #define LP_ENCODING_IS_7BIT_UINT(byte) (((byte)\u0026amp;LP_ENCODING_7BIT_UINT_MASK)==LP_ENCODING_7BIT_UINT) ...... #define LP_ENCODING_32BIT_STR 0xF0 #define LP_ENCODING_32BIT_STR_MASK 0xFF #define LP_ENCODING_IS_32BIT_STR(byte) (((byte)\u0026amp;LP_ENCODING_32BIT_STR_MASK)==LP_ENCODING_32BIT_STR) // 从左向右正向查询listpack时，先调用该函数跳过头部 unsigned char *lpFirst(unsigned char *lp) { lp += LP_HDR_SIZE; /* Skip the header. */ if (lp[0] == LP_EOF) return NULL; return lp; } // 然后调用lpNext()函数获得下一个列表项的指针 unsigned char *lpNext(unsigned char *lp, unsigned char *p) { ((void) lp); /* lp is not used for now. However lpPrev() uses it. */ p = lpSkip(p); if (p[0] == LP_EOF) return NULL; return p; } unsigned char *lpSkip(unsigned char *p) { unsigned long entrylen = lpCurrentEncodedSize(p); //根据当前列表项第1个字节的取值，来计算当前项的编码类型，并根据编码类型，计算当前项编码类型和实际数据的总⻓度 entrylen += lpEncodeBacklen(NULL,entrylen); //根据编码类型和实际数据的⻓度之和，进⼀步计算列表项最后⼀部分entry-len本⾝的⻓度 p += entrylen; return p; } Radix Tree 前缀树的一种，前缀树也称为Trie Tree，它的每个节点只保存单个字符，根节点不保存字符，从根节点到当前节点的字符拼接起来就得到了key。对于前缀的字符，是下面的节点共用的，所以相对于hash表来说能够节省内存空间。但是当一个节点到另一个节点之间每个节点都只有一个字节点，还进行拆分，让每个节点保存一个字符，就会造成内存空间的浪费，同时也会影响查询性能。\n如果前缀树中一系列单字符节点之间的分支连接是唯一的，对这些单字符节点进行合并，就得到了Radix Tree， 也称为基数树。\n前缀树和基数树的图示： 在基数树中，有两类节点：\n非压缩节点：包含头部HDR、多个子节点对应的字符和指向多个节点的指针，以及如果从根节点到该节点的字符串对应一个key的话包含指向这个key对应的value的指针； 压缩节点：包含头部HDR、该节点代表的合并的字符串和指向一个子节点的指针，以及如果从根节点到该节点的字符串对应一个key的话包含指向这个key对应的value的指针； 压缩节点： 无论是压缩节点还是非压缩节点，都具有两个特点：\n它们所代表的key，是从根节点到当前节点路径上的字符串，但并不包含当前节点； 它们本⾝就已经包含了⼦节点代表的字符或合并字符串。⽽对于它们的⼦节点来说，也都属于⾮压缩或压缩节点，所以，子节点本身又会保存，子节点的子节点所代表的字符或合并字符串。（也就是每个节点都包含了他们的子节点所代表的字符或合并字符串） 所以，⾮叶⼦节点⽆法同时指向表⽰单个字符的⼦节点和表⽰合并字符串的⼦节点字符串的⼦节点。 叶⼦节点的raxNode元数据size为0，没有⼦节点指针。如果叶⼦节点代表了⼀个key，那么它的raxNode中是会保存这个key的value指针的 #define RAX_NODE_MAX_SIZE ((1\u0026lt;\u0026lt;29)-1) // Radix Tree 节点结构体 typedef struct raxNode { // HDR头部信息，共占用32bit uint32_t iskey:1; /* 节点是否包含key? */ uint32_t isnull:1; /* 节点的值是否为NULL. */ uint32_t iscompr:1; /* 压缩节点还是非压缩节点. */ uint32_t size:29; /* 节点大小. */ // 对于压缩节点，data数组包含子节点对应的合并字符串、指向子节点的指针，以及节点为key时的value指针。 // 对于非压缩节点，data数组包含子节点对应的字符、指向子节点的指针，以及节点为key时的value指针。 unsigned char data[]; /* 节点数据. */ } raxNode; // Radix Tree 结构体 typedef struct rax { raxNode *head; /* 头指针. */ uint64_t numele; /* key数量. */ uint64_t numnodes; /* raxNode数量. */ } rax; // 创建一个Radix Tree rax *raxNew(void) { rax *rax = rax_malloc(sizeof(*rax)); if (rax == NULL) return NULL; rax-\u0026gt;numele = 0; rax-\u0026gt;numnodes = 1; rax-\u0026gt;head = raxNewNode(0,0); if (rax-\u0026gt;head == NULL) { rax_free(rax); return NULL; } else { return rax; } } // 创建非压缩节点 raxNode *raxNewNode(size_t children, int datafield) { size_t nodesize = sizeof(raxNode)+children+raxPadding(children)+ sizeof(raxNode*)*children; if (datafield) nodesize += sizeof(void*); raxNode *node = rax_malloc(nodesize); if (node == NULL) return NULL; node-\u0026gt;iskey = 0; node-\u0026gt;isnull = 0; node-\u0026gt;iscompr = 0; node-\u0026gt;size = children; return node; } // 创建压缩节点 raxNode *raxCompressNode(raxNode *n, unsigned char *s, size_t len, raxNode **child) { assert(n-\u0026gt;size == 0 \u0026amp;\u0026amp; n-\u0026gt;iscompr == 0); void *data = NULL; /* Initialized only to avoid warnings. */ size_t newsize; debugf(\u0026#34;Compress node: %.*s\\n\u0026#34;, (int)len,s); /* Allocate the child to link to this node. */ *child = raxNewNode(0,0); if (*child == NULL) return NULL; /* Make space in the parent node. */ newsize = sizeof(raxNode)+len+raxPadding(len)+sizeof(raxNode*); if (n-\u0026gt;iskey) { data = raxGetData(n); /* To restore it later. */ if (!n-\u0026gt;isnull) newsize += sizeof(void*); } raxNode *newn = rax_realloc(n,newsize); if (newn == NULL) { rax_free(*child); return NULL; } n = newn; n-\u0026gt;iscompr = 1; n-\u0026gt;size = len; memcpy(n-\u0026gt;data,s,len); if (n-\u0026gt;iskey) raxSetData(n,data); raxNode **childfield = raxNodeLastChildPtr(n); memcpy(childfield,child,sizeof(*child)); return n; } // 插入长度为len的字符串s int raxGenericInsert(rax *rax, unsigned char *s, size_t len, void *data, void **old, int overwrite) { 1、遍历字符串，查找插入路径； 2、若节点已存在且为完整匹配，则更新数据或返回失败； 3、若在压缩节点中间停止（字符不匹配），则按算法拆分节点； 3.1 创建新的分支节点替代原压缩节点 3.2 将原压缩节点拆分为两部分 3.3 前半部分保留在树中，后半部分成为新分支节点的子节点 3.4 为新插入的键创建另一条路径 4、若完全匹配但未到字符串末尾，则继续创建新节点； 5、插入过程中若内存不足，则回滚并返回错误。 } // 查找字符串s static inline size_t raxLowWalk(rax *rax, unsigned char *s, size_t len, raxNode **stopnode, raxNode ***plink, int *splitpos, raxStack *ts) { 1、从根节点开始遍历，逐字符匹配； 2、若当前节点为压缩节点，则按字节比较； 3、若为非压缩节点，则线性查找子节点； 4、匹配失败或到达叶节点时停止，返回已匹配长度； 5、可选保存停止节点、父链接和分裂位置。 } /* 设置raxNode中保存的value指针 */ void raxSetData(raxNode *n, void *data) { n-\u0026gt;iskey = 1; if (data != NULL) { n-\u0026gt;isnull = 0; void **ndata = (void**) ((char*)n+raxNodeCurrentLength(n)-sizeof(void*)); memcpy(ndata,\u0026amp;data,sizeof(data)); } else { n-\u0026gt;isnull = 1; } } /* 获得raxNode中保存的value指针 */ void *raxGetData(raxNode *n) { if (n-\u0026gt;isnull) return NULL; void **ndata =(void**)((char*)n+raxNodeCurrentLength(n)-sizeof(void*)); void *data; memcpy(\u0026amp;data,ndata,sizeof(data)); return data; } 数据类型 String 字符串，最基本的kv结构，不一定是字符串，也可以是数字等，最大容量是512M。常用于缓存对象、计数、分布式锁、共享session信息等场景。\n常用命令 SET name lin #设置kv GET name #获取v EXISTS name #判断是否存在 STRLEN name #获取v长度 DEL name #删除kv MSET key1 value1 key2 value2 #批量设置kv INCR number #自增number DECR number #自减number INCRBY number 10 #将number+10 DECRBY number 10 #将number-10 EXPIRE name 60 #设置过期时间60s TTL name #获取过期时间 SET key value EX 60 #设置kv及过期时间 SETNX key value #不存在就插入 内部实现 String 类型的底层的数据结构实现主要是 int 和 SDS（简单动态字符串）。并没有使用C语言中的string（C中的字符串需要手动检查和分配字符串空间，图片等数据无法用字符串保存，它使用\\0来表示字符串的结束，strlen等方法都需要遍历字符串，复杂度较高）。SDS能够提升字符串的操作效率，并且可以保存二进制数据。\n字符串对象的内部编码（encoding）有 3 种 ：int、raw和 embstr。其中raw和 embstr都是sds，当小于等于44字节时，则采用embstr编码。两者不同之处在于 embstr 会通过一次内存分配函数来分配一块连续的内存空间来保存 redisObject 和 SDS ，而 raw 编码会通过调用两次内存分配函数来分别分配两块空间来保存 redisObject 和 SDS。\nembstr的缺点：如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间，所以 embstr编码的字符串对象实际上是只读的 ，redis没有为embstr编码的字符串对象编写任何相应的修改程序。当我们对embstr编码的字符串对象执行任何修改命令（例如append）时，程序会先将对象的编码从embstr转换成raw，然后再执行修改命令。\n// 创建sds #define OBJ_ENCODING_EMBSTR_SIZE_LIMIT 44 robj *createStringObject(const char *ptr, size_t len) { if (len \u0026lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) return createEmbeddedStringObject(ptr,len); else return createRawStringObject(ptr,len); } // 创建String robj *tryObjectEncoding(robj *o) { long value; sds s = o-\u0026gt;ptr; size_t len; // 初始化检查和准备 /* 检查是否是字符串类型 */ serverAssertWithInfo(NULL,o,o-\u0026gt;type == OBJ_STRING); // /* 只对 RAW 或 EMBSTR 编码的对象进行处理 */ if (!sdsEncodedObject(o)) return o; /* 不处理共享对象（引用计数大于1的对象） */ if (o-\u0026gt;refcount \u0026gt; 1) return o; /* 判断是否可以转换为整数编码 */ len = sdslen(s); if (len \u0026lt;= 20 \u0026amp;\u0026amp; string2l(s,len,\u0026amp;value)) { // 如果字符串长度不超过20个字符，并且能成功转换为长整型 /* 如果符合共享整数条件，则使用预创建的共享整数对象（0-9999） */ if ((server.maxmemory == 0 || !(server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_NO_SHARED_INTEGERS)) \u0026amp;\u0026amp; value \u0026gt;= 0 \u0026amp;\u0026amp; value \u0026lt; OBJ_SHARED_INTEGERS) { decrRefCount(o); incrRefCount(shared.integers[value]); return shared.integers[value]; } else { // 否则，对于 RAW 编码对象直接修改为 OBJ_ENCODING_INT 并将值存储在指针中 if (o-\u0026gt;encoding == OBJ_ENCODING_RAW) { sdsfree(o-\u0026gt;ptr); o-\u0026gt;encoding = OBJ_ENCODING_INT; o-\u0026gt;ptr = (void*) value; return o; } else if (o-\u0026gt;encoding == OBJ_ENCODING_EMBSTR) { // 对于 EMBSTR 编码对象，则创建新的整数编码对象并销毁原对象 decrRefCount(o); return createStringObjectFromLongLongForValue(value); } } } /* 如果字符串较小（不超过44个字符），并且还不是 EMBSTR 编码，则将其转换为嵌入式字符串编码，这样可以减少内存碎片和缓存未命中。 */ if (len \u0026lt;= OBJ_ENCODING_EMBSTR_SIZE_LIMIT) { robj *emb; if (o-\u0026gt;encoding == OBJ_ENCODING_EMBSTR) return o; emb = createEmbeddedStringObject(s,sdslen(s)); decrRefCount(o); return emb; } /* 如果无法进一步优化编码，则至少尝试释放 SDS 字符串中多余的预留空间（如果空闲空间超过已使用空间的10%）。 */ trimStringObjectIfNeeded(o); /* Return the original object. */ return o; } List List 列表是简单的字符串列表，按照插入顺序排序，可以从头部或尾部向 List 列表添加元素，最大长度为 2^32-1。\n应用场景有：消息队列，但是List不支持多个消费者消费同一条消息\n常用命令 # 将一个或多个值value插入到key列表的表头(最左边)，最后的值在最前面 LPUSH key value [value ...] # 将一个或多个值value插入到key列表的表尾(最右边) RPUSH key value [value ...] # 移除并返回key列表的头元素 LPOP key # 移除并返回key列表的尾元素 RPOP key # 返回列表key中指定区间内的元素，区间以偏移量start和stop指定，从0开始 LRANGE key start stop # 从key列表表头弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞 BLPOP key [key ...] timeout # 从key列表表尾弹出一个元素，没有就阻塞timeout秒，如果timeout=0则一直阻塞 BRPOP key [key ...] timeout 内部实现 List 类型的底层数据结构是由 双向链表或压缩列表 实现的：\n如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用 压缩列表 作为 List 类型的底层数据结构； 如果列表的元素不满足上面的条件，Redis 会使用 双向链表 作为 List 类型的底层数据结构； 但是 在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表 。\n// 创建quicklist实现的List对象 robj *createQuicklistObject(void) { quicklist *l = quicklistCreate(); robj *o = createObject(OBJ_LIST,l); o-\u0026gt;encoding = OBJ_ENCODING_QUICKLIST; return o; } // 已弃用 robj *createZiplistObject(void) { unsigned char *zl = ziplistNew(); robj *o = createObject(OBJ_LIST,zl); o-\u0026gt;encoding = OBJ_ENCODING_ZIPLIST; return o; } // push命令 void pushGenericCommand(client *c, int where) { int j, pushed = 0; robj *lobj = lookupKeyWrite(c-\u0026gt;db,c-\u0026gt;argv[1]); if (lobj \u0026amp;\u0026amp; lobj-\u0026gt;type != OBJ_LIST) { addReply(c,shared.wrongtypeerr); return; } for (j = 2; j \u0026lt; c-\u0026gt;argc; j++) { if (!lobj) { // 如果List不存在，则创建一个新的quicklist对象 lobj = createQuicklistObject(); quicklistSetOptions(lobj-\u0026gt;ptr, server.list_max_ziplist_size, server.list_compress_depth); dbAdd(c-\u0026gt;db,c-\u0026gt;argv[1],lobj); } listTypePush(lobj,c-\u0026gt;argv[j],where); pushed++; } addReplyLongLong(c, (lobj ? listTypeLength(lobj) : 0)); if (pushed) { char *event = (where == LIST_HEAD) ? \u0026#34;lpush\u0026#34; : \u0026#34;rpush\u0026#34;; signalModifiedKey(c-\u0026gt;db,c-\u0026gt;argv[1]); notifyKeyspaceEvent(NOTIFY_LIST,event,c-\u0026gt;argv[1],c-\u0026gt;db-\u0026gt;id); } server.dirty += pushed; } void lpushCommand(client *c) { pushGenericCommand(c,LIST_HEAD); } void rpushCommand(client *c) { pushGenericCommand(c,LIST_TAIL); } Hash Hash是一个键值对集合。\n应用场景有：缓存对象等\n常用命令 # 存储一个哈希表key的键值 HSET key field value # 获取哈希表key对应的field键值 HGET key field # 在一个哈希表key中存储多个键值对 HMSET key field value [field value...] # 批量获取哈希表key中多个field键值 HMGET key field [field ...] # 删除哈希表key中的field键值 HDEL key field [field ...] # 返回哈希表key中field的数量 HLEN key # 返回哈希表key中所有的键值 HGETALL key # 为哈希表key中field键的值加上增量n HINCRBY key field n 内部实现 Hash 类型的底层数据结构是由 压缩列表或哈希表 实现的：\n如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用 压缩列表 作为 Hash 类型的底层数据结构； 如果哈希类型元素不满足上面条件，Redis 会使用 哈希表 作为 Hash 类型的 底层数据结构。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。\n// 创建ziplist实现的Hash对象，Redis默认创建ziplist实现的Hash对象 robj *createHashObject(void) { unsigned char *zl = ziplistNew(); robj *o = createObject(OBJ_HASH, zl); o-\u0026gt;encoding = OBJ_ENCODING_ZIPLIST; return o; } // 当Hash对象变大或元素数量超过阈值时会转换为Hash表 void hashTypeTryConversion(robj *o, robj **argv, int start, int end) { int i; if (o-\u0026gt;encoding != OBJ_ENCODING_ZIPLIST) return; for (i = start; i \u0026lt;= end; i++) { if (sdsEncodedObject(argv[i]) \u0026amp;\u0026amp; sdslen(argv[i]-\u0026gt;ptr) \u0026gt; server.hash_max_ziplist_value) { hashTypeConvert(o, OBJ_ENCODING_HT); break; } } } // Redis 7.0 采用listpack数据结构 robj *createHashObject(void) { unsigned char *zl = lpNew(0); robj *o = createObject(OBJ_HASH, zl); o-\u0026gt;encoding = OBJ_ENCODING_LISTPACK; return o; } Set Set是无序且唯一的集合，支持并交差等操作，但是计算复杂度较高。与List的区别是：\nList 可以存储重复元素，Set 只能存储非重复元素； List 是按照元素的先后顺序存储元素的，而 Set 则是无序方式存储元素的。 应用场景：点赞、共同关注、抽奖活动等\n常用命令 # 往集合key中存入元素，元素存在则忽略，若key不存在则新建 SADD key member [member ...] # 从集合key中删除元素 SREM key member [member ...] # 获取集合key中所有元素 SMEMBERS key # 获取集合key中的元素个数 SCARD key # 判断member元素是否存在于集合key中 SISMEMBER key member # 从集合key中随机选出count个元素，元素不从key中删除 SRANDMEMBER key [count] # 从集合key中随机选出count个元素，元素从key中删除 SPOP key [count] # 交集运算 SINTER key [key ...] # 将交集结果存入新集合destination中 SINTERSTORE destination key [key ...] # 并集运算 SUNION key [key ...] # 将并集结果存入新集合destination中 SUNIONSTORE destination key [key ...] # 差集运算 SDIFF key [key ...] # 将差集结果存入新集合destination中 SDIFFSTORE destination key [key ...] 内部实现 Set 类型的底层数据结构是由 哈希表或整数集合 实现的：\n如果集合中的元素都是整数且元素个数小于 512 （默认值， set-maxintset-entries 配置）个，Redis 会使用 整数集合 作为 Set 类型的底层数据结构； 如果集合中的元素不满足上面条件，则 Redis 使用 哈希表 作为 Set 类型的底层数据结构。 // 创建Hash表实现的Set对象 robj *createSetObject(void) { dict *d = dictCreate(\u0026amp;setDictType,NULL); robj *o = createObject(OBJ_SET,d); o-\u0026gt;encoding = OBJ_ENCODING_HT; return o; } // 创建整数集合实现的Set对象 robj *createIntsetObject(void) { intset *is = intsetNew(); robj *o = createObject(OBJ_SET,is); o-\u0026gt;encoding = OBJ_ENCODING_INTSET; return o; } /* 检查是否可以转换为long long类型，如果可以转换为整数，则创建一个整数集合对象（intset），否则创建一个Hash表实现的Set对象（HT） */ robj *setTypeCreate(sds value) { if (isSdsRepresentableAsLongLong(value,NULL) == C_OK) return createIntsetObject(); return createSetObject(); } Sorted Set Sorted Set是一种集合类型，支持元素带权重，并按照权重排序。当数据较少（元素个数小于128且每个元素的值小于64字节）时，采用ziplist存储，每个元素和权重紧凑排列，节省内存；当数据超过阈值（set-max-ziplist-entries、zset-max-ziplist-valu）后，转为hash表+跳表存储，降低查询复杂度。\n关于为什么用用跳表而不用平衡二叉树？\n跳表更省内存：跳表平均每个节点指针数是1.33，平衡二叉树每个节点指针数为2 跳表遍历更优化：跳表找到⼤于⽬标元素后，向后遍历链表即可，平衡树需要通过中序遍历⽅式来完成，实现也略复杂 跳表更易实现和维护：扩展跳表只需要改少量代码即可完成，平衡树维护起来较复杂 // zset结构 typedef struct zset { dict *dict; // hash表，key为元素，value为权重，指向跳表中的权重 zskiplist *zsl; // 跳表 } zset; // 添加元素 int zsetAdd(robj *zobj, double score, sds ele, int *flags, double *newscore) { /* Turn options into simple to check vars. */ int incr = (*flags \u0026amp; ZADD_INCR) != 0; int nx = (*flags \u0026amp; ZADD_NX) != 0; int xx = (*flags \u0026amp; ZADD_XX) != 0; *flags = 0; /* We\u0026#39;ll return our response flags. */ double curscore; /* NaN as input is an error regardless of all the other parameters. */ if (isnan(score)) { *flags = ZADD_NAN; return 0; } // 先判断采用的编码方式， ziplist or skiplist /* Update the sorted set according to its encoding. */ if (zobj-\u0026gt;encoding == OBJ_ENCODING_ZIPLIST) { // 如果是ziplist unsigned char *eptr; if ((eptr = zzlFind(zobj-\u0026gt;ptr,ele,\u0026amp;curscore)) != NULL) { // 先用zzlFind查是否存在 /* NX? Return, same element already exists. */ if (nx) { *flags |= ZADD_NOP; return 1; } /* 计算新权重 */ if (incr) { score += curscore; if (isnan(score)) { *flags |= ZADD_NAN; return 0; } if (newscore) *newscore = score; } /* 权重变化则删除重新插入 */ if (score != curscore) { zobj-\u0026gt;ptr = zzlDelete(zobj-\u0026gt;ptr,eptr); zobj-\u0026gt;ptr = zzlInsert(zobj-\u0026gt;ptr,ele,score); *flags |= ZADD_UPDATED; } return 1; } else if (!xx) { /* 检查长度是否过长或元素过大 */ zobj-\u0026gt;ptr = zzlInsert(zobj-\u0026gt;ptr,ele,score); if (zzlLength(zobj-\u0026gt;ptr) \u0026gt; server.zset_max_ziplist_entries || sdslen(ele) \u0026gt; server.zset_max_ziplist_value) zsetConvert(zobj,OBJ_ENCODING_SKIPLIST); if (newscore) *newscore = score; *flags |= ZADD_ADDED; return 1; } else { *flags |= ZADD_NOP; return 1; } } else if (zobj-\u0026gt;encoding == OBJ_ENCODING_SKIPLIST) { zset *zs = zobj-\u0026gt;ptr; zskiplistNode *znode; dictEntry *de; de = dictFind(zs-\u0026gt;dict,ele); // 用dictFind查是否存在 if (de != NULL) { // 存在 /* NX? Return, same element already exists. */ if (nx) { *flags |= ZADD_NOP; return 1; } curscore = *(double*)dictGetVal(de); /* 计算新权重. */ if (incr) { score += curscore; if (isnan(score)) { *flags |= ZADD_NAN; return 0; } if (newscore) *newscore = score; } /* 如果权重变化则更新跳表和hash表 */ if (score != curscore) { znode = zslUpdateScore(zs-\u0026gt;zsl,curscore,ele,score); // 跳表更新 /* Note that we did not removed the original element from * the hash table representing the sorted set, so we just * update the score. */ dictGetVal(de) = \u0026amp;znode-\u0026gt;score; /* Update score ptr. */ // hash表更新，指向新的score *flags |= ZADD_UPDATED; } return 1; } else if (!xx) { // 不存在就插入，分开插入，保持两者的独立性 ele = sdsdup(ele); znode = zslInsert(zs-\u0026gt;zsl,score,ele); // 跳表插入 serverAssert(dictAdd(zs-\u0026gt;dict,ele,\u0026amp;znode-\u0026gt;score) == DICT_OK); // hash表插入 *flags |= ZADD_ADDED; if (newscore) *newscore = score; return 1; } else { *flags |= ZADD_NOP; return 1; } } else { serverPanic(\u0026#34;Unknown sorted set encoding\u0026#34;); } return 0; /* Never reached. */ } Stream 消息队列，保存的消息具有以下两个特征：\n一条消息由一个或多个键值对组成 每插入一条消息，这条消息都会对应⼀个消息ID 常用命令 XADD：插入消息，保证有序，可以自动生成全局唯一 ID； XLEN ：查询消息长度； XREAD：用于读取消息，可以按 ID 读取数据； XDEL ： 根据消息 ID 删除消息； DEL ：删除整个 Stream； XRANGE ：读取区间消息； XREADGROUP：按消费组形式读取消息； XPENDING 和 XACK： XPENDING 命令可以用来查询每个消费组内所有消费者「已读取、但尚未确认」的消息； XACK 命令用于向消息队列确认消息处理已完成。 内部实现 Stream基于listpack和 Radix Tree实现，其中消息ID是作为Radix Tree中的key，消息具体数据是使⽤listpack保存，并作为value和消息ID⼀起保存到Radix Tree中。\n// 消息ID结构体 typedef struct streamID { uint64_t ms; /* Unix time in milliseconds. */ uint64_t seq; /* Sequence number. */ } streamID; // Stream结构体 typedef struct stream { rax *rax; /* 保存消息的Radix Tree */ uint64_t length; /* 消息个数 */ streamID last_id; /* 最后插入的消息ID. */ rax *cgroups; /* 消费组信息: name -\u0026gt; streamCG， Radix Tree保存 */ } stream; 源码 源码地址：https://github.com/redis/redis\n下面源码分析基于Redis 5.0.8 目录结构 deps: 依赖的第三方库，包括客⼾端代码hiredis、jemalloc内存分配器代码、readline功能的替代代码linenoise，以及lua脚本代码 src: 所有功能模块的代码文件，功能模块之间并没有设置目录分隔，而是通过头文件包含来互相调用（C语言风格） tests: 单元测试和功能测试（集群功能、哨兵功能、主从复制功能）代码 utils: 工具代码，包括⽤于创建Redis Cluster的脚本、⽤于测试LRU算法效果的程序，以及可视化rehash过程的程序 配置文件：Redis实例的配置⽂件redis.conf和哨兵的配置⽂件sentinel.conf。 功能模块与源码对应关系 服务端实例 初始化、主控制流程：server.c,server.h 网络通信框架：ae.c, ae.h, ae_epoll.c, ae_evport.c, ae_kqueue.c, ae_select.c TCP Socket：anet.c, anet.h 客户端实现：networking.c 数据类型与操作 各种数据结构对应源码如下图，除此外基本的对kv增删改查操作在db.c中实现。\nRedis内存优化，主要从以下三个方面：\n内存分配：Redis⽀持使⽤不同的内存分配器，包括glibc库提供的默认分配器tcmalloc、第三⽅库提供的jemalloc，对内存分配器实现封装在zmalloc.h/zmalloc.c中。 内存回收：Redis⽀持设置过期key，并针对过期key可以使⽤不同删除策略，代码实现在expire.c⽂件中。同时，为了避免⼤量key删除回收内存，会对系统性能产⽣影响，Redis在lazyfree.c中实现了异步删除的功能，所以就可以使⽤后台IO线程来完成删除，以避免对Redis主线程的影响。 数据替换：如果内存满了，Redis还会按照⼀定规则清除不需要的数据，包括LRU、LFU等算法，实现在evict.c中。 ⾼可靠性和⾼可扩展性 数据持久化实现：内存快照RDB和AOF日志，分别在rdb.h/rdb.c和aof.caof.c中，此外还有对两类文件的检查功能，实现在edis-check-rdb.c和redis-check-aof.c中。 主从复制功能实现：实现在replication.c中，哨兵机制实现在sentinel.c中，集群功能实现在cluster.h/cluster.c。 辅助功能 用于支持系统运维\n在latency.h/latency.c中实现了操作延迟监控的功能，便于运维⼈员查看分析不同操作的延迟产⽣来源 在slowlog.h/slowlog.c中实现了慢命令的记录功能，便于运维⼈员查找运⾏过慢的操作命令 对系统进⾏性能评测的功能实现在redis-benchmark.c中。 总结 数据类型：\nString (t_string.c, sds.c, bitops.c) List (t_list.c, ziplist.c) Hash (t_hash.c, ziplist.c, dict.c) Set (t_set.c, intset.c) Sorted Set (t_zset.c, ziplist.c, dict.c) HyperLogLog (hyperloglog.c) Geo (geo.c, geohash.c, geohash_helper.c) Stream (t_stream.c, rax.c, listpack.c) 全局：\nServer (server.c, anet.c) Object (object.c) 键值对 (db.c) 事件驱动 (ae.c, ae_epoll.c, ae_kqueue.c, ae_evport.c, ae_select.c, networking.c) 内存回收 (expire.c, lazyfree.c) 数据替换 (evict.c) 后台线程 (bio.c) 事务 (multi.c) PubSub (pubsub.c) 内存分配 (zmalloc.c) 双向链表 (adlist.c) 高可用\u0026amp;集群：\n持久化：RDB（rdb.c、redis-check-rdb.c）、AOF（aof.c、redis-check-aof.c） 主从复制（replication.c） 哨兵（sentinel.c） 集群（cluster.c） 辅助功能：\n延迟统计（latency.c） 慢日志（slowlog.c） 通知（notify.c） 基准性能（redis-benchmark.c） 事件驱动框架 Redis Server启动过程 main函数：\n基本初始化 检查哨兵模式，并检查是否要执⾏RDB检测或AOF检测 运⾏参数解析 初始化server 执⾏事件驱动框架 int main(int argc, char **argv) { struct timeval tv; int j; // 测试模式处理：如果编译时启用了测试功能且命令行参数指定了测试，则运行相应的单元测试 #ifdef REDIS_TEST // 如果启动参数有test和ziplist，那么就调⽤ziplistTest函数进⾏ziplist的测试 if (argc == 3 \u0026amp;\u0026amp; !strcasecmp(argv[1], \u0026#34;test\u0026#34;)) { if (!strcasecmp(argv[2], \u0026#34;ziplist\u0026#34;)) { return ziplistTest(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;quicklist\u0026#34;)) { quicklistTest(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;intset\u0026#34;)) { return intsetTest(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;zipmap\u0026#34;)) { return zipmapTest(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;sha1test\u0026#34;)) { return sha1Test(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;util\u0026#34;)) { return utilTest(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;endianconv\u0026#34;)) { return endianconvTest(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;crc64\u0026#34;)) { return crc64Test(argc, argv); } else if (!strcasecmp(argv[2], \u0026#34;zmalloc\u0026#34;)) { return zmalloc_test(argc, argv); } return -1; /* test not found */ } #endif // 基础环境初始化 #ifdef INIT_SETPROCTITLE_REPLACEMENT spt_init(argc, argv); #endif // 设置时区 setlocale(LC_COLLATE,\u0026#34;\u0026#34;); tzset(); /* Populates \u0026#39;timezone\u0026#39; global. */ zmalloc_set_oom_handler(redisOutOfMemoryHandler); srand(time(NULL)^getpid()); gettimeofday(\u0026amp;tv,NULL); // 设置随机种子，用于哈希函数 redis_version = REDIS_VERSION char hashseed[16]; getRandomHexChars(hashseed,sizeof(hashseed)); dictSetHashFunctionSeed((uint8_t*)hashseed); server.sentinel_mode = checkForSentinelMode(argc,argv); // 检测是否以Sentinel模式运行 initServerConfig(); // 初始化服务器配置 moduleInitModulesSystem(); // 初始化模块系统 /* 保存执行参数 */ server.executable = getAbsolutePath(argv[0]); server.exec_argv = zmalloc(sizeof(char*)*(argc+1)); server.exec_argv[argc] = NULL; for (j = 0; j \u0026lt; argc; j++) server.exec_argv[j] = zstrdup(argv[j]); /* Sentinel模式初始化 */ if (server.sentinel_mode) { initSentinelConfig(); /* 初始化sentinel配置 */ initSentinel(); /* 初始化sentinel */ } /* 检查是否以redis-check-rdb或redis-check-aof模式运行，如果是则执行相应的检查功能。 */ if (strstr(argv[0],\u0026#34;redis-check-rdb\u0026#34;) != NULL) redis_check_rdb_main(argc,argv,NULL); else if (strstr(argv[0],\u0026#34;redis-check-aof\u0026#34;) != NULL) redis_check_aof_main(argc,argv); // 命令行参数解析 if (argc \u0026gt;= 2) { j = 1; /* First option to parse in argv[] */ sds options = sdsempty(); char *configfile = NULL; /* Handle special options --help and --version */ if (strcmp(argv[1], \u0026#34;-v\u0026#34;) == 0 || strcmp(argv[1], \u0026#34;--version\u0026#34;) == 0) version(); if (strcmp(argv[1], \u0026#34;--help\u0026#34;) == 0 || strcmp(argv[1], \u0026#34;-h\u0026#34;) == 0) usage(); if (strcmp(argv[1], \u0026#34;--test-memory\u0026#34;) == 0) { if (argc == 3) { memtest(atoi(argv[2]),50); exit(0); } else { fprintf(stderr,\u0026#34;Please specify the amount of memory to test in megabytes.\\n\u0026#34;); fprintf(stderr,\u0026#34;Example: ./redis-server --test-memory 4096\\n\\n\u0026#34;); exit(1); } } /* First argument is the config file name? */ if (argv[j][0] != \u0026#39;-\u0026#39; || argv[j][1] != \u0026#39;-\u0026#39;) { configfile = argv[j]; server.configfile = getAbsolutePath(configfile); /* Replace the config file in server.exec_argv with * its absolute path. */ zfree(server.exec_argv[j]); server.exec_argv[j] = zstrdup(server.configfile); j++; } /* All the other options are parsed and conceptually appended to the * configuration file. For instance --port 6380 will generate the * string \u0026#34;port 6380\\n\u0026#34; to be parsed after the actual file name * is parsed, if any. */ while(j != argc) { if (argv[j][0] == \u0026#39;-\u0026#39; \u0026amp;\u0026amp; argv[j][1] == \u0026#39;-\u0026#39;) { /* Option name */ if (!strcmp(argv[j], \u0026#34;--check-rdb\u0026#34;)) { /* Argument has no options, need to skip for parsing. */ j++; continue; } if (sdslen(options)) options = sdscat(options,\u0026#34;\\n\u0026#34;); options = sdscat(options,argv[j]+2); options = sdscat(options,\u0026#34; \u0026#34;); } else { /* Option argument */ options = sdscatrepr(options,argv[j],strlen(argv[j])); options = sdscat(options,\u0026#34; \u0026#34;); } j++; } if (server.sentinel_mode \u0026amp;\u0026amp; configfile \u0026amp;\u0026amp; *configfile == \u0026#39;-\u0026#39;) { serverLog(LL_WARNING, \u0026#34;Sentinel config from STDIN not allowed.\u0026#34;); serverLog(LL_WARNING, \u0026#34;Sentinel needs config file on disk to save state. Exiting...\u0026#34;); exit(1); } resetServerSaveParams(); loadServerConfig(configfile,options); sdsfree(options); } serverLog(LL_WARNING, \u0026#34;oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\u0026#34;); serverLog(LL_WARNING, \u0026#34;Redis version=%s, bits=%d, commit=%s, modified=%d, pid=%d, just started\u0026#34;, REDIS_VERSION, (sizeof(long) == 8) ? 64 : 32, redisGitSHA1(), strtol(redisGitDirty(),NULL,10) \u0026gt; 0, (int)getpid()); if (argc == 1) { serverLog(LL_WARNING, \u0026#34;Warning: no config file specified, using the default config. In order to specify a config file use %s /path/to/%s.conf\u0026#34;, argv[0], server.sentinel_mode ? \u0026#34;sentinel\u0026#34; : \u0026#34;redis\u0026#34;); } else { serverLog(LL_WARNING, \u0026#34;Configuration loaded\u0026#34;); } // 检查是否需要以监督模式运行，如果需要后台运行则执行daemonize server.supervised = redisIsSupervised(server.supervised_mode); int background = server.daemonize \u0026amp;\u0026amp; !server.supervised; if (background) daemonize(); // 初始化服务器 initServer(); // 初始化服务器核心组件 if (background || server.pidfile) createPidFile(); // 创建PID文件 redisSetProcTitle(argv[0]); // 设置进程标题 redisAsciiArt(); // 显示ASCII LOGO checkTcpBacklogSettings(); // 检查TCP backlog设置 // 判断当前server是否为哨兵模式 if (!server.sentinel_mode) { /* Things not needed when running in Sentinel mode. */ serverLog(LL_WARNING,\u0026#34;Server initialized\u0026#34;); #ifdef __linux__ linuxMemoryWarnings(); #endif moduleLoadFromQueue(); InitServerLast(); loadDataFromDisk(); // 从磁盘加载AOF或者RDB文件，恢复数据 if (server.cluster_enabled) { if (verifyClusterConfigWithData() == C_ERR) { serverLog(LL_WARNING, \u0026#34;You can\u0026#39;t have keys in a DB different than DB 0 when in \u0026#34; \u0026#34;Cluster mode. Exiting.\u0026#34;); exit(1); } } if (server.ipfd_count \u0026gt; 0) serverLog(LL_NOTICE,\u0026#34;Ready to accept connections\u0026#34;); if (server.sofd \u0026gt; 0) serverLog(LL_NOTICE,\u0026#34;The server is now ready to accept connections at %s\u0026#34;, server.unixsocket); } else { InitServerLast(); sentinelIsRunning(); // 启动哨兵模式 } /* Warning the user about suspicious maxmemory setting. */ if (server.maxmemory \u0026gt; 0 \u0026amp;\u0026amp; server.maxmemory \u0026lt; 1024*1024) { serverLog(LL_WARNING,\u0026#34;WARNING: You specified a maxmemory value that is less than 1MB (current value is %llu bytes). Are you sure this is what you really want?\u0026#34;, server.maxmemory); } // 执⾏事件驱动框架 ，设置事件循环的前后处理函数并启动主事件循环 aeSetBeforeSleepProc(server.el,beforeSleep); aeSetAfterSleepProc(server.el,afterSleep); aeMain(server.el); aeDeleteEventLoop(server.el); return 0; } 主要参数 Redis参数的设置方法： Redis对运⾏参数的设置实际上会经过三轮赋值，分别是默认配置值、命令⾏启动参数，以及配置⽂件配置值。\nRedis在main函数中会先调⽤initServerConfig函数，为各种参数设置默认值先调⽤initServerConfig函数，为各种参数设置默认值。参数的默认值统⼀定义在server.h⽂件中，都是以CONFIG_DEFAULT开头的宏定义变量； main函数就会对Redis程序启动时的对Redis程序启动时的命令⾏参数进⾏逐⼀解析，并保存为字符串； 调⽤loadServerConfig函数进⾏第二轮和第三轮的赋值，loadServerConfig函数会把解析后的命令⾏参数，追加到配置⽂件形成的配置项字符串 loadServerConfig函数会进⼀步调⽤loadServerConfigFromString函数，对配置项字符串中的每⼀个配置项进⾏匹配和检查配置项的值是否合理，匹配成功后设置server的参数 loadServerConfigFromString(char *config) { ... //参数名匹配,检查参数是否为\u0026#34;timeout\u0026#34; if ( ! strcasecmp(argv[0], \u0026#34;timeout\u0026#34;) \u0026amp;\u0026amp; argc == 2) { //设置server的maxidletime参数 server.maxidletime = atoi(argv[1]); //检查参数值是否小于,小于⊙则报错 if (server.maxidletime \u0026lt; 0) { err = \u0026#34;Invalid timeout value\u0026#34;; goto loaderr; } } //参数名匹配,检查参数是否为\u0026#34;tcp-keepalive\u0026#34; else if ( ! strcasecmp(argv[0], \u0026#34;tcp-keepalive\u0026#34;) \u0026amp;\u0026amp; argc == 2) { //设置server的tcpkeepalive参数 server.tcpkeepalive = atoi(argv[1]); //检查参数值是否小于0,小于⊙则报错 if (server.tcpkeepalive \u0026lt; 0) { err = \u0026#34;Invalid tcp-keepalive value\u0026#34;; goto loaderr; } } ... } InitServer 初始化步骤主要有三步：\nRedis Server运行时需要对多种资源进行管理 在完成资源管理信息的初始化后，initServer函数会对Redis数据库进⾏初始化 initServer函数会为运⾏的Redis server创建事件驱动框架，并开始启动端⼝监听，⽤于接收外部请求 void initServer(void) { int j; // 信号处理初始化 signal(SIGHUP, SIG_IGN); signal(SIGPIPE, SIG_IGN); setupSignalHandlers(); // 系统日志配置 if (server.syslog_enabled) { openlog(server.syslog_ident, LOG_PID | LOG_NDELAY | LOG_NOWAIT, server.syslog_facility); } // 基本服务器状态初始化 server.hz = server.config_hz; server.pid = getpid(); server.current_client = NULL; server.fixed_time_expire = 0; server.clients = listCreate(); // 客户端初始化 server.clients_index = raxNew(); server.clients_to_close = listCreate(); server.slaves = listCreate(); server.monitors = listCreate(); server.clients_pending_write = listCreate(); server.slaveseldb = -1; /* Force to emit the first SELECT command. */ server.unblocked_clients = listCreate(); server.ready_keys = listCreate(); server.clients_waiting_acks = listCreate(); server.get_ack_from_slaves = 0; server.clients_paused = 0; server.system_memory_size = zmalloc_get_memory_size(); // 共享对象和文件描述符限制 createSharedObjects(); adjustOpenFilesLimit(); // 事件循环初始化 server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR); // 创建事件循环框架 if (server.el == NULL) { serverLog(LL_WARNING, \u0026#34;Failed creating the event loop. Error message: \u0026#39;%s\u0026#39;\u0026#34;, strerror(errno)); exit(1); } server.db = zmalloc(sizeof(redisDb)*server.dbnum); /* 网络监听初始化 */ if (server.port != 0 \u0026amp;\u0026amp; // 开始监听设置的⽹络端⼝ listenToPort(server.port,server.ipfd,\u0026amp;server.ipfd_count) == C_ERR) exit(1); /* Open the listening Unix domain socket. */ if (server.unixsocket != NULL) { unlink(server.unixsocket); /* don\u0026#39;t care if this fails */ server.sofd = anetUnixServer(server.neterr,server.unixsocket, server.unixsocketperm, server.tcp_backlog); if (server.sofd == ANET_ERR) { serverLog(LL_WARNING, \u0026#34;Opening Unix socket: %s\u0026#34;, server.neterr); exit(1); } anetNonBlock(NULL,server.sofd); } /* Abort if there are no listening sockets at all. */ if (server.ipfd_count == 0 \u0026amp;\u0026amp; server.sofd \u0026lt; 0) { serverLog(LL_WARNING, \u0026#34;Configured to not listen anywhere, exiting.\u0026#34;); exit(1); } // 为每个数据库执⾏初始化操作 for (j = 0; j \u0026lt; server.dbnum; j++) { //创建全局哈希表 server.db[j].dict = dictCreate(\u0026amp;dbDictType, NULL); //创建过期key的信息表 server.db[j].expires = dictCreate(\u0026amp;keyptrDictType, NULL); //为被BLPOP阻塞的key创建信息表 server.db[j].blocking_keys = dictCreate(\u0026amp;keylistDictType, NULL); //为将执行PUSH的阻塞key创建信息表 server.db[j].ready_keys = dictCreate(\u0026amp;objectKeyPointerValueDictType, NULL); //为被MULTI/WATCH操作监听的key创建信息表 server.db[j].watched_keys = dictCreate(\u0026amp;keylistDictType, NULL); ... } evictionPoolAlloc(); /* Initialize the LRU keys pool. */ // 发布/订阅系统初始化 server.pubsub_channels = dictCreate(\u0026amp;keylistDictType,NULL); server.pubsub_patterns = listCreate(); listSetFreeMethod(server.pubsub_patterns,freePubsubPattern); listSetMatchMethod(server.pubsub_patterns,listMatchPubsubPattern); server.cronloops = 0; server.rdb_child_pid = -1; server.aof_child_pid = -1; server.rdb_child_type = RDB_CHILD_TYPE_NONE; server.rdb_bgsave_scheduled = 0; server.child_info_pipe[0] = -1; server.child_info_pipe[1] = -1; server.child_info_data.magic = 0; aofRewriteBufferReset(); server.aof_buf = sdsempty(); server.lastsave = time(NULL); /* At startup we consider the DB saved. */ server.lastbgsave_try = 0; /* At startup we never tried to BGSAVE. */ server.rdb_save_time_last = -1; server.rdb_save_time_start = -1; server.dirty = 0; resetServerStats(); /* A few stats we don\u0026#39;t want to reset: server startup time, and peak mem. */ server.stat_starttime = time(NULL); server.stat_peak_memory = 0; server.stat_rdb_cow_bytes = 0; server.stat_aof_cow_bytes = 0; server.cron_malloc_stats.zmalloc_used = 0; server.cron_malloc_stats.process_rss = 0; server.cron_malloc_stats.allocator_allocated = 0; server.cron_malloc_stats.allocator_active = 0; server.cron_malloc_stats.allocator_resident = 0; server.lastbgsave_status = C_OK; server.aof_last_write_status = C_OK; server.aof_last_write_errno = 0; server.repl_good_slaves_count = 0; /* 事件处理器注册 */ // 为server后台任务创建定时事件 if (aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) == AE_ERR) { serverPanic(\u0026#34;Can\u0026#39;t create event loop timers.\u0026#34;); exit(1); } // 为每⼀个监听的IP设置连接事件的处理函数acceptTcpHandler for (j = 0; j \u0026lt; server.ipfd_count; j++) { if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE, acceptTcpHandler,NULL) == AE_ERR) { serverPanic( \u0026#34;Unrecoverable error creating server.ipfd file event.\u0026#34;); } } // 注册Unix socket连接处理器 if (server.sofd \u0026gt; 0 \u0026amp;\u0026amp; aeCreateFileEvent(server.el,server.sofd,AE_READABLE, acceptUnixHandler,NULL) == AE_ERR) serverPanic(\u0026#34;Unrecoverable error creating server.sofd file event.\u0026#34;); /* Register a readable event for the pipe used to awake the event loop * when a blocked client in a module needs attention. */ if (aeCreateFileEvent(server.el, server.module_blocked_pipe[0], AE_READABLE, moduleBlockedClientPipeReadable,NULL) == AE_ERR) { serverPanic( \u0026#34;Error registering the readable event for the module \u0026#34; \u0026#34;blocked clients subsystem.\u0026#34;); } /* AOF初始化 */ if (server.aof_state == AOF_ON) { server.aof_fd = open(server.aof_filename, O_WRONLY|O_APPEND|O_CREAT,0644); if (server.aof_fd == -1) { serverLog(LL_WARNING, \u0026#34;Can\u0026#39;t open the append-only file: %s\u0026#34;, strerror(errno)); exit(1); } } /* 32位系统特殊处理 */ if (server.arch_bits == 32 \u0026amp;\u0026amp; server.maxmemory == 0) { serverLog(LL_WARNING,\u0026#34;Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with \u0026#39;noeviction\u0026#39; policy now.\u0026#34;); server.maxmemory = 3072LL*(1024*1024); /* 3 GB */ server.maxmemory_policy = MAXMEMORY_NO_EVICTION; } // 子系统初始化 if (server.cluster_enabled) clusterInit(); replicationScriptCacheInit(); scriptingInit(1); slowlogInit(); latencyMonitorInit(); } 执行事件执行框架 事件驱动框架是Redis server运⾏的核⼼。该框架⼀旦启动后，就会⼀直循环执⾏，每次循环会处理⼀批触发的⽹络读写事件。main函数直接调⽤事件框架的主体函数aeMain后就进入了事件处理循环。\n// main() 最后 aeSetBeforeSleepProc(server.el,beforeSleep); eaSetAfterSleepProc(server.el,afterSleep); aeMain(server.el); aeDeleteEventLoop(server.el); 事件驱动框架 select、poll、epoll Socket编程模型包括创建Socket、监听端口、处理连接请求和读写请求。但是Socket编程模型一次只能处理一个客户端连接的请求，一种方案是多线程，但是Redis是单线程的，为了实现高并发，Redis使用了Linux提供的select、poll、epoll中的epoll模型。\nselect：限制了文件描述符的个数，最大支持1024个，程序调用select后就会阻塞，当有就绪的描述符后，select返回就绪的数量，然后遍历描述符集合，找到就绪的描述符。 poll：不限制文件描述符数量，但是仍然需要遍历描述符集合，才能找到就绪的描述符 epoll：在epoll实例内部维护了两个结构，要监听的描述符和已经就绪的描述符，所以epoll不需要遍历所有描述符 Reactor模型 Reactor模型就是⽹络服务器端⽤来处理⾼并发⽹络IO请求的⼀种编程模型，特征是：\n三类处理事件，即连接事件、写事件、读事件； 当⼀个客⼾端要和服务器端进⾏交互时，客⼾端会向服务器端发送连接请求，以建⽴连接，这就对应了服务器端的⼀个连接事件连接事件； ⼀旦连接建⽴后，客⼾端会给服务器端发送读请求，以便读取数据。服务器端在处理读请求时，需要向客⼾端写回数据，这对应了服务器端的写事件写事件； ⽆论客⼾端给服务器端发送读或写请求，服务器端都需要从客⼾端读取请求内容，所以在这⾥，读或写请求的读取就对应了服务端的读请求； 三个关键⻆⾊，即reactor、acceptor、handler 连接事件由acceptor来处理，负责接收连接；acceptor在接收连接后，会创建handler，⽤于⽹络连接上对后续读写事件的处理； 读写事件由handler处理 在⾼并发场景中，连接事件、读写事件会同时发⽣，所以，我们需要有⼀个⻆⾊专⻔监听和分配事件，这就是reactor⻆⾊。当有连接请求时，reactor将产⽣的连接事件交由acceptor处理；当有读写请求时，reactor将读写事件交给handler处理。 Reactor模型有三类：\n单 Reactor 单线程：accept-\u0026gt;read-\u0026gt;处理业务逻辑-\u0026gt;write都在⼀个线程 单 Reactor 多线程：accept/read/write在⼀个线程，处理业务逻辑在另⼀个线程 多 Reactor 多线程/进程：accept在⼀个线程/进程，read/处理业务逻辑/write在另⼀个线程/进程 Redis 6.0 以下版本，属于单 Reactor 单线程模型，监听请求、读取数据、处理请求、写回数据都在⼀个线程中执⾏，存在三个问题：\n单线程⽆法利⽤多核 处理请求发⽣耗时，会阻塞整个线程，影响整体性能 并发请求过⾼，读取/写回数据存在瓶颈 Redis 6.0 进⾏了优化，引⼊了 IO 多线程，把读写请求数据的逻辑，⽤多线程处理，提升并发性能，但处理请求的逻辑依旧是单线程处理\n事件驱动框架 事件驱动框架，就是在实现Reactor模型时，需要实现的代码整体控制逻辑。简单来说，事件驱动框架包括了两部分：\n⼀是事件初始化，在服务器程序启动时就执⾏，它的作⽤主要是创建需要监听的事件类型，以及该类事件对应的handler； ⼆是事件捕获、分发和处理主循环，在while循环中，捕获发⽣的事件、判断事件类型，并根据事件类型，调用初始化创建好的acceptor和handler处理时间。 Redis中Reactor的实现 Redis的事件驱动框架定义了两类事件：IO事件和时间事件IO事件和时间事件，分别对应了客⼾端发送的⽹络请求和Redis⾃⾝的周期性操作\n// IO事件的数据结构 typedef struct aeFileEvent { int mask; /* 事件类型的掩码，包括可读事件、可写事件和屏障事件 AE_(READABLE|WRITABLE|BARRIER)，屏障事件是⽤来反转事件的处理顺序 */ aeFileProc *rfileProc; /* 指向AE_READABLE事件处理函数 */ aeFileProc *wfileProc; /* 指向AE_WRITABLE事件处理函数 */ void *clientData; /* 指向客⼾端私有数据的指针 */ } aeFileEvent; // 框架主循环， 在server.c main()最后被调用 void aeMain(aeEventLoop *eventLoop) { eventLoop-\u0026gt;stop = 0; while (!eventLoop-\u0026gt;stop) { // 如果事件循环的停⽌标记被设置为true，就停止 if (eventLoop-\u0026gt;beforesleep != NULL) eventLoop-\u0026gt;beforesleep(eventLoop); aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP); } } // 负责捕获事件、判断事件类型和调⽤具体的事件处理函数 int aeProcessEvents(aeEventLoop *eventLoop, int flags) { int processed = 0, numevents; /* 若没有事件处理，则⽴刻返回 */ if (!(flags \u0026amp; AE_TIME_EVENTS) \u0026amp;\u0026amp; !(flags \u0026amp; AE_FILE_EVENTS)) return 0; /* 如果有IO事件发⽣，或者紧急的时间事件发⽣，则开始处理 */ if (eventLoop-\u0026gt;maxfd != -1 || ((flags \u0026amp; AE_TIME_EVENTS) \u0026amp;\u0026amp; !(flags \u0026amp; AE_DONT_WAIT))) { int j; aeTimeEvent *shortest = NULL; struct timeval tv, *tvp; if (flags \u0026amp; AE_TIME_EVENTS \u0026amp;\u0026amp; !(flags \u0026amp; AE_DONT_WAIT)) shortest = aeSearchNearestTimer(eventLoop); if (shortest) { long now_sec, now_ms; aeGetTime(\u0026amp;now_sec, \u0026amp;now_ms); tvp = \u0026amp;tv; /* How many milliseconds we need to wait for the next * time event to fire? */ long long ms = (shortest-\u0026gt;when_sec - now_sec)*1000 + shortest-\u0026gt;when_ms - now_ms; if (ms \u0026gt; 0) { tvp-\u0026gt;tv_sec = ms/1000; tvp-\u0026gt;tv_usec = (ms % 1000)*1000; } else { tvp-\u0026gt;tv_sec = 0; tvp-\u0026gt;tv_usec = 0; } } else { /* If we have to check for events but need to return * ASAP because of AE_DONT_WAIT we need to set the timeout * to zero */ if (flags \u0026amp; AE_DONT_WAIT) { tv.tv_sec = tv.tv_usec = 0; tvp = \u0026amp;tv; } else { /* Otherwise we can block */ tvp = NULL; /* wait forever */ } } /* 处理网络事件，调用aeApiPoll，ae_epoll.c中的aeApiPoll封装了epoll_wait的实现 */ numevents = aeApiPoll(eventLoop, tvp); /* After sleep callback. */ if (eventLoop-\u0026gt;aftersleep != NULL \u0026amp;\u0026amp; flags \u0026amp; AE_CALL_AFTER_SLEEP) eventLoop-\u0026gt;aftersleep(eventLoop); for (j = 0; j \u0026lt; numevents; j++) { aeFileEvent *fe = \u0026amp;eventLoop-\u0026gt;events[eventLoop-\u0026gt;fired[j].fd]; int mask = eventLoop-\u0026gt;fired[j].mask; int fd = eventLoop-\u0026gt;fired[j].fd; int fired = 0; /* Number of events fired for current fd. */ /* Normally we execute the readable event first, and the writable * event laster. This is useful as sometimes we may be able * to serve the reply of a query immediately after processing the * query. * * However if AE_BARRIER is set in the mask, our application is * asking us to do the reverse: never fire the writable event * after the readable. In such a case, we invert the calls. * This is useful when, for instance, we want to do things * in the beforeSleep() hook, like fsynching a file to disk, * before replying to a client. */ int invert = fe-\u0026gt;mask \u0026amp; AE_BARRIER; /* Note the \u0026#34;fe-\u0026gt;mask \u0026amp; mask \u0026amp; ...\u0026#34; code: maybe an already * processed event removed an element that fired and we still * didn\u0026#39;t processed, so we check if the event is still valid. * * Fire the readable event if the call sequence is not * inverted. */ if (!invert \u0026amp;\u0026amp; fe-\u0026gt;mask \u0026amp; mask \u0026amp; AE_READABLE) { fe-\u0026gt;rfileProc(eventLoop,fd,fe-\u0026gt;clientData,mask); fired++; } /* Fire the writable event. */ if (fe-\u0026gt;mask \u0026amp; mask \u0026amp; AE_WRITABLE) { if (!fired || fe-\u0026gt;wfileProc != fe-\u0026gt;rfileProc) { fe-\u0026gt;wfileProc(eventLoop,fd,fe-\u0026gt;clientData,mask); fired++; } } /* If we have to invert the call, fire the readable event now * after the writable one. */ if (invert \u0026amp;\u0026amp; fe-\u0026gt;mask \u0026amp; mask \u0026amp; AE_READABLE) { if (!fired || fe-\u0026gt;wfileProc != fe-\u0026gt;rfileProc) { fe-\u0026gt;rfileProc(eventLoop,fd,fe-\u0026gt;clientData,mask); fired++; } } processed++; } } /* 检查是否有时间事件，若有，则调⽤processTimeEvents函数处理 */ if (flags \u0026amp; AE_TIME_EVENTS) processed += processTimeEvents(eventLoop); return processed; /* 返回已经处理的⽂件或时间 */ } // 负责事件和handler注册，在initSever中被调用 int aeCreateFileEvent(aeEventLoop *eventLoop, int fd, int mask, aeFileProc *proc, void *clientData) // 循环流程结构体*eventLoop、IO事件对应的⽂件描述符fd，事件类型掩码mask、事件处理回调函数*proc，以及事件私有数据*clientData { if (fd \u0026gt;= eventLoop-\u0026gt;setsize) { errno = ERANGE; return AE_ERR; } aeFileEvent *fe = \u0026amp;eventLoop-\u0026gt;events[fd]; // 根据传⼊的⽂件描述符fd获取该描述符关联的IO事件指针变量*fe // 添加要监听的事件，Linux中aeApiAddEvent封装了epoll_ctl if (aeApiAddEvent(eventLoop, fd, mask) == -1) return AE_ERR; fe-\u0026gt;mask |= mask; if (mask \u0026amp; AE_READABLE) fe-\u0026gt;rfileProc = proc; if (mask \u0026amp; AE_WRITABLE) fe-\u0026gt;wfileProc = proc; fe-\u0026gt;clientData = clientData; if (fd \u0026gt; eventLoop-\u0026gt;maxfd) eventLoop-\u0026gt;maxfd = fd; return AE_OK; } // ae_epoll.c中的实现，添加事件，调用操作系统的IO多路复用方法 static int aeApiAddEvent(aeEventLoop *eventLoop, int fd, int mask) { aeApiState *state = eventLoop-\u0026gt;apidata; // 从eventLoop结构体中获取aeApiState变量，⾥⾯保存了epoll实例 struct epoll_event ee = {0}; /* 创建epoll_event类型变量 */ /* 如果⽂件描述符fd对应的IO事件已存在，则操作类型为修改，否则为添加. */ int op = eventLoop-\u0026gt;events[fd].mask == AE_NONE ? EPOLL_CTL_ADD : EPOLL_CTL_MOD; ee.events = 0; mask |= eventLoop-\u0026gt;events[fd].mask; /* Merge old events */ // //将可读或可写IO事件类型转换为epoll监听的类型EPOLLIN或EPOLLOUT if (mask \u0026amp; AE_READABLE) ee.events |= EPOLLIN; if (mask \u0026amp; AE_WRITABLE) ee.events |= EPOLLOUT; ee.data.fd = fd; //将要监听的⽂件描述符赋值给ee if (epoll_ctl(state-\u0026gt;epfd,op,fd,\u0026amp;ee) == -1) return -1; // 调⽤epoll_ctl实际创建监听事件 return 0; } Redis是如何处理IO事件和时间事件的？ // 事件驱动框架循环流程的数据结构 typedef struct aeEventLoop { int maxfd; /* highest file descriptor currently registered */ int setsize; /* max number of file descriptors tracked */ long long timeEventNextId; time_t lastTime; /* Used to detect system clock skew */ aeFileEvent *events; /* IO事件 */ aeFiredEvent *fired; /* 记录已触发事件对应的⽂件描述符信息 */ aeTimeEvent *timeEventHead; /* 时间事件 */ int stop; void *apidata; /* 和API调⽤接⼝相关的数据 */ aeBeforeSleepProc *beforesleep; /* 进⼊事件循环流程前执⾏的函数 */ aeBeforeSleepProc *aftersleep; /* 退出事件循环流程后执⾏的函数 */ } aeEventLoop; // server.h 宏定义 #define CONFIG_MIN_RESERVED_FDS 32 #define CONFIG_FDSET_INCR (CONFIG_MIN_RESERVED_FDS+96) // 初始化， initServer()中server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR); aeEventLoop *aeCreateEventLoop(int setsize) { // maxclients在redis.conf中配置，默认1000， aeEventLoop *eventLoop; int i; // 分配内存空间和变量初始化赋值 if ((eventLoop = zmalloc(sizeof(*eventLoop))) == NULL) goto err; eventLoop-\u0026gt;events = zmalloc(sizeof(aeFileEvent)*setsize); eventLoop-\u0026gt;fired = zmalloc(sizeof(aeFiredEvent)*setsize); if (eventLoop-\u0026gt;events == NULL || eventLoop-\u0026gt;fired == NULL) goto err; eventLoop-\u0026gt;setsize = setsize; eventLoop-\u0026gt;lastTime = time(NULL); eventLoop-\u0026gt;timeEventHead = NULL; eventLoop-\u0026gt;timeEventNextId = 0; eventLoop-\u0026gt;stop = 0; eventLoop-\u0026gt;maxfd = -1; eventLoop-\u0026gt;beforesleep = NULL; eventLoop-\u0026gt;aftersleep = NULL; if (aeApiCreate(eventLoop) == -1) goto err; // 实际调用操作系统的IO多路复用函数，Linux中是epoll， // 会调用epoll_create创建epoll实例，同时会创建epoll_event结构的数组，保存在aeApiState结构体中，会赋值给eventLoop中的apidata /* 将所有⽹络IO事件对应⽂件描述符的掩码设置为AE_NONE，后续如果要监听的⽂件描述符fd在数组中的类型不是AE_NONE，则表明该描述符已做过设置 */ for (i = 0; i \u0026lt; setsize; i++) eventLoop-\u0026gt;events[i].mask = AE_NONE; return eventLoop; err: if (eventLoop) { zfree(eventLoop-\u0026gt;events); zfree(eventLoop-\u0026gt;fired); zfree(eventLoop); } return NULL; } 读事件处理流程：\ninitServer中调用aeCreateFileEvent创建可读事件，并设置回调函数为acceptTcpHandler，⽤来处理客⼾端连接 acceptTcpHandler接受客⼾端连接，并创建已连接套接字cfd，然后调用acceptCommonHandler，并传递已连接套接字cfd acceptCommonHandler会调⽤createClient创建客⼾端，并且调用aeCreateFileEvent aeCreateFileEvent会针对已连接套接字创建监听事件，类型为AE_READABLE，对应客⼾端读写请求，回调函数是readQueryFromClient 这样就增加了对⼀个客⼾端已连接套接字的监听，⼀旦客⼾端有请求发送到server，框架就会回调readQueryFromClient函数处理请求 写事件处理流程：\n收到客户端请求后，会调用readQueryFromClient，随后会调用processCommand，处理命令 在处理客⼾端命令后，将要返回的数据写⼊客⼾端输出缓冲区，如图1 在每次循环进⼊事件处理函数前，都会先调用beforeSleep（这是一个循环，执行完一轮请求马上就是进入下一轮循环，所以等于执行完这轮循环就会执行beforeSleep，具体看aeMain） beforeSleep中会调用handleClientsWithPendingWrites handleClientsWithPendingWrites会遍历每⼀个待写回数据的客⼾端，然后调⽤writeToClient函数，将客⼾端输出缓冲区中的数据写回 如果输出缓冲区的数据还没有写完，handleClientsWithPendingWrites会调用aeCreateFileEvent，创建可写事件，设置回调函数sendReplyToClient sendReplyToClient里面会调用writeToClient写回数据 // aeProcessEvents函数会根据事件的可读或可写类型，调⽤相应的回调函数进⾏处理 int aeProcessEvents(aeEventLoop *eventLoop, int flags) { // 调用aeApiPoll获取就绪的描述符 numevents = aeApiPoll(eventLoop, tvp); for (j = 0; j \u0026lt; numevents; j++) { aeFileEvent *fe = \u0026amp;eventLoop-\u0026gt;events[eventLoop-\u0026gt;fired[j].fd]; // 如果触发的是可读事件,调用事件注册时设置的读事件回调处理函数 if (!invert \u0026amp;\u0026amp; fe-\u0026gt;mask \u0026amp; mask \u0026amp; AE_READABLE) { fe-\u0026gt;rfileProc(eventLoop, fd, fe-\u0026gt;clientData, mask); fired++; } // 如果触发的是可写事件,调用事件注册时设置的写事件回调处理函数 if (fe-\u0026gt;mask \u0026amp; mask \u0026amp; AE_WRITABLE) { if (!fired || fe-\u0026gt;wfileProc != fe-\u0026gt;rfileProc) { fe-\u0026gt;wfileProc(eventLoop, fd, fe-\u0026gt;clientData, mask); fired++; } } } } 时间事件处理：\n// 时间事件结构体，时间事件是以链表的形式组织起来的 typedef struct aeTimeEvent { long long id; /* 时间事件ID */ long when_sec; /* 事件到达的秒级时间戳 */ long when_ms; /* 事件到达的毫秒级时间戳 */ aeTimeProc *timeProc; /* 时间事件触发后的处理函数 */ aeEventFinalizerProc *finalizerProc; /* 事件结束后的处理函数 */ void *clientData; /* 事件相关的私有数据 */ struct aeTimeEvent *prev; /* 时间事件链表的前向指针 */ struct aeTimeEvent *next; /* 时间事件链表的后向指针 */ } aeTimeEvent; // 时间事件创建，对te并插⼊到框架循环流程结构体eventLoop中的时间事件链表 long long aeCreateTimeEvent(aeEventLoop *eventLoop, long long milliseconds, //milliseconds表示所创建时间事件的触发时间距离当前时间的时⻓ aeTimeProc *proc, void *clientData, // proc是所创建时间事件触发后的回调函数 aeEventFinalizerProc *finalizerProc) { long long id = eventLoop-\u0026gt;timeEventNextId++; aeTimeEvent *te; te = zmalloc(sizeof(*te)); if (te == NULL) return AE_ERR; te-\u0026gt;id = id; aeAddMillisecondsToNow(milliseconds,\u0026amp;te-\u0026gt;when_sec,\u0026amp;te-\u0026gt;when_ms); te-\u0026gt;timeProc = proc; te-\u0026gt;finalizerProc = finalizerProc; te-\u0026gt;clientData = clientData; te-\u0026gt;prev = NULL; te-\u0026gt;next = eventLoop-\u0026gt;timeEventHead; if (te-\u0026gt;next) te-\u0026gt;next-\u0026gt;prev = te; eventLoop-\u0026gt;timeEventHead = te; return id; } 时间事件回调函数是serverCron，它会顺序调⽤⼀些函数，来实现时间事件被触发后，执⾏⼀些后台任务，还会以不同的频率周期性执⾏⼀些任务，这是通过执⾏宏run_with_period来实现的。\n// serverCron() // 如果收到进程结束信号,则执行server关闭操作 if (server.shutdown_asap) { if (prepareForShutdown(SHUTDOWN_NOFLAGS) == C_OK) exit(0); } clientCron(); // 执行客户端的异步操作 databaseCron(); // 执行数据库的后台操作 // 根据Redis实例配置⽂件redis.conf中定义的hz值，来判断参数_ms_表⽰的时间戳是否到达，⼀旦到达，serverCron就可以执⾏相应的任务，这些任务在run_with_period宏定义的代码块中 #define run_with_period(_ms_) if ((_ms_ \u0026lt;= 1000/server.hz) || !(server.cronloops%((_ms_)/(1000/server.hz))) run_with_period(100) { trackInstantaneousMetric(STATS_METRIC_COMMAND,server.stat_numcommands); trackInstantaneousMetric(STATS_METRIC_NET_INPUT, server.stat_net_input_bytes); trackInstantaneousMetric(STATS_METRIC_NET_OUTPUT, server.stat_net_output_bytes); } run_with_period(1000) {......} 时间事件的触发处理：aeProcessEvents函数在执⾏流程的最后，会调⽤processTimeEvents函数处理相应到时调⽤processTimeEvents函数处理相应到时的任务。\nproecessTimeEvent的基本流程就是从时间事件链表上逐⼀取出每⼀个事件，然后根据当前时间判断该事件的触发时间戳是否已满⾜。如果已满⾜，那么就调⽤该事件对应的回调函数进⾏处理。\naeProcessEvents() { ... // 检测时间事件是否触发 if (flags \u0026amp; AE_TIME_EVENTS) processed += processTimeEvents(eventLoop); ... } 执行模型 执行./redis-server /etc/redis/redis.conf后的过程：\n在shell进程执行redis-server后会fork()一个子进程 然后会调用execve系统调用函数，将⼦进程执⾏的主体替换成Redis的可执⾏⽂件，然后就会执行main函数了 然后就会执行initServerConfig初始化运行参数，调用loadServerConfig加载配置文件参数 完成参数解析后，会根据两个配置参数daemonize和supervised来决定是否以守护进程方式运行 daemonize表示是否要设置Redis以守护进程⽅式运⾏ supervised表示是否使用upstart或者systemd来管理Redis // 如果配置参数daemonize为1, supervised值为0,那么设置background值为1,否则,设置其为0。 int main(int argc, char ** argv) { int background = server.daemonize \u0026amp;\u0026amp; !server.supervised; // 如果background值为1,调用daemonize函数。 if (background) daemonize(); ... // fork返回值小于0,则失败；=0表示当前在子进程上运行，\u0026gt;0表示当前在父进程上运行 void daemonize(void) { if (fork() != 0) exit(0); // fork成功执行或失败,则父进程退出，代替原来的父进程，以守护进程方式运行 setsid(); // 创建新的session } 除了主IO线程，Redis还会启动一些后台线程，包括⽂件关闭后台任务、AOF⽇志同步写回后台任务、惰性删除后台任务\n启动后台线程的步骤：\nRedis是先通过bioInit函数初始化和创建后台线程； 后台线程运⾏的是bioProcessBackgroundJobs函数，这个函数会轮询任务队列，并根据要处理的任务类型，调⽤相应函数进⾏处理； 后台线程要处理的任务是由bioCreateBackgroundJob函数来创建的，这些任务创建后会被放到任务队列中，等待bioProcessBackgroundJobs函数处理 这种设计方式是生产者-消费者模型： // bio.c中相关的数组 // 保存线程描述符的数组 static pthread_t bio_threads[BIO_NUM_OPS]; // 保存互斥锁的数组 static pthread_mutex_t bio_mutex[BIO_NUM_OPS]; // 保存条件变量的两个数组 static pthread_cond_t bio_newjob_cond[BIO_NUM_OPS]; static pthread_cond_t bio_step_cond[BIO_NUM_OPS]; #define BIO_CLOSE_FILE 0 /* ⽂件关闭后台任务 */ #define BIO_AOF_FSYNC 1 /* AOF⽇志同步写回后台任务 */ #define BIO_LAZY_FREE 2 /* 惰性删除后台任务 */ #define BIO_NUM_OPS 3 struct bio_job { time_t time; // 任务创建时间 void *arg1, *arg2, *arg3; // 任务参数 }; // 以后台线程方式运行的任务列表 static list *bio_jobs[BIO_NUM_OPS]; // 被阻塞的后台任务数组 static unsigned long long bio_pending[BIO_NUM_OPS]; // 初始化 void bioInit(void) { pthread_attr_t attr; pthread_t thread; size_t stacksize; int j; /* Initialization of state vars and objects */ for (j = 0; j \u0026lt; BIO_NUM_OPS; j++) { pthread_mutex_init(\u0026amp;bio_mutex[j],NULL); pthread_cond_init(\u0026amp;bio_newjob_cond[j],NULL); pthread_cond_init(\u0026amp;bio_step_cond[j],NULL); bio_jobs[j] = listCreate(); // 为每个元素创建一个列表，bio_jobs数组元素为bio_job结构体类型，⽤来表⽰后台任务 bio_pending[j] = 0; // 表⽰每种任务中，处于等待状态的任务个数 } /* 计算栈⼤⼩属性值 */ pthread_attr_init(\u0026amp;attr); // 初始化线程属性变量attr pthread_attr_getstacksize(\u0026amp;attr,\u0026amp;stacksize); // 获取线程的栈⼤⼩这⼀属性的当前值 if (!stacksize) stacksize = 1; /* 如果为0就设置为1 */ while (stacksize \u0026lt; REDIS_THREAD_STACK_SIZE) stacksize *= 2; // 如果小于REDIS_THREAD_STACK_SIZE（默认4MB）就*2， pthread_attr_setstacksize(\u0026amp;attr, stacksize); /* Ready to spawn our threads. We use the single argument the thread * function accepts in order to pass the job ID the thread is * responsible of. */ for (j = 0; j \u0026lt; BIO_NUM_OPS; j++) { void *arg = (void*)(unsigned long) j; if (pthread_create(\u0026amp;thread,\u0026amp;attr,bioProcessBackgroundJobs,arg) != 0) { // 调用pthread_create创建线程 serverLog(LL_WARNING,\u0026#34;Fatal: Can\u0026#39;t initialize Background Jobs.\u0026#34;); exit(1); } bio_threads[j] = thread; } } // 处理后台任务 void *bioProcessBackgroundJobs(void *arg) { struct bio_job *job; unsigned long type = (unsigned long) arg; // 后台任务的操作码 sigset_t sigset; /* Check that the type is within the right interval. */ if (type \u0026gt;= BIO_NUM_OPS) { serverLog(LL_WARNING, \u0026#34;Warning: bio thread started with wrong type %lu\u0026#34;,type); return NULL; } /* Make the thread killable at any time, so that bioKillThreads() * can work reliably. */ pthread_setcancelstate(PTHREAD_CANCEL_ENABLE, NULL); pthread_setcanceltype(PTHREAD_CANCEL_ASYNCHRONOUS, NULL); pthread_mutex_lock(\u0026amp;bio_mutex[type]); /* Block SIGALRM so we are sure that only the main thread will * receive the watchdog signal. */ sigemptyset(\u0026amp;sigset); sigaddset(\u0026amp;sigset, SIGALRM); if (pthread_sigmask(SIG_BLOCK, \u0026amp;sigset, NULL)) serverLog(LL_WARNING, \u0026#34;Warning: can\u0026#39;t mask SIGALRM in bio.c thread: %s\u0026#34;, strerror(errno)); while(1) { // 循环从bio_jobs这个数组中取出相应任务，并根据任务类型，调⽤具体的函数来执⾏ listNode *ln; /* The loop always starts with the lock hold. */ if (listLength(bio_jobs[type]) == 0) { pthread_cond_wait(\u0026amp;bio_newjob_cond[type],\u0026amp;bio_mutex[type]); continue; } /* 从类型为type的任务队列中获取第⼀个任务 */ ln = listFirst(bio_jobs[type]); job = ln-\u0026gt;value; /* It is now possible to unlock the background system as we know have * a stand alone job structure to process.*/ pthread_mutex_unlock(\u0026amp;bio_mutex[type]); /* 判断当前处理的后台任务类型是哪⼀种 */ if (type == BIO_CLOSE_FILE) { // //如果是关闭⽂件任务，那就调⽤close函数 close((long)job-\u0026gt;arg1); } else if (type == BIO_AOF_FSYNC) { // 如果是AOF同步写任务，那就调⽤redis_fsync函数 redis_fsync((long)job-\u0026gt;arg1); } else if (type == BIO_LAZY_FREE) { // 如果是惰性删除任务，那根据任务的参数分别调⽤不同的惰性删除函数执⾏ /* What we free changes depending on what arguments are set: * arg1 -\u0026gt; free the object at pointer. * arg2 \u0026amp; arg3 -\u0026gt; free two dictionaries (a Redis DB). * only arg3 -\u0026gt; free the skiplist. */ if (job-\u0026gt;arg1) lazyfreeFreeObjectFromBioThread(job-\u0026gt;arg1); else if (job-\u0026gt;arg2 \u0026amp;\u0026amp; job-\u0026gt;arg3) lazyfreeFreeDatabaseFromBioThread(job-\u0026gt;arg2,job-\u0026gt;arg3); else if (job-\u0026gt;arg3) lazyfreeFreeSlotsMapFromBioThread(job-\u0026gt;arg3); } else { serverPanic(\u0026#34;Wrong job type in bioProcessBackgroundJobs().\u0026#34;); } zfree(job); /* Lock again before reiterating the loop, if there are no longer * jobs to process we\u0026#39;ll block again in pthread_cond_wait(). */ pthread_mutex_lock(\u0026amp;bio_mutex[type]); listDelNode(bio_jobs[type],ln); // 任务执⾏完成后，调⽤listDelNode在任务队列中删除该任务 bio_pending[type]--; // 将对应的等待任务个数减⼀ /* Unblock threads blocked on bioWaitStepOfType() if any. */ pthread_cond_broadcast(\u0026amp;bio_step_cond[type]); } } // 创建后台任务， 在aof.c等文件中会调用 void bioCreateBackgroundJob(int type, void *arg1, void *arg2, void *arg3) { struct bio_job *job = zmalloc(sizeof(*job)); //创建新的任务 // 设置任务数据结构中的参数 job-\u0026gt;time = time(NULL); job-\u0026gt;arg1 = arg1; job-\u0026gt;arg2 = arg2; job-\u0026gt;arg3 = arg3; pthread_mutex_lock(\u0026amp;bio_mutex[type]); listAddNodeTail(bio_jobs[type],job); // 将任务加到bio_jobs数组的对应任务列表中 bio_pending[type]++; // 将对应任务列表上等待处理的任务个数加1 pthread_cond_signal(\u0026amp;bio_newjob_cond[type]); pthread_mutex_unlock(\u0026amp;bio_mutex[type]); } Redis 6.0 多IO线程模型 Redis 6.0 之前，处理客⼾端请求是单线程，这种模型的缺点是，只能⽤到「单核」CPU。如果并发量很⾼，那么在读写客⼾端数据时，容易引发性能瓶颈，所以 Redis 6.0 引⼊了多IO线程解决这个问题\n在bioInit后调用了initThreadedIO()来初始化多IO线程\n// 基于Redis 6.0.15 // server.c void InitServerLast() { bioInit(); initThreadedIO(); //调用initThreadedIO函数初始化IO线程 set_jemalloc_bg_thread(server.jemalloc_bg_thread); server.initial_memory_usage = zmalloc_used_memory(); } // networking.c pthread_t io_threads[IO_THREADS_MAX_NUM]; // 记录线程描述符的数组 pthread_mutex_t io_threads_mutex[IO_THREADS_MAX_NUM]; // 记录线程互斥锁的数组 _Atomic unsigned long io_threads_pending[IO_THREADS_MAX_NUM]; // 记录线程待处理的客户端个数 list *io_threads_list[IO_THREADS_MAX_NUM]; // 记录线程对应处理的客户端 // 初始化多IO线程 void initThreadedIO(void) { server.io_threads_active = 0; /* 初始化为0，表示没激活 */ /* 如果IO线程数量为1就直接返回 */ if (server.io_threads_num == 1) return; // 如果IO线程数量大于（默认128），就报错退出 if (server.io_threads_num \u0026gt; IO_THREADS_MAX_NUM) { serverLog(LL_WARNING,\u0026#34;Fatal: too many I/O threads configured. \u0026#34; \u0026#34;The maximum number is %d.\u0026#34;, IO_THREADS_MAX_NUM); exit(1); } /* 为每个IO线程初始化 */ for (int i = 0; i \u0026lt; server.io_threads_num; i++) { /* io_threads_list保存每个IO线程要处理的客户端，初始化为列表 */ io_threads_list[i] = listCreate(); if (i == 0) continue; /* 编号从1开始，0为主IO线程 */ /* Things we do only for the additional threads. */ pthread_t tid; pthread_mutex_init(\u0026amp;io_threads_mutex[i],NULL); io_threads_pending[i] = 0; // io_threads_pending保存等待每个IO线程处理的客户端个数 pthread_mutex_lock(\u0026amp;io_threads_mutex[i]); /* pthread_mutex_lock保存线程互斥锁 */ if (pthread_create(\u0026amp;tid,NULL,IOThreadMain,(void*)(long)i) != 0) { // 创建线程 serverLog(LL_WARNING,\u0026#34;Fatal: Can\u0026#39;t initialize IO thread.\u0026#34;); exit(1); } io_threads[i] = tid; // io_threads保存每个IO线程的描述符 } } // 创建IO线程 void *IOThreadMain(void *myid) { /* The ID is the thread number (from 0 to server.iothreads_num-1), and is * used by the thread to just manipulate a single sub-array of clients. */ long id = (unsigned long)myid; char thdname[16]; snprintf(thdname, sizeof(thdname), \u0026#34;io_thd_%ld\u0026#34;, id); redis_set_thread_title(thdname); redisSetCpuAffinity(server.server_cpulist); makeThreadKillable(); while(1) { /* Wait for start */ for (int j = 0; j \u0026lt; 1000000; j++) { if (io_threads_pending[id] != 0) break; } /* Give the main thread a chance to stop this thread. */ if (io_threads_pending[id] == 0) { pthread_mutex_lock(\u0026amp;io_threads_mutex[id]); pthread_mutex_unlock(\u0026amp;io_threads_mutex[id]); continue; } serverAssert(io_threads_pending[id] != 0); if (tio_debug) printf(\u0026#34;[%ld] %d to handle\\n\u0026#34;, id, (int)listLength(io_threads_list[id])); /* Process: note that the main thread will never touch our list * before we drop the pending count to 0. */ listIter li; listNode *ln; listRewind(io_threads_list[id],\u0026amp;li); // 获取IO线程要处理的客⼾端列表 while((ln = listNext(\u0026amp;li))) { client *c = listNodeValue(ln); // 从客⼾端列表中获取⼀个客⼾端 if (io_threads_op == IO_THREADS_OP_WRITE) { // 写操作， writeToClient(c,0); // 调⽤writeToClient将数据写回客⼾端 } else if (io_threads_op == IO_THREADS_OP_READ) { // 读操作 readQueryFromClient(c-\u0026gt;conn); // 调⽤readQueryFromClient从客⼾端读取数据 } else { serverPanic(\u0026#34;io_threads_op value is unknown\u0026#34;); } } listEmpty(io_threads_list[id]); // 处理完所有客⼾端后，清空该线程的客⼾端列表 io_threads_pending[id] = 0; // 置为0 if (tio_debug) printf(\u0026#34;[%ld] Done\\n\u0026#34;, id); } } IO线程要处理的客⼾端是如何添加到io_threads_list数组中的？\n在处理可读事件的回调函数readQueryFromClient中，会调用postponeClientRead方法，如果满足条件，会将客户端添加到clients_pending_read数组中。 addReply()-\u0026gt;prepareClientToWrite()-\u0026gt;clientHasPendingReplies()，如果满足条件，会将客户端添加到clients_pending_writet数组中。 clients_pending_read数组和clients_pending_writet数组是server的两个成员变量\nvoid readQueryFromClient(connection *conn) { client *c = connGetPrivateData(conn); //从连接数据结构中获取客户端 ··· if (postponeClientRead(c)) return; //判断是否推迟从客户端读取数据 } int postponeClientRead(client *c) { //判断IO线程是否激活, if (server.io_threads_active \u0026amp;\u0026amp; server.io_threads_do_reads \u0026amp;\u0026amp; !ProcessingEventsWhileBlocked \u0026amp;\u0026amp; !(c-\u0026gt;flags \u0026amp; (CLIENT_MASTER | CLIENT_SLAVE | CLIENT_PENDING_READ))) { c-\u0026gt;flags |= CLIENT_PENDING_READ; //给客户端的flag添加CLIENT_PENDING_READ标记,表示推迟该客户端的读操作 listAddNodeHead(server.clients_pending_read,c);//将客户端添加到clients_pending_read列表中 return 1; } else { return 0; } } void clientInstallWriteHandler(client *c) { //如果客户端没有设置过CLIENT_PENDING_WRITE标识,并且客户端没有在进行主从复制,或者客户端是主从复制中的从节点,已经能接收请求 if (!(c-\u0026gt;flags \u0026amp; CLIENT_PENDING_WRITE) \u0026amp;\u0026amp; (c-\u0026gt;replstate == REPL_STATE_NONE || (c-\u0026gt;replstate == SLAVE_STATE_ONLINE \u0026amp;\u0026amp; !c-\u0026gt;repl_put_online_on_ack))) { //将客户端的标识设置为待写回,即CLIENT_PENDING_WRITE c-\u0026gt;flags |= CLIENT_PENDING_WRITE; listAddNodeHead(server.clients_pending_write, c); //将可获得加入clients_pending_write列表 } } 如何把待读/写客⼾端分配给IO线程执⾏？ 通过以下两个方法，在beforeSleep中被调用\nhandleClientsWithPendingReadsUsingThreads函数：该函数主要负责将clients_pending_read列表中的客⼾端分配给IO线程进⾏处理。 handleClientsWithPendingWritesUsingThreads函数：该函数主要负责将clients_pending_write列表中的客⼾端分配给IO线程进⾏处理。 int handleClientsWithPendingReadsUsingThreads(void) { // 根据全局变量server的io_threads_active成员变量，判定IO线程是否激活，并且根据server的io_threads_do_reads成员变量，判定⽤⼾是否设置了Redis可以⽤IO线程处理待读客⼾端 if (!server.io_threads_active || !server.io_threads_do_reads) return 0; int processed = listLength(server.clients_pending_read); if (processed == 0) return 0; if (tio_debug) printf(\u0026#34;%d TOTAL READ pending clients\\n\u0026#34;, processed); /* Distribute the clients across N different lists. */ listIter li; listNode *ln; listRewind(server.clients_pending_read,\u0026amp;li); // 获取clients_pending_read列表的⻓度，这代表了要处理的待读客⼾端个数 int item_id = 0; while((ln = listNext(\u0026amp;li))) { client *c = listNodeValue(ln); // 逐⼀取出待处理的客⼾端 int target_id = item_id % server.io_threads_num; // 对IO线程数量进⾏取模运算 listAddNodeTail(io_threads_list[target_id],c); // 根据余数把客户端分给对应的IO线程 item_id++; } /* 遍历io_threads_list数组中的每个元素列表⻓度，等待每个线程处理的客⼾端数量，赋值给io_threads_pending数组 */ io_threads_op = IO_THREADS_OP_READ; for (int j = 1; j \u0026lt; server.io_threads_num; j++) { int count = listLength(io_threads_list[j]); io_threads_pending[j] = count; } /* 取出0号列表，让主IO线程来处理 */ listRewind(io_threads_list[0],\u0026amp;li); while((ln = listNext(\u0026amp;li))) { client *c = listNodeValue(ln); readQueryFromClient(c-\u0026gt;conn); } listEmpty(io_threads_list[0]); /* 等待所有IO线程完成待读客⼾端的处理 */ while(1) { unsigned long pending = 0; for (int j = 1; j \u0026lt; server.io_threads_num; j++) pending += io_threads_pending[j]; if (pending == 0) break; } if (tio_debug) printf(\u0026#34;I/O READ All threads finshed\\n\u0026#34;); /* 再次遍历⼀遍clients_pending_read列表，依次取出其中的客⼾端，判断客⼾端的标识中是否有CLIENT_PENDING_COMMAND，有则表明已经被执行 */ while(listLength(server.clients_pending_read)) { ln = listFirst(server.clients_pending_read); client *c = listNodeValue(ln); c-\u0026gt;flags \u0026amp;= ~CLIENT_PENDING_READ; listDelNode(server.clients_pending_read,ln); /* Clients can become paused while executing the queued commands, * so we need to check in between each command. If a pause was * executed, we still remove the command and it will get picked up * later when clients are unpaused and we re-queue all clients. */ if (clientsArePaused()) continue; if (processPendingCommandsAndResetClient(c) == C_ERR) { /* If the client is no longer valid, we avoid * processing the client later. So we just go * to the next. */ continue; } // 解析客⼾端中所有命令并执⾏ processInputBuffer(c); /* We may have pending replies if a thread readQueryFromClient() produced * replies and did not install a write handler (it can\u0026#39;t). */ if (!(c-\u0026gt;flags \u0026amp; CLIENT_PENDING_WRITE) \u0026amp;\u0026amp; clientHasPendingReplies(c)) clientInstallWriteHandler(c); } /* Update processed count on server */ server.stat_io_reads_processed += processed; return processed; } // handleClientsWithPendingWritesUsingThreads的不同在于 // 会判断IO线程数量是否为1，或者待写客⼾端数量是否⼩于IO线程数量的2倍。是否⼩于IO线程数量的2倍 // 如果是则不会用多线程来处理，而是调用handleClientsWithPendingWrites由主线程处理 // 主要是为了在待写客⼾端数量不多时，避免采⽤多线程，从⽽节省CPU开销 多IO线程本⾝并不会执⾏命令，它们只是利⽤多核并⾏地读取数据和解析命令，或是将server数据写回。所以，Redis执⾏命令的线程还是主IO线程\n一条命令的执行过程 Redis和一个客户端建立连接后，就会在事件驱动框架中注册可读事件，回调函数就是readQueryFromClient，基本流程：\n命令读取，readQueryFromClient函数 通过read函数读取命令 是否读取异常，有则异常处理 是否是主节点客户端，是则将数据追加到⽤于主从节点命令同步的缓冲区中 然后调用processInputBufferAndReplicate 命令解析，processInputBufferAndReplicate函数 是否有CLIENT_MASTER标记，没有则直接调用processInputBuffer函数，对客⼾端输⼊缓冲区中的命令和参数进⾏解析 有则除调用processInputBuffer外，还要调用replicationFeedSlavesFromMasterStream，将接收的命令同步给从节点 命令解析实际在processInputBuffer函数中 执行while循环读取数据，判断读取到的命令格式，是否以“*”开头 如果是则是ROTO_REQ_MULTIBULK类型的命令，使用processMultibulkBuffer进行解析 如果不是则是ROTO_REQ_INLINE类型的命令，调用processInlineBuffer解析 调用processCommand执行命令 命令执行，processCommand函数 调用moduleCallCommandFilters函数，将Redis命令替换成module中想要替换的命令 判断当前命令是否为quit命令，并进⾏相应处理 调⽤lookupCommand函数，在全局变量server的commands哈希表中查找相关的命令 进⾏多种检查，⽐如命令的参数是否有效、发送命令的⽤⼾是否进⾏过验证、当前内存的使⽤情况 判断当前客户端是否有CLIENT_MULTI标记，有的话，就表明要处理的是Redis事务的相关命令，会按照事务的要求，调⽤queueMultiCommand函数将命令⼊队保存，等待后续⼀起处理 没有则调⽤call函数来实际执⾏命令 结果返回，addReply函数 调用prepareClientToWrite-\u0026gt;clientInstallWriteHandler,待写回客⼾端加⼊到全局变量server的clients_pending_write列表中 调⽤_addReplyToBuffer等函数，将要返回的结果添加到客⼾端的输出缓冲区中 IO多路复⽤机制是在readQueryFromClient函数执⾏前发挥作⽤的\n缓存模块 LRU 最近最少使⽤（Least Recently Used，LRU）算法：从基本原理上来说，LRU算法会使⽤⼀个链表来维护缓存中每⼀个数据的访问情况，并根据数据的实时访问，调整数据在链表中的位置，然后通过数据在链表中的位置，来表⽰数据是最近刚访问的，还是已经有⼀段时间没有访问了\n主要分三种情况：\n新数据插入，插入到链表头部 访问数据，将对应节点移至头部 链表长度超过阈值，删除尾节点 在Redis中是实现了一个近似LRU算法，分为三步：\n判断当前内存使⽤情况 freeMemoryIfNeeded函数会调⽤getMaxmemoryState函数，评估当前的内存使⽤情况 如果当前内存使⽤量没有超过maxmemory，就会直接返回C_OK getMaxmemoryState如果发现已⽤内存超出了maxmemory，它就会计算需要释放的内存量 getMaxmemoryState在计算已使⽤的内存量时会减去主从复制的复制缓冲区⼤⼩ 如果当前server使⽤的内存量，超出maxmemory的上限了，freeMemoryIfNeeded就会执行while循环来淘汰数据 更新待淘汰的候选键值对集合 Redis中定义了⼀个数组EvictionPoolLRU，⽤来保存待淘汰的候选键值对，元素类型位evictionPoolEntry，在initServer中会调用evictionPoolAlloc为它分配内存，大小由EVPOOL_SIZE决定，默认16 在while循环中，会更新这个EvictionPoolLRU数组 freeMemoryIfNeeded会调用evictionPoolPopulate，evictionPoolPopulate又调用dictGetSomeKeys，从带采样的哈希表中随机获取一定数量的key maxmemory_policy如果时allkeys_lru，则从全局哈希表中采样，否则就是在设置了过期时间的key的哈希表 dictGetSomeKeys采样的数量由maxmemory-samples决定，默认为5 获取到采样的键值对集合后，evictionPoolPopulate调用estimateObjectIdleTime来计算每个键值对的空闲时间 然后evictionPoolPopulate遍历EvictionPoolLRU数组，如果能在数组中找到⼀个尚未插⼊键值对的空位或者能在数组中找到⼀个空闲时间⼩于采样键值对空闲时间的键值对，就将采样的键值对插入数组 选择被淘汰的键值对并删除 evictionPoolPopulate函数更新完EvictionPoolLRU数组后，key是按空闲时间从⼩到⼤排好序的 freeMemoryIfNeeded遍历EvictionPoolLRU数组，从数组的最后⼀个key开始，如果选到的key不是空值，那么就把它作为最终淘汰的key 对于需要被淘汰的key，如果Redis配置了惰性删除，则进行异步惰性删除，否则同步删除 如果淘汰后还没有达到要释放的空间大小，freeMemoryIfNeeded函数还会重复执⾏更新EvictionPoolLRU数组和淘汰key的步骤 static struct evictionPoolEntry *EvictionPoolLRU; struct evictionPoolEntry { unsigned long long idle; // 待淘汰的键值对的空闲时间 sds key; // 待淘汰的键值对的key sds cached; // 缓存的SDS对象 int dbid; // 待淘汰键值对的key所在的数据库ID }; redis.conf中的两个配置：\nmaxmemory，该配置项设定了Redis server可以使⽤的最⼤内存容量，⼀旦server使⽤的实际内存量超出该阈值时，server就会根据maxmemory-policy配置项定义的策略，执⾏内存淘汰操作； maxmemory-policy，该配置项设定了Redis server的内存淘汰策略，主要包括近似LRU算法、LFU算法、按TTL值淘汰和随机淘汰等算法 ⼀旦我们设定了maxmemory选项，并且将maxmemory-policy配置为allkeys-lru或是volatile-lru时，近似LRU算法就被启⽤了。\n全局LRU时钟值的计算： 在redisOjbect中用24bits来保存LRU时钟信息，在server中还有一个成员变量lruclock来保存全局时钟，在initServerConfig时调用getLRUClock设置，getLRUClock会获取当前的时间戳（单位为毫秒），然后除以LRU_CLOCK_RESOLUTION（默认1000），所以LRU时钟精度为1秒，并且与LRU_CLOCK_MAX进行与运算，这样就得到了全局LRU时钟值，全局LRU时钟值的更新在时间事件的回调函数serverCron中，频率由redis.conf中的hz决定，默认为10，即每100毫秒（1秒/10）执行一次。\n而键值对的LRU时钟值的初始化在createObject中，如果采用LFU算法就初始化为LFU算法的计算值，如果是LRU算法就调用LRU_CLOCK来设置，LRU_CLOCK会返回当前的全局LRU时钟值。当键值对被访问时，访问操作的最后就会调用lookupKey，LRU时钟值就会被更新，也是根据maxmemory_policy来设置的。\ntypedef struct redisObject { unsigned type: 4; // 类型 unsigned encoding: 4; // 编码方式 unsigned lru: LRU_BITS; // 记录LRU信息，宏定义LRU_BITS是24 bits int refcount; // 引用计数 void *ptr; // 指向实际数据的指针 } robj; // 初始化 robj *createObject(int type, void *ptr) { robj *o = zmalloc(sizeof(*o)); // ... 其他代码 if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_LFU) { o-\u0026gt;lru = (LFUGetTimeInMinutes() \u0026lt;\u0026lt; 8) | LFU_INIT_VAL; // 使用LFU算法时，lru变量包括以分钟为精度的UNIX时间戳和访问次数 } else { o-\u0026gt;lru = LRU_CLOCK(); // 调⽤LRU_CLOCK函数获取LRU时钟值 } return o; } // 更新 robj *lookupKey(redisDb *db, robj *key, int flags) { // ... 其他代码 if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_LFU) { updateLFU(val); // 使用LFU算法时，调用updateLFU函数更新访问频率 } else { val-\u0026gt;lru = LRU_CLOCK(); // 使用LRU算法时，调用LRU_CLOCK } } LFU 最不频繁使⽤（Least Frequently Used，LFU）算法，会把最不频繁访问的数据淘汰。LFU算法会记录每个数据的访问次数，访问频率是指在⼀定时间内的访问次数，在计算访问频率时，我们不仅需要记录访问次数，还要记录这些访问是在多⻓时间内执⾏的。\nRedis中LFU算法的实现： maxmemory-policy可以设置为allkeys-lfu或是volatile-lfu，表⽰淘汰的键值对会分别从所有键值对或是设置了过期时间的键值对中筛选。 为了节省内存，redis复用了24bits的lru变量来记录LFU算法所需的访问频率信息。其中，低8bits用于记录访问频率，高16bits用于记录上一次访问时间戳。仍然是在createObject中被初始化，前16位通过LFUGetTimeInMinutes计算得到，后8位被设置为LFU_INIT_VAL（默认为5）。当被访问时，也是在lookupKey中调用updateLFU来更新。淘汰数据的步骤和LRU一样，不同点在于键值对的空闲时间，LRU是通过成员变量idle来记录距上次访问的空闲时间，LFU则是通过255减去键值对的访问次数来计算idle的，在计算前会调用LFUDecrAndReturn进行衰减一次。\nvoid updateLFU(robj *val) { unsigned long counter = LFUDecrAndReturn(val); // 根据距离上次访问的时⻓，衰减访问次数 counter = LFULogIncr(counter); // 增加键值对的访问次数 val-\u0026gt;lru = (LFUGetTimeInMinutes()\u0026lt;\u0026lt;8) | counter; // 更新lru值，通过调用LFUGetTimeInMinutes获取当前时间戳，并组合 } // ⾸先获取当前键值对的上⼀次访问时间，然后根据全局变量server的lru_decay_time成员变量的取值，来计算衰减的⼤⼩num_period unsigned long LFUDecrAndReturn(robj *o) { unsigned long ldt = o-\u0026gt;lru \u0026gt;\u0026gt; 8; // 获取当前键值对的上一次访问时间 unsigned long counter = o-\u0026gt;lru \u0026amp; 255; // 获取当前的访问次数 // 计算访问次数的衰减⼤⼩，LFUTimeElapsed是计算距离键值对的上⼀次访问已经过去的时⻓ // 在默认情况下，访问次数的衰减⼤⼩就是等于上⼀次访问距离当前的分钟数 unsigned long num_periods = server.lfu_decay_time ? LFUTimeElapsed(ldt) / server.lfu_decay_time : 0; if (num_periods) // 如果衰减大小不为0 // 如果衰减大小小于当前访问次数，那么衰减后的访问次数是当前访问次数减去衰减大小；否则，衰减后的访问次数等于0 counter = (num_periods \u0026gt; counter) ? 0 : counter - num_periods; return counter; // 如果衰减大小为0，则返回原来的访问次数 } // 增加键值对的访问次数 uint8_t LFULogIncr(uint8_t counter) { if (counter == 255) return 255; // 如果访问次数已经达到最大值，则返回最大值 double r = (double)rand()/RAND_MAX; // 0-1的概率值 double baseval = counter - LFU_INIT_VAL; if (baseval \u0026lt; 0) baseval = 0; double p = 1.0/(baseval*server.lfu_log_factor+1); // 根据baseval和lfu_log_factor计算阈值p if (r \u0026lt; p) counter++; // 概率小于p才加一 return counter; } lazy free惰性删除 // redis.conf中的四个配置项，分别对应四种场景 lazyfree-lazy-eviction no // 控制缓存淘汰策略触发时是否使用惰性删除 lazyfree-lazy-expire no // 控制过期key清理时是否使用惰性删除 lazyfree-lazy-server-del no // 控制用户显式删除操作是否使用惰性删除 replica-lazy-flush no // 控制从节点全量同步时是否使用惰性删除 删除被淘汰数据的过程：\n/* * 函数名：freeMemoryIfNeeded * 功能描述：根据服务器配置的最大内存限制和当前使用的内存情况， * 按照指定的淘汰策略删除部分键以释放内存。 * 参数说明：无 * 返回值说明： * - C_OK 表示成功释放了足够的内存或无需释放； * - C_ERR 表示无法满足内存释放需求（例如使用 NO_EVICTION 策略）。 */ int freeMemoryIfNeeded(void) { /* 默认情况下，从节点应忽略 maxmemory 设置， * 只作为主节点数据的精确副本。*/ if (server.masterhost \u0026amp;\u0026amp; server.repl_slave_ignore_maxmemory) return C_OK; size_t mem_reported, mem_tofree, mem_freed; mstime_t latency, eviction_latency; long long delta; int slaves = listLength(server.slaves); /* 当客户端被暂停时，整个数据集应该是静态不变的， * 不仅是不能写入，而且也不能执行过期键清理与内存淘汰操作。*/ if (clientsArePaused()) return C_OK; /* 获取当前内存状态，如果尚未达到最大内存限制则直接返回 OK。*/ if (getMaxmemoryState(\u0026amp;mem_reported,NULL,\u0026amp;mem_tofree,NULL) == C_OK) return C_OK; mem_freed = 0; /* 如果采用的是不允许淘汰的策略，则跳转到错误处理逻辑。*/ if (server.maxmemory_policy == MAXMEMORY_NO_EVICTION) goto cant_free; /* 需要释放内存但策略禁止这样做。*/ latencyStartMonitor(latency); while (mem_freed \u0026lt; mem_tofree) { int j, k, i, keys_freed = 0; static unsigned int next_db = 0; sds bestkey = NULL; int bestdbid; redisDb *db; dict *dict; dictEntry *de; /* 处理基于 LRU、LFU 或 TTL 的淘汰策略 */ if (server.maxmemory_policy \u0026amp; (MAXMEMORY_FLAG_LRU|MAXMEMORY_FLAG_LFU) || server.maxmemory_policy == MAXMEMORY_VOLATILE_TTL) { struct evictionPoolEntry *pool = EvictionPoolLRU; while(bestkey == NULL) { unsigned long total_keys = 0, keys; /* 在所有数据库中采样键并填充淘汰池 */ for (i = 0; i \u0026lt; server.dbnum; i++) { db = server.db+i; dict = (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_ALLKEYS) ? db-\u0026gt;dict : db-\u0026gt;expires; if ((keys = dictSize(dict)) != 0) { evictionPoolPopulate(i, dict, db-\u0026gt;dict, pool); total_keys += keys; } } if (!total_keys) break; /* 没有可淘汰的键 */ /* 从最优到最差依次尝试选择一个键进行淘汰 */ for (k = EVPOOL_SIZE-1; k \u0026gt;= 0; k--) { if (pool[k].key == NULL) continue; bestdbid = pool[k].dbid; /* 根据策略决定是从 dict 还是 expires 字典查找键 */ if (server.maxmemory_policy \u0026amp; MAXMEMORY_FLAG_ALLKEYS) { de = dictFind(server.db[pool[k].dbid].dict, pool[k].key); } else { de = dictFind(server.db[pool[k].dbid].expires, pool[k].key); } /* 清除池中的条目信息 */ if (pool[k].key != pool[k].cached) sdsfree(pool[k].key); pool[k].key = NULL; pool[k].idle = 0; /* 若该键存在，则选为待淘汰键；否则继续检查下一个 */ if (de) { bestkey = dictGetKey(de); break; } else { /* 键已不存在（ghost），继续遍历 */ } } } } /* 处理随机淘汰策略（volatile-random 和 allkeys-random） */ else if (server.maxmemory_policy == MAXMEMORY_ALLKEYS_RANDOM || server.maxmemory_policy == MAXMEMORY_VOLATILE_RANDOM) { /* 尝试在每个数据库中随机选取一个键用于淘汰 */ for (i = 0; i \u0026lt; server.dbnum; i++) { j = (++next_db) % server.dbnum; db = server.db+j; dict = (server.maxmemory_policy == MAXMEMORY_ALLKEYS_RANDOM) ? db-\u0026gt;dict : db-\u0026gt;expires; if (dictSize(dict) != 0) { de = dictGetRandomKey(dict); bestkey = dictGetKey(de); bestdbid = j; break; } } } /* 实际执行键的删除操作 */ if (bestkey) { db = server.db+bestdbid; robj *keyobj = createStringObject(bestkey,sdslen(bestkey)); // 先给被淘汰的key创建一个sds对象 propagateExpire(db,keyobj,server.lazyfree_lazy_eviction); // 调用propagateExpire // 它会先创建一个redisObject结构体数组，第一个元素是删除操作对应的命令对象，第二个元素是被删除的key对象 // 第一个命令对象会使用shared变量中的unlink对象或者delete对象 // 然后判断是否启用了AOF日志，如果启用了就调用feedAppendOnlyFile先将删除操作记录到AOF日志中 // 然后调用replicationFeedSlaves把删除操作同步给从节点，调用decrRefCount释放参数对象的引用计数 /* 记录删除前后的内存变化量，并实际执行删除 */ delta = (long long) zmalloc_used_memory(); latencyStartMonitor(eviction_latency); // 实际删除分两个子步骤：1.把淘汰的键值对从哈希表中删除，2.释放这些键值对占用的内存空间，如果两个步骤一起就是同步，分开就是异步 // 通过DictGenericDelete来实现这两个子步骤，是通过分别调用dictfreekey、dictfreevalue和dictfree这三个函数来释放key、value和建制队对应哈希项这三个占用的内存空间的 if (server.lazyfree_lazy_eviction) // 如果开启了惰性删除 dbAsyncDelete(db,keyobj); // 调用dbAsyncDelete，异步删除 // 1.调用DictDelete在过期key的哈希表中同步删除被淘汰的键值对，2.调用DictUnlink在全局哈希表中异步删除键值对 // 3.调用lazyfreegetFreeEffort计算释放内存所需开销，如果不是集合类型或者元素个数小于lazyfreethreshold（默认64）就同步删除，否则就交给后台线程异步删除 else dbSyncDelete(db,keyobj); // 调用dbSyncDelete，同步删除 // 调用两次dictdelay在过期key的哈希表中删除键值对和在全局哈希表中删除键值对 latencyEndMonitor(eviction_latency); latencyAddSampleIfNeeded(\u0026#34;eviction-del\u0026#34;,eviction_latency); latencyRemoveNestedEvent(latency,eviction_latency); delta -= (long long) zmalloc_used_memory(); // 计算出删除键值对释放的内存量 mem_freed += delta; // 和前面已经释放的内存量相加 server.stat_evictedkeys++; notifyKeyspaceEvent(NOTIFY_EVICTED, \u0026#34;evicted\u0026#34;, keyobj, db-\u0026gt;id); decrRefCount(keyobj); keys_freed++; /* 如果存在从节点，在淘汰大量键时主动刷新输出缓冲区以避免延迟 */ if (slaves) flushSlavesOutputBuffers(); /* 对于异步删除模式，定期重新评估是否还需要继续淘汰 */ if (server.lazyfree_lazy_eviction \u0026amp;\u0026amp; !(keys_freed % 16)) { if (getMaxmemoryState(NULL,NULL,NULL,NULL) == C_OK) { mem_freed = mem_tofree; } } } /* 如果本轮未找到任何可以淘汰的键，则跳出循环 */ if (!keys_freed) { latencyEndMonitor(latency); latencyAddSampleIfNeeded(\u0026#34;eviction-cycle\u0026#34;,latency); goto cant_free; /* 没有更多键可供释放 */ } } latencyEndMonitor(latency); latencyAddSampleIfNeeded(\u0026#34;eviction-cycle\u0026#34;,latency); return C_OK; cant_free: /* 如果常规方式无法释放足够内存，则等待后台懒惰释放线程完成任务 */ while(bioPendingJobsOfType(BIO_LAZY_FREE)) { if (((mem_reported - zmalloc_used_memory()) + mem_freed) \u0026gt;= mem_tofree) break; usleep(1000); } return C_ERR; } 可靠性保证模块 RDB快照 创建RDB文件函数有三个：\nrdbSave函数：在本地磁盘创建RDB⽂件，对应Redis的save命令，最终调用rdbSaveRio函数来创建RDB⽂件 rdbSaveBackground函数：后台⼦进程⽅式，在本地磁盘创建RDB⽂件，对应了Redis的bgsave命令，会用fork创建子进程，让⼦进程调⽤rdbSave函数来继续创建RDB⽂件 rdbSaveToSlavesSockets函数：采⽤不落盘⽅式传输RDB⽂件进⾏主从复制时创建RDB⽂件，对应redis server执⾏主从复制命令，以及周期性检测主从复制状态时触发RDB⽣成，也是fork创建子进程，让子进程创建，但是通过网络以字节流的形式发送给从节点 创建RDB文件的函数调用关系： 一个RDB文件的组成：\n文件头：Redis的魔数、RDB版本、Redis版本、RDB⽂件创建时间、键值对占⽤的内存⼤⼩等信息 文件数据部分：实际的所有键值对 文件尾：RDB⽂件的结束标识符，以及整个⽂件的校验值，校验值⽤来在Redis server加载RDB文件后检查文件是否被篡改过。 // 最后都是通过这个函数来创建RDB文件 int rdbSaveRio(rio *rdb, int *error, int rdbflags, rdbSaveInfo *rsi) { dictIterator *di = NULL; dictEntry *de; char magic[10]; int j; uint64_t cksum; size_t processed = 0; if (server.rdb_checksum) rdb-\u0026gt;update_cksum = rioGenericUpdateChecksum; snprintf(magic,sizeof(magic),\u0026#34;REDIS%04d\u0026#34;,RDB_VERSION); // 生成魔数，由“REDIS”和版本的宏定义RDB_VERSION组成 if (rdbWriteRaw(rdb,magic,9) == -1) goto werr; // 写入魔数 if (rdbSaveInfoAuxFields(rdb,rdbflags,rsi) == -1) goto werr; // 写入redis server相关的属性信息，包括运行平台的架构信息、RDB文件创建时间、已使用内存量等 // rdbSaveInfoAuxFields在保存属性信息时会根据属性是字符串还是整数调用rdbSaveAuxFieldStrStr和rdbSaveAuxFieldStrInt // 它们最后都会调用rdbSaveAuxField，它会先写入一个操作码，来表明后面的内容是属性信息 // 然后调用rdbSaveRawString来写入属性的键，它会先记录字符串的长度，再记录实际的字符串，如果是整数的话会调用rdbTryIntegerEncoding使用紧凑结构 // 然后也是调用rdbSaveRawString来写入属性的值，和键类似 if (rdbSaveModulesAux(rdb, REDISMODULE_AUX_BEFORE_RDB) == -1) goto werr;// // 遍历数据库，将其中的键值对写⼊RDB⽂件 for (j = 0; j \u0026lt; server.dbnum; j++) { redisDb *db = server.db+j; dict *d = db-\u0026gt;dict; if (dictSize(d) == 0) continue; di = dictGetSafeIterator(d); /* 先将SELECTDB操作码和对应的数据库编号写⼊RDB⽂件 */ if (rdbSaveType(rdb,RDB_OPCODE_SELECTDB) == -1) goto werr; if (rdbSaveLen(rdb,j) == -1) goto werr; /* 写⼊RESIZEDB操作码，⽤来标识全局哈希表和过期key哈希表中键值对数量的记录 */ uint64_t db_size, expires_size; db_size = dictSize(db-\u0026gt;dict); // 获取全局哈希表⼤⼩ expires_size = dictSize(db-\u0026gt;expires); // 获取过期key哈希表⼤⼩ if (rdbSaveType(rdb,RDB_OPCODE_RESIZEDB) == -1) goto werr; // 写⼊RESIZEDB操作码 if (rdbSaveLen(rdb,db_size) == -1) goto werr; // 写⼊全局哈希表⼤⼩ if (rdbSaveLen(rdb,expires_size) == -1) goto werr; // 写⼊过期key哈希表⼤⼩ /* Iterate this DB writing every entry */ while((de = dictNext(di)) != NULL) { // 读取数据库中的每⼀个键值对 sds keystr = dictGetKey(de); // 获取键值对的key robj key, *o = dictGetVal(de); long long expire; initStaticStringObject(key,keystr); // 获取键值对的value expire = getExpire(db,\u0026amp;key); // 获取键值对中的过期时间 if (rdbSaveKeyValuePair(rdb,\u0026amp;key,o,expire) == -1) goto werr; // 把key和value写⼊RDB⽂件 // rdbSaveKeyValuePair会先将键值对的过期时间、LRU空闲时间或是LFU访问频率写⼊RDB⽂件。且都会先调⽤rdbSaveType函数，写⼊标识这些信息的操作码 // 然后rdbSaveKeyValuePair就能写入实际的键值对了，它会先调用rdbSaveObjectType写⼊键值对的类型标识（宏定义）， // 然后调用rdbSaveStringObject写入key，调用rdbSaveObject写入value。 /* When this RDB is produced as part of an AOF rewrite, move * accumulated diff from parent to child while rewriting in * order to have a smaller final write. */ if (rdbflags \u0026amp; RDBFLAGS_AOF_PREAMBLE \u0026amp;\u0026amp; rdb-\u0026gt;processed_bytes \u0026gt; processed+AOF_READ_DIFF_INTERVAL_BYTES) { processed = rdb-\u0026gt;processed_bytes; aofReadDiffFromParent(); } } dictReleaseIterator(di); di = NULL; /* So that we don\u0026#39;t release it again on error. */ } /* If we are storing the replication information on disk, persist * the script cache as well: on successful PSYNC after a restart, we need * to be able to process any EVALSHA inside the replication backlog the * master will send us. */ if (rsi \u0026amp;\u0026amp; dictSize(server.lua_scripts)) { di = dictGetIterator(server.lua_scripts); while((de = dictNext(di)) != NULL) { robj *body = dictGetVal(de); if (rdbSaveAuxField(rdb,\u0026#34;lua\u0026#34;,3,body-\u0026gt;ptr,sdslen(body-\u0026gt;ptr)) == -1) goto werr; } dictReleaseIterator(di); di = NULL; /* So that we don\u0026#39;t release it again on error. */ } if (rdbSaveModulesAux(rdb, REDISMODULE_AUX_AFTER_RDB) == -1) goto werr; /* 写⼊结束操作码 */ if (rdbSaveType(rdb,RDB_OPCODE_EOF) == -1) goto werr; /* CRC64 checksum. It will be zero if checksum computation is disabled, the * loading code skips the check in this case. */ cksum = rdb-\u0026gt;cksum; // 写⼊校验值 memrev64ifbe(\u0026amp;cksum); if (rioWrite(rdb,\u0026amp;cksum,8) == 0) goto werr; return C_OK; werr: if (error) *error = errno; if (di) dictReleaseIterator(di); return C_ERR; } AOF重写 RDB快照是保存某一时刻的内存数据，AOF是记录所有的写操作，当写操作很多时，AOF日志就会越来越大，所以就需要对AOF进行重写，也就是记录当前数据库中每个键值对的最新内容，而不是记录历史操作。\n实现AOF重写的函数是rewriteAppendOnlyFileBackground rewriteAppendOnlyFileBackground被调用的地方： bgrewriteaofCommand函数：对应Redis server的bgrewriteaof命令，即手动触发，它会判断当前是否有AOF重写的子进程在执行和当前是否有创建RDB的子进程在执行，都没有才会调用rewriteAppendOnlyFileBackground来执行AOF重写，如果有RDB子进程就会将AOF重写设置成待调度执行 startAppendOnly函数：被configSetCommand和sendSynchronousCommand函数调用，configSetCommand对应执行config命令来启用AOF功能，一旦启用就会执行startAppendOnly函数，而sendSynchronousCommand会在主从节点的复制过程中被调用，如果有RDB子进程在执行，就会将AOF重写设置成待调度执行，如果有AOF重写子进程，则将这个子进程Q掉， serverCron函数：它是周期性执行的，会检查当前是否有AOF重写子进程和RDB子进程，以及是否有AOF重写被设置成了待调度执行，如果都满足就会调用rewriteAppendOnlyFileBackground。此外，就算没有AOF重写被设置成待调度执行，它也会进行判断是否需要进行AOF重写，判断条件为AOF功能是否开启、AOF文件大小比例是否超出了阈值、AOF文件大小绝对值是否超出了阈值，当都满足的时候就会进行AOF重写。 所以可以设置两个阈值来自动执行AOF重写，一个阈值是auto-aof-rewrite-percentage，它表示aof文件大小超出基础大小的比例，默认是100，即超出一倍大小，一个阈值是auto-aof-rewrite-min-size，表示AOF文件大小绝对值的最小值 int rewriteAppendOnlyFileBackground(void) { pid_t childpid; long long start; // 检查是否有AOF子进程或者RDB子进程 if (server.aof_child_pid != -1 || server.rdb_child_pid != -1) return C_ERR; if (aofCreatePipes() != C_OK) return C_ERR; // 创建管道用于父子进程之间通信 // 管道其实是在内核中创建一块缓冲区，只能从头部读，从尾部写，类似队列，不能两方同时写，如果需要就要创建两个管道 // 创建管道时需要传入一个pipefd数组，pipefd[0]对应读描述符，pipefd[1]对应写描述符 // aofCreatePipes会创建一个包含6个文件描述符的数组，用于创建三个管道 // 然后会调用anetNonBlock，将前两个描述符对应的管道设置为非阻塞 // 然后调用aeCreateFileEvent在fd[2]上注册读事件的监听，回调函数为aofChildPipeReadable // 最后会把6个描述符赋值给server的成员变量 // fd[0]和fd[1]对应了主进程和重写子进程用于传递操作命令的管道，分别对应读和写描述符，AOF重写过程中，主进程还会继续接受和处理客户端写请求，会被写入AOF日志，还会判断当前是否有AOF重写子进程在执行，如果有就会调用aofRewriteBufferAppend将命令操作记录到aof_rewrite_buf_blocks列表中，它会检查fd[1]赋值给server的aof_pipe_write_data_to_child管道描述符上是否注册了写事件，如果没有就会注册写事件来监听这个管道描述符，当写入数据时，写事件对应的回调函数aofChildWriteDiffData就会被调用执行，它会从aof_rewrite_buf_blocks列表中取出数据块，通过管道发送给重写子进程。重写子进程通过aofReadDiffFromParent来读取命令。 // fd[2]和fd[3]对应了重写子进程向父进程发送ACK消息的管道 // fd[4]和fd[5]对应了父进程向重写子进程发送ACK消息的管道 openChildInfoPipe(); start = ustime(); if ((childpid = fork()) == 0) { // fork一个子进程来进程AOF重写 char tmpfile[256]; /* Child */ closeListeningSockets(0); redisSetProcTitle(\u0026#34;redis-aof-rewrite\u0026#34;); snprintf(tmpfile,256,\u0026#34;temp-rewriteaof-bg-%d.aof\u0026#34;, (int) getpid()); if (rewriteAppendOnlyFile(tmpfile) == C_OK) { // 调用rewriteAppendOnlyFile，里面会掉用rewriteAppendOnlyFileRio，主要是遍历每一个数据库，对于其中每一个键值对，会先记录键值对对应的插入命令，再记录键值对本身 size_t private_dirty = zmalloc_get_private_dirty(-1); if (private_dirty) { serverLog(LL_NOTICE, \u0026#34;AOF rewrite: %zu MB of memory used by copy-on-write\u0026#34;, private_dirty/(1024*1024)); } server.child_info_data.cow_size = private_dirty; sendChildInfo(CHILD_INFO_TYPE_AOF); exitFromChild(0); } else { exitFromChild(1); } } else { /* 父进程， AOF重写过程中，写操作也要尽可能的写入AOF重写日志中，采用管道实现父进程与子进程之间的通信 */ // 记录AOF重写的开始时间 server.stat_fork_time = ustime()-start; server.stat_fork_rate = (double) zmalloc_used_memory() * 1000000 / server.stat_fork_time / (1024*1024*1024); /* GB per second. */ latencyAddSampleIfNeeded(\u0026#34;fork\u0026#34;,server.stat_fork_time/1000); if (childpid == -1) { closeChildInfoPipe(); serverLog(LL_WARNING, \u0026#34;Can\u0026#39;t rewrite append only file in background: fork: %s\u0026#34;, strerror(errno)); aofClosePipes(); return C_ERR; } serverLog(LL_NOTICE, \u0026#34;Background append only file rewriting started by pid %d\u0026#34;,childpid); server.aof_rewrite_scheduled = 0; // 将aof_rewrite_scheduled置为0 server.aof_rewrite_time_start = time(NULL); server.aof_child_pid = childpid; // 记录子进程的进程号 updateDictResizePolicy(); // 禁止AOF重写过程中进行rehash操作，因为rehash会带来比较多的数据移动 /* We set appendseldb to -1 in order to force the next call to the * feedAppendOnlyFile() to issue a SELECT command, so the differences * accumulated by the parent into server.aof_rewrite_buf will start * with a SELECT statement and it will be safe to merge. */ server.aof_selected_db = -1; replicationScriptCacheFlush(); return C_OK; } return C_OK; /* unreached */ } 主从复制 Redis的主从复制分三种情况：\n全量复制，传输RDB文件 增量复制，传输主从断连期间的命令 长连接同步，主节点正常收到的请求传输给从节点 主从复制的四个阶段：\n初始化阶段：当我们把一个Redis实例A设置为另一个实例B的从库时，实例A会完成初始化操作，主要是获得了主库的IP和端口号。可以用三种方式来设置： 在实例A上执行replicaof masterip masterport的主从复制命令，指明实例B的IP（masterip）和端口号（masterport） 在实例A的配置文件中设置replicaof masterip masterport，实例A可以通过解析文件获得主库IP和端口号 在实例A启动时，设置启动参数–replicaof [masterip] [masterport]。实例A解析启动参数，就能获得主库的IP和端口号 建立连接阶段：一旦实例A获得了主库IP和端口号，该实例就会尝试和主库建立TCP网络连接，并且会在建立好的网络连接上，监听是否有主库发送的命令。 主从握手阶段：主从库间相互发送PING-PONG消息，同时从库根据配置信息向主库进行验证。最后，从库把自己的IP、端口号，以及对无盘复制和PSYNC 2协议的支持情况发给主库。 复制类型判断与执行阶段：握手完成后，从库就会给主库发送PSYNC命令。主库会根据命令参数作出回复：执行全量复制、执行增量复制、发生错误。最后，从库收到回复后执行具体复制操作。 主从复制的实现是基于状态机的，每一个Redis实例都有一个redisServer结构体：\nstruct redisServer { ... /* 复制相关(slave) */ char *masterauth; /* 用于和主库进行验证的密码*/ char *masterhost; /* 主库主机名 */ int masterport; /* 主库端口号r */ … client *master; /* 从库上用来和主库连接的客户端 */ client *cached_master; /* 从库上缓存的主库信息 */ int repl_state; /* 从库的复制状态机 */ ... } 初始化阶段 在server.c中的initServerConfig中，会把repl_state设置为REPL_STATE_NONE，当执行replicaof masterip masterport命令时，就会调用replicaofCommand。 // replicaofCommand /* 检查是否已记录主库信息，如果已经记录了，那么直接返回连接已建立的消息 */ if (server.masterhost \u0026amp;\u0026amp; !strcasecmp(server.masterhost,c-\u0026gt;argv[1]-\u0026gt;ptr) \u0026amp;\u0026amp; server.masterport == port) { serverLog(LL_NOTICE,\u0026#34;REPLICAOF would result into synchronization with the master we are already connected with. No operation performed.\u0026#34;); addReplySds(c,sdsnew(\u0026#34;+OK Already connected to specified master\\r\\n\u0026#34;)); return; } /* 如果没有记录主库的IP和端口号，设置主库的信息 */ replicationSetMaster(c-\u0026gt;argv[1]-\u0026gt;ptr, port); // replicationSetMaster会记录主库ip和端口，并且把状态机设置为REPL_STATE_CONNECT 建立连接阶段 在redis中有一个replicationCron周期任务，每1秒执行一次，它会检查从库的复制状态机状态，如果是REPL_STATE_CONNECT，就会调用connectWithMaster与主库建立连接。connectWithMaster会把状态机设置为REPL_STATE_CONNECTING。 replicationCron() { … /* 如果从库实例的状态是REPL_STATE_CONNECT，那么从库通过connectWithMaster和主库建立连接 */ if (server.repl_state == REPL_STATE_CONNECT) { serverLog(LL_NOTICE,\u0026#34;Connecting to MASTER %s:%d\u0026#34;, server.masterhost, server.masterport); if (connectWithMaster() == C_OK) { serverLog(LL_NOTICE,\u0026#34;MASTER \u0026lt;-\u0026gt; REPLICA sync started\u0026#34;); } } … } int connectWithMaster(void) { int fd; //从库和主库建立连接 fd = anetTcpNonBlockBestEffortBindConnect(NULL, server.masterhost,server.masterport,NET_FIRST_BIND_ADDR); … //在建立的连接上注册读写事件，对应的回调函数是syncWithMaster if(aeCreateFileEvent(server.el,fd,AE_READABLE|AE_WRITABLE,syncWithMaster, NULL) ==AE_ERR) { close(fd); serverLog(LL_WARNING,\u0026#34;Can\u0026#39;t create readable event for SYNC\u0026#34;); return C_ERR; } //完成连接后，将状态机设置为REPL_STATE_CONNECTING … server.repl_state = REPL_STATE_CONNECTING; return C_OK; } 主从握手阶段 握手通信的目的，主要包括从库和主库进行验证，以及从库将自身的IP和端口号发给主库。在第二阶段建立连接后，从库的回调函数syncWithMaster会被执行，它会发送发送PING消息给主库，并且将状态机设置为REPL_STATE_RECEIVE_PONG。当从库收到主库返回的PONG消息后，从库会依次给主库发送验证信息、端口号、IP、对RDB文件和无盘复制的支持情况，期间每一次发送消息都对应一组状态变迁。\n复制类型判断与执行阶段 握手完成后，从库状态机为REPL_STATE_RECEIVE_CAPA，紧接着变更为REPL_STATE_SEND_PSYNC，表明要开始向主库发送PSYNC命令，开始实际的数据同步。它会调用slaveTryPartialResynchronization向主库发送PSYNC命令，并且将状态机设置为REPL_STATE_RECEIVE_PSYNC。slaveTryPartialResynchronization负责向主库发送数据同步的命令。主库收到命令后，会根据从库发送的主库ID、复制进度值offset，来判断是进行全量复制还是增量复制，或者是返回错误。\n/* 从库状态机进入REPL_STATE_RECEIVE_CAPA. */ if (server.repl_state == REPL_STATE_RECEIVE_CAPA) { … //读取主库返回的CAPA消息响应 server.repl_state = REPL_STATE_SEND_PSYNC; } //从库状态机变迁为REPL_STATE_SEND_PSYNC后，开始调用slaveTryPartialResynchronization函数向主库发送PSYNC命令，进行数据同步 if (server.repl_state == REPL_STATE_SEND_PSYNC) { if (slaveTryPartialResynchronization(fd,0) == PSYNC_WRITE_ERROR) { … } server.repl_state = REPL_STATE_RECEIVE_PSYNC; return; } int slaveTryPartialResynchronization(int fd, int read_reply) { … //发送PSYNC命令 if (!read_reply) { //从库第一次和主库同步时，设置offset为-1 server.master_initial_offset = -1; … //调用sendSynchronousCommand发送PSYNC命令 reply = sendSynchronousCommand(SYNC_CMD_WRITE,fd,\u0026#34;PSYNC\u0026#34;,psync_replid,psync_offset,NULL); … //发送命令后，等待主库响应 return PSYNC_WAIT_REPLY; } //读取主库的响应 reply = sendSynchronousCommand(SYNC_CMD_READ,fd,NULL); //主库返回FULLRESYNC，全量复制 if (!strncmp(reply,\u0026#34;+FULLRESYNC\u0026#34;,11)) { … return PSYNC_FULLRESYNC; } //主库返回CONTINUE，执行增量复制 if (!strncmp(reply,\u0026#34;+ CONTINUE\u0026#34;,11)) { … return PSYNC_CONTINUE; } //主库返回错误信息 if (strncmp(reply,\u0026#34;-ERR\u0026#34;,4)) { … } return PSYNC_NOT_SUPPORTED; } void syncWithMaster(aeEventLoop *el, int fd, void *privdata, int mask) { ...... //读取PSYNC命令的返回结果 psync_result = slaveTryPartialResynchronization(fd,1); //PSYNC结果还没有返回，先从syncWithMaster函数返回处理其他操作 if (psync_result == PSYNC_WAIT_REPLY) return; //如果PSYNC结果是PSYNC_CONTINUE，从syncWithMaster函数返回，后续执行增量复制 if (psync_result == PSYNC_CONTINUE) { … return; } //如果执行全量复制的话，针对连接上的读事件，创建readSyncBulkPayload回调函数 if (aeCreateFileEvent(server.el,fd, AE_READABLE,readSyncBulkPayload,NULL) == AE_ERR) { … } //将从库状态机置为REPL_STATE_TRANSFER server.repl_state = REPL_STATE_TRANSFER; ... } 哨兵机制 哨兵实例属于一个特殊模式的Redis Server，在main函数中会判断当前实例是否为哨兵实例。\n哨兵机制的初始化：在initServerConfig之前，会调用checkForSentinelMode来判断当前实例是否为哨兵实例，checkForSentinelMode通过下面两个条件来判断。 // server.sentinel_mode = checkForSentinelMode(argc,argv); int checkForSentinelMode(int argc, char **argv) { int j; // 第⼀个判断条件，判断执⾏命令本⾝是否为redis-sentinel if (strstr(argv[0],\u0026#34;redis-sentinel\u0026#34;) != NULL) return 1; for (j = 1; j \u0026lt; argc; j++) // 第⼆个判断条件，判断命令参数是否有\u0026#34;--sentienl\u0026#34; if (!strcmp(argv[j],\u0026#34;--sentinel\u0026#34;)) return 1; return 0; } 这两个判断条件对应哨兵实例的两种启动方式：redis-sentinel sentinel.conf⽂件路径和redis-server sentinel.conf⽂件路径 —sentinel\n初始化配置项：判断为哨兵实例后，会调用initSentinelConfig和initSentinel完成哨兵实例专门的配置项初始化。 if (server.sentinel_mode) { initSentinelConfig(); //将当前server的端⼝号转为哨兵实例专⽤的端⼝号REDIS_SENTINEL_PORT（默认26379），且将protected_mode设置为0，允许外部连接哨兵实例 initSentinel(); // 1.会替换server能执行的命令表，哨兵实例能执行的命令保存在sentinelcmds数组中，2.初始化哨兵实例⽤到的各种属性信息（在sentinelState结构体中） // 哨兵实例的命令：ping、sentinel、subscribe、unsubscribe、psubscribe、punsubscribe、publish、info、role、client、shutdown、auth等 } struct sentinelState { char myid[CONFIG_RUN_ID_SIZE+1]; //哨兵实例ID uint64_t current_epoch; //当前纪元 dict *masters; //监听的主节点的哈希表 int tilt; //是否处于TILT模式 int running_scripts; //运行的脚本个数 mstime_t tilt_start_time; //tilt模式的起始时间 mstime_t previous_time; //上一次执行时间处理函数的时间 list *scripts_queue; //用于保存脚本的队列 char *announce_ip; //向其他哨兵实例发送的IP信息 int announce_port; //向其他哨兵实例发送的端口号 ... } sentinel; 启动哨兵实例：在initServer后会调用sentinelIsRunning启动哨兵实例，它会确认哨兵实例的配置⽂件存在并且可以正常写⼊，检查哨兵实例是否设置了ID，如果没有会随机生成一个ID，还有调用会调⽤sentinelGenerateInitialMonitorEvent给每个监听的主节点发送事件信息，主节点信息保存在sentinelState的masters中，由sentinelRedisInstance结构体保存，地址信息是sentienlAddr结构体，包含ip和端口号。在函数中对调用sentinelIsRunning向被监听的主节点发送事件信息，从⽽开始监听主节点。它们采用Pub/Sub订阅频道模式的通信方式。 // sentinelRedisInstance除了表示主节点，也可以表示从节点和其他哨兵实例 typedef struct sentinelRedisInstance { int flags; //实例类型、状态的标记，SRI_MASTER、SRI_SLAVE或SRI_SENTINEL分别表示主节点、从节点或其他哨兵，状态标记位会标识主观下线、客观下线等 char *name; //实例名称 char *runid; //实例ID uint64_t config_epoch; //配置的纪元 sentinelAddr *addr; //实例地址信息 ... mstime_t s_down_since_time; //主观下线的时长 mstime_t o_down_since_time; //客观下线的时长 ... dict *sentinels; //监听同一个主节点的其他哨兵实例 dict *slaves; //主节点的从节点 ... } void sentinelGenerateInitialMonitorEvents(void) { dictIterator *di; dictEntry *de; di = dictGetIterator(sentinel.masters); //获取masters的迭代器 while((de = dictNext(di)) != NULL) { //获取被监听的主节点 sentinelRedisInstance *ri = dictGetVal(de); sentinelEvent(LL_WARNING, \u0026#34;+monitor\u0026#34;, ri, \u0026#34;%@ quorum %d\u0026#34;, ri-\u0026gt;quorum); } dictReleaseIterator(di); //发送+monitor事件 } void sentinelEvent(int level, char *type, sentinelRedisInstance *ri, const char *fmt, ...) { va_list ap; char msg[LOG_MAX_LEN]; robj *channel, *payload; /* 如果传递消息以\u0026#34;%\u0026#34;和\u0026#34;@\u0026#34;开头，就判断实例是否为主节点 */ if (fmt[0] == \u0026#39;%\u0026#39; \u0026amp;\u0026amp; fmt[1] == \u0026#39;@\u0026#39;) { // 判断实例的flags标签是否为SRI_MASTER，如果是，就表明实例是主节点 sentinelRedisInstance *master = (ri-\u0026gt;flags \u0026amp; SRI_MASTER) ? NULL : ri-\u0026gt;master; // 如果当前实例是主节点，根据实例的名称、IP地址、端⼝号等信息调⽤snprintf⽣成传递的消息msg if (master) { snprintf(msg, sizeof(msg), \u0026#34;%s %s %s %d @ %s %s %d\u0026#34;, sentinelRedisInstanceTypeStr(ri), ri-\u0026gt;name, ri-\u0026gt;addr-\u0026gt;ip, ri-\u0026gt;addr-\u0026gt;port, master-\u0026gt;name, master-\u0026gt;addr-\u0026gt;ip, master-\u0026gt;addr-\u0026gt;port); } else { snprintf(msg, sizeof(msg), \u0026#34;%s %s %s %d\u0026#34;, sentinelRedisInstanceTypeStr(ri), ri-\u0026gt;name, ri-\u0026gt;addr-\u0026gt;ip, ri-\u0026gt;addr-\u0026gt;port); } fmt += 2; } else { msg[0] = \u0026#39;\\0\u0026#39;; } /* Use vsprintf for the rest of the formatting if any. */ if (fmt[0] != \u0026#39;\\0\u0026#39;) { va_start(ap, fmt); vsnprintf(msg+strlen(msg), sizeof(msg)-strlen(msg), fmt, ap); va_end(ap); } /* Log the message if the log level allows it to be logged. */ if (level \u0026gt;= server.verbosity) serverLog(level,\u0026#34;%s %s\u0026#34;,type,msg); /* Publish the message via Pub/Sub if it\u0026#39;s not a debugging one. */ if (level != LL_DEBUG) { channel = createStringObject(type,strlen(type)); payload = createStringObject(msg,strlen(msg)); pubsubPublishMessage(channel,payload); // 将消息发送到对应的频道中 decrRefCount(channel); decrRefCount(payload); } /* Call the notification script if applicable. */ if (level == LL_WARNING \u0026amp;\u0026amp; ri != NULL) { sentinelRedisInstance *master = (ri-\u0026gt;flags \u0026amp; SRI_MASTER) ? ri : ri-\u0026gt;master; if (master \u0026amp;\u0026amp; master-\u0026gt;notification_script) { sentinelScheduleScriptExecution(master-\u0026gt;notification_script, type,msg,NULL); } } } 哨兵Leader选举 当哨兵发现主节点有故障时，它们就会选举⼀个Leader出来，由这个Leader负责执⾏具体的故障切换流程。哨兵机制采用Raft协议选举leader，但是实现时不是完全按照Raft协议，因为不同实例之间不是主从节点的关系，而是对等的。\n发起投票的哨兵想要成为leader需要满足条件：\n获得超过半数的其他哨兵的赞成票 获得超过预设的quorum阈值的赞成票数 如果当前节点是哨兵实例，在serverCron中会调用哨兵的时间事件处理函数sentinelTimer，sentinelTimer会调用sentinelHandleDictOfRedisInstances，参数为当前哨兵实例状态信息sentinelState结构中维护的master哈希表，sentinelHandleDictOfRedisInstances会执行一个循环，调用sentinelHandleRedisInstance对每一个master节点进行处理。\nvoid sentinelTimer(void) { sentinelCheckTiltCondition(); // 检查是否需要进⼊TILT模式，当哨兵连续两次的时间事件处理间隔时⻓为负值，或是间隔时⻓过⻓，那么哨兵就会进⼊TILT模式。 // 在该模式下，哨兵只会定期发送命令收集信息，⽽不会执⾏故障切换流程。 sentinelHandleDictOfRedisInstances(sentinel.masters); sentinelRunPendingScripts(); // 运⾏待执⾏的脚本 sentinelCollectTerminatedScripts(); // 收集已终⽌的脚本 sentinelKillTimedoutScripts(); // 终⽌超时的脚本 /* We continuously change the frequency of the Redis \u0026#34;timer interrupt\u0026#34; * in order to desynchronize every Sentinel from every other. * This non-determinism avoids that Sentinels started at the same time * exactly continue to stay synchronized asking to be voted at the * same time again and again (resulting in nobody likely winning the * election because of split brain voting). */ server.hz = CONFIG_DEFAULT_HZ + rand() % CONFIG_DEFAULT_HZ; // server.hz默认值的基础上增加⼀个随机值 } void sentinelHandleDictOfRedisInstances(dict *instances) { dictIterator *di; dictEntry *de; sentinelRedisInstance *switch_to_promoted = NULL; /* There are a number of things we need to perform against every master. */ di = dictGetIterator(instances); // 获取哈希表的迭代器 while((de = dictNext(di)) != NULL) { sentinelRedisInstance *ri = dictGetVal(de); // 从哈希表中取出一个实例 sentinelHandleRedisInstance(ri); // 调用sentinelHandleRedisInstance处理实例， 检查主节点的在线状态，并在主节点客观下线时进行故障切换 if (ri-\u0026gt;flags \u0026amp; SRI_MASTER) { sentinelHandleDictOfRedisInstances(ri-\u0026gt;slaves); sentinelHandleDictOfRedisInstances(ri-\u0026gt;sentinels); if (ri-\u0026gt;failover_state == SENTINEL_FAILOVER_STATE_UPDATE_CONFIG) { switch_to_promoted = ri; } } } if (switch_to_promoted) sentinelFailoverSwitchToPromotedSlave(switch_to_promoted); dictReleaseIterator(di); } void sentinelHandleRedisInstance(sentinelRedisInstance *ri) { /* ========== MONITORING HALF ============ */ /* Every kind of instance */ sentinelReconnectInstance(ri); // 尝试和断连的实例重新建⽴连接 // 在sentinelRedisInstance结构体中有一个instanceLink类型的变量link，其中记录了⽤来向主节点发送命令的连接cc和⽤来发送Pub/Sub消息的连接pc // sentinelReconnectInstance会检查这两个连接是否为NULL，如果为NULL则调用redisAsyncConnectBind重新建⽴连接 sentinelSendPeriodicCommands(ri); // 向实例发送PING、INFO等命令 // 先调用redisAsyncCommand，通过连接cc，向主节点发送INFO命令 // 再通过sentinelSendPing向主节点发送PING命令 // 再调用sentinelSendHello通过连接cc，向主节点发送PUBLISH命令，将哨兵⾃⾝的IP、端⼝号和ID号信息发送给主节点 /* ============== ACTING HALF ============= */ /* We don\u0026#39;t proceed with the acting half if we are in TILT mode. * TILT happens when we find something odd with the time, like a * sudden change in the clock. */ if (sentinel.tilt) { if (mstime()-sentinel.tilt_start_time \u0026lt; SENTINEL_TILT_PERIOD) return; sentinel.tilt = 0; sentinelEvent(LL_WARNING,\u0026#34;-tilt\u0026#34;,NULL,\u0026#34;#tilt mode exited\u0026#34;); } /* Every kind of instance */ sentinelCheckSubjectivelyDown(ri); // 检查监听的实例是否主观下线 // ⾸先会计算当前距离上次哨兵发送PING命令的时⻓elapsed // 分别检测哨兵和主节点的命令发送连接，以及Pub/Sub连接的活跃程度。如果活跃度不够，那么哨兵会调⽤instanceLinkCloseConnection，断开当前连接，以便重新连接。 // 判断主节点是否为主观下线的两个条件：1、当前距离上次发送PING的时⻓已经超过down_after_period阈值，还没有收到回复，阈值由down-after-milliseconds决定，默认30s // 2、哨兵认为当前实例是主节点，但是这个节点向哨兵报告它将成为从节点，并且在down_after_period时⻓，再加上两个INFO命令间隔后，该节点还是没有转换成功。 // 满足其一则判断为主管下线，就会掉用sentinelEvent发送“sdown”事件信息 /* Masters and slaves */ if (ri-\u0026gt;flags \u0026amp; (SRI_MASTER|SRI_SLAVE)) { /* Nothing so far. */ } /* 只有主节点才执行 */ if (ri-\u0026gt;flags \u0026amp; SRI_MASTER) { sentinelCheckObjectivelyDown(ri); // 检查监听的实例是否客观下线 // 1. 在sentinelRedisInstance结构体中保存有用来监听主节点的其他哨兵实例 dict *sentinels; // 2. sentinelCheckObjectivelyDown会遍历这个sentinels哈希表，获取其他哨兵实例对主节点主观下线的判断结果 // 3. sentinelRedisInstance结构体中的flags会记录哨兵对主观下线的判断结果（采用位掩码的方式，不同位有不同的含义） // 4. 方法中会使用quorum变量来记录判断主节点为主观下线的哨兵数量，如果设置了SRI_MASTER_DOWN就+1（包括自身实例） // 5. 如果quorum大于阈值，则判断为客观下线，并且设置变量odown为1（表⽰当前哨兵对主节点客观下线的判断结果） // 6. 阈值保存在主节点的数据结构中master-\u0026gt;quorum，在sentinel.conf中配置 // 7. 判断客观下线后会调用sentinelEvent发送+odown事件，并且主节点flags设置为SRI_O_DOWN if (sentinelStartFailoverIfNeeded(ri)) // 判断是否要启动故障切换，判断条件为1.主节点的flags已经标记了SRI_O_DOWN； // 2.当前没有在执⾏故障切换；3.果已经开始故障切换，那么开始时间距离当前时间，是否超过了sentinel failover-timeout（在sentinel.conf中配置）的两倍 // 如果要，就获取其他哨兵实例对主节点状态的判断，并向其他哨兵发送is-master-down-by-addr命令 sentinelAskMasterStateToOtherSentinels(ri,SENTINEL_ASK_FORCED); sentinelFailoverStateMachine(ri); // 执⾏故障切换，调用sentinelAskMasterStateToOtherSentinels进行leader选举 sentinelAskMasterStateToOtherSentinels(ri,SENTINEL_NO_FLAGS); // 获取其他哨兵实例对主节点状态的判断 } } 发布订阅机制 在全局变量server中有一个成员变量pubsub_channels，在initServer中会进行初始化，它是一个keylistDictType类型的哈希表，每一个kv就是一个频道，key为频道的名称，value是一个list，存储订阅这个频道的订阅者\n发布命令对应redis中的publish，实现函数是publishCommand，通过调用pubsubPublishMessage发布消息，并返回接收消息的订阅者数量，pubsubPublishMessage会遍历订阅者列表，并发送消息。 订阅命令对应redis中的subscribe，实现函数是subscribeCommand，通过调用pubsubSubscribeChannel订阅频道。pubsubSubscribeChannel会先把需要订阅的频道加入server.pubsub_channels,如果频道不存在就会新建一个列表并加入这个订阅者，最后返回订阅成功的频道数量。 发布消息：sentinelEvent函数，在三个地方被调用，分别是： 1.哨兵调用sentinelCheckSubjectivelyDown函数检测出主节点主观下线后，向“+sdown”频道发布消息 2.哨兵初始化时，sentinelGenerateInitialMonitorEvents函数“向monitor”频道发布消息 3.哨兵在完成主节点切换后，sentinelFailoverSwitchToPromotedSlave函数向“switch-master”频道发布消息 哨兵工作中会用到的频道： 此外，哨兵会订阅主节点的“sentinel:hello”频道，通过这个频道，监听同⼀主节点的不同哨兵就能通过频道上的hello消息，来交互彼此的访问信息了。\nRedis Cluster模块 Redis Cluster模块是采用分片的方式来实现分布式存储的，需要维护节点之间的信息。有两种方法，一种是中心化的方法，使用一个第三方系统来维护；第二种是去中心化的方法，让每个节点都维护批次的信息、状态，并且使用集群通信协议Gossip在节点间传播更新的信息，让每个节点都拥有一致的信息。\nGossip协议：每个节点按一定频率从集群中随机挑选一些其他节点，把自身的信息和已知的其他节点的信息，用PING消息发送给选出的节点，这个节点收到后，也把自身信息和已知的其他节点的信息，用PONG消息返回给发送者。通过这种随机挑选通信节点的方式，让节点信息在集群中传播，经过几轮后其他节点就收到了这个信息。\n在serverCron中会掉用clusterCron函数，每100毫秒执行一次，每执行10次clusterCron就会随机挑选5个节点，在这5个节点中选出最早向当前节点发送Pong消息的那个节点，并向它发送Ping消息。相当于每一秒选择一个节点发送ping消息。\n// cluster.h 常见的消息类型： #define CLUSTERMSG_TYPE_PING 0 //Ping消息,用来向其他节点发送当前节点信息 #define CLUSTERMSG_TYPE_PONG 1 //Pong消息,对Ping消息的回复 #define CLUSTERMSG_TYPE_MEET 2 //Meet消息,表示某个节点要加入集群 #define CLUSTERMSG_TYPE_FAIL 3 //Fail消息,表示某个节点有故障 // 消息的结构体 typedef struct { ... uint32_t totlen; // 消息长度 uint16_t type; // 消息类型 ... char sender[CLUSTER_NAMELEN]; // 发送消息节点的名称 unsigned char myslots[CLUSTER_SLOTS/8]; // 发送消息节点负责的slots char myip[NET_IP_STR_LEN]; // 发送消息节点的IP uint16_t cport; // 发送消息节点的通信端口 ... union clusterMsgData data; // 消息体 } clusterMsg; union clusterMsgData { // Ping、Pong和Meet消息类型对应的数据结构 struct { clusterMsgDataGossip gossip[1]; } ping; // Fail消息类型对应的数据结构 struct { clusterMsgDataFail about; } fail; // Publish消息类型对应的数据结构 struct { clusterMsgDataPublish msg; } publish; // Update消息类型对应的数据结构 struct { clusterMsgDataUpdate nodecfg; } update; // Module消息类型对应的数据结构 struct { clusterMsgModule msg; } module; }; typedef struct { char nodename[CLUSTER_NAMELEN]; // 节点名称 uint32_t ping_sent; // 节点发送Ping的时间 uint32_t pong_received; // 节点收到Pong的时间 char ip[NET_IP_STR_LEN]; // 节点IP uint16_t port; // 节点和客户端的通信端口 uint16_t cport; // 节点用于集群通信的端口 uint16_t flags; // 节点的标记 uint32_t notused1; // 未用字段 } clusterMsgDataGossip; ","permalink":"https://oyzg.github.io/archives/redis/","summary":"Redis相关内容.","title":"Redis"},{"content":"容器 容器的本质其实就是一个特殊的进程。它通过Linux NameSpace、Linux CGroups、rootfs来实现对进程的隔离和约束。\nDocker的核心步骤： 1、启用 Linux Namespace 配置； 2、设置指定的 Cgroups 参数； 3、切换进程的根目录（Change Root）。\nNameSpace NameSpace是用来修改进程视图的方法，通过NameSpace技术可以让这个进程只能看到自己这个namespace中的空间，使用方式则是通过添加相关参数。\nint pid = clone(main_function, stack_size, SIGCHLD, NULL); # 添加CLONE_NEWPID后容器内看到的pid就是1了，除此外还有Mount、UTS、IPC、Network 和 User等Namespace int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); CGroups 在NameSpace修改视图后，相比于虚拟化技术还是隔离得不够彻底，多个容器间使用的还是同一个操作系统内核，而且还有很多资源和对象不能NameSpace化，比如时间，在容器内修改时间是对宿主机可见的。\nCGroups（Linux Control Group）技术是用来对进程进行资源限制的，限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等，从而防止一个进程把整个系统的资源吃光的情况。此外，还有对进程进行优先级设置、审计等作用。\n它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下，通过修改该路径下的文件即可实现对进程的资源限制。\nrootfs 在对进程进行隔离后，容器内看到的文件系统应该是完全独立的，让其不受宿主机和其他容器的影响。Mount Namespace 修改的是容器进程对文件系统“挂载点”的认知（对挂载点进行了隔离），即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样，只有在容器执行挂载后，才会看到与宿主机不同的文件系统。在Linux中有一个chroot命令可以改变进程的根目录到你指定的位置。\n但是我们一般需要在容器的根目录下挂载完整操作系统的文件系统，挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。\n需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。所有容器都共享宿主机的操作系统内核。\nDocker在制作镜像时，引入了层的概念，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。rootfs由三部分组成，如图所示。只读层包含操作系统的基础文件，可读写层包含你的程序、依赖包等，如果你删除了只读层的文件，就会在可读写层生成一个foo文件，Init层主要包含一些操作系统的配置信息，如/etc/hosts、/etc/resolv.conf，这些文件输入只读层，但是我们需要对这些配置文件进行修改，并且希望这些配置信息在docker commit时不包含在内。\nDocker Docker常用命令 数据拷贝：docker cp [源路径] [目标路径]，对于容器内的文件采用[容器名或容器ID]:[路径]的形式，如docker cp a.txt 062:/tmp 共享目录：通过-v参数挂载，docker run -v [宿主机路径]:[容器内路径] 网络模式 null：容器与宿主机之间不通信 host：容器与宿主机共享网络，docker run时使用\u0026ndash;net=host参数开启 bridge：容器与宿主机之间通过docker0网桥进行通信，使用\u0026ndash;net=bridge开启，一般不需要，因为是默认的 端口号映射需要bridge模式，在docker run 中指定-p参数，-p [宿主机端口]:[容器端口] Dockerfile 如前面rootfs部分所述，docker镜像是分成很多层的，可以用docker inspect [image name]来查看分层信息。\n如果需要自己构建镜像则可以通过docker build命令进行构建，-f参数指定Dockerfile文件，没有就是Dockerfile文件，需要目录下只有这一个Dockerfile文件\nDockerfile中的常用命令有：\nFROM:基础镜像 COPY:用于将本机的源码、配置文件等复制到镜像中，源文件必须在“构建上下文”路径中 RUN:用于执行SHELL命令，一行只能有一条命令，所以末尾用\\，命令之间使用\u0026amp;\u0026amp;相连，为了美观可以写一个sh文件，用COPY拷贝进来再RUN执行 变量:ARG和ENV，ARG变量只在镜像过程中可见，ENV变量在镜像构建和容器运行时均可见 TIP：Dockerfile每个指令都会生成一个镜像层，所以要精简\nKubernetes Kubernetes是一个容器编排工具。编排的意思就是能够按照用户意愿和系统规则，完全自动化的处理好容器之间的关系。调度是把容器按照某种规则，放在最佳节点上运行起来。\nKubunetes分为Master节点和note节点，Master节点由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。node节点最核心的部分是kubectl，用于和容器运行时打交道,此外还有kube-proxy用于管理容器的网络通信，ccontainer-runtime管理Pod的生命周期。整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。\n除此之外，还有kubectl，是Kubernetes的命令行工具，用于和集群进行交互。\nminikube minikube 是一个迷你版的Kubernetes集群，常用命令：\nminikube version： 查看minikube版本 minikube start：启动minikube集群，\u0026ndash;kubernetes-version=指定版本 minikube stop：停止minikube集群 minikube status：查看minikube集群状态 minikube delete：删除minikube集群 minikube node list：查看minikube集群节点 kubectl kubectl是Kubernetes的命令行工具，用于和集群进行交互，常用命令：\nkubectl get pods|nodes|services|deployments|namespaces：查看所有pod｜node｜service｜deployment｜namespace信息 kubectl describe pods|nodes|services|deployments|namespaces：查看指定pod｜node｜service｜deployment｜namespace信息 kubectl create|delete|edit -f [文件名]：创建|删除|编辑 指定资源 kubectl apply -f [文件名]：使用文件创建资源 kubectl logs [pod名]：查看指定pod的日志 kubectl exec [pod名] [命令]：在指定pod中执行命令, kubectl exec [pod名] -it \u0026ndash;/bin/bash进入pod kubectl run [pod名] \u0026ndash;image=[镜像名]：运行一个pod kubectl get pod -n [namespace]：查看指定namespace下的所有pod kubectl api-resources：查看支持的所有API对象 kubectl explain [API对象]：查看API对象， \u0026ndash;dry-run=client -o yaml可以生成yaml样板 API对象 由于Kubenetes的设计思路——“单一职责”和“组合优于继承”，所有对象都尽量只关注自己的职责。 Pod Pod是Kubernetes的最小运行单位，一个Pod中可以运行多个容器，每个容器之间是相互隔离的，但是可以共享同一个IP地址和端口。yaml示例：\napiVersion: v1 kind: Pod metadata: name: busy-pod labels: owner: chrono env: demo region: north tier: back spec: containers: - image: busybox:latest name: busy imagePullPolicy: IfNotPresent env: - name: os value: \u0026#34;ubuntu\u0026#34; - name: debug value: \u0026#34;on\u0026#34; command: - /bin/echo args: - \u0026#34;$(os), $(debug)\u0026#34; resources: # 限制使用资源 requests: # 要申请的资源 cpu: 10m # 1000m = 1CPU时间 memory: 100Mi limits: # 使用资源的上限 cpu: 20m memory: 200Mi 由apiVersion、kind、metadata、spec四个基本组成部分，metadata包含pod的名称、标签、注解等信息，spec包含pod的运行参数，包括容器镜像、环境变量、命令、参数、端口映射、卷挂载、资源限制、调度策略等等。\nKubernetes 为检查应用状态定义了三种探针，它们分别对应容器不同的状态：\nStartup，启动探针，用来检查应用是否已经启动成功，适合那些有大量初始化工作要做，启动很慢的应用。如果Startup探针失败，会尝试反复重启 Liveness，存活探针，用来检查应用是否正常运行，是否存在死锁、死循环。如果容器异常也会重启容器 Readiness，就绪探针，用来检查应用是否已经准备好接收流量。如果失败会从Service负载均衡中移除，不再分配流量 应用程序先启动，加载完配置文件等基本的初始化数据就进入了 Startup 状态，之后如果没有什么异常就是 Liveness 存活状态，但可能有一些准备工作没有完成，还不一定能对外提供服务，只有到最后的 Readiness 状态才是一个容器最健康可用的状态\n要使用探针需要预留“检查口”，如下，在ConfigMap中使用/ready作为检查口。\napiVersion: v1 kind: ConfigMap metadata: name: ngx-conf data: default.conf: | server { listen 80; location = /ready { return 200 \u0026#39;I am ready\u0026#39;; } } 探针的具体定义：\napiVersion: v1 kind: Pod metadata: name: ngx-pod-probe spec: volumes: - name: ngx-conf-vol configMap: name: ngx-conf containers: - image: nginx:alpine name: ngx ports: - containerPort: 80 volumeMounts: - mountPath: /etc/nginx/conf.d name: ngx-conf-vol startupProbe: # 启动探针 periodSeconds: 1 # 执行探测动作的时间间隔 # timeoutSeconds字段 探测动作的超时时间 # successThreshold字段 连续几次探测成功才认为是正常 # failureThreshold字段 连续几次探测失败才认为是异常 # 三种探测方式： exec、TCP Socket、HTTP GET exec: # 执行一个Linux命令 command: [\u0026#34;cat\u0026#34;, \u0026#34;/var/run/nginx.pid\u0026#34;] livenessProbe: # 存活探针 periodSeconds: 10 tcpSocket: # 使用 TCP 协议尝试连接容器的指定端口 port: 80 readinessProbe: # 就绪探针 periodSeconds: 5 httpGet: # 连接端口并发送 HTTP GET 请求 path: /ready port: 80 Job和CronJob 在线业务：长时间运行的，如nginx 离线业务：短时间运行的，如定时任务 Job和CronJob都是用来处理离线业务的，Job是处理临时任务，CronJob是处理定时任务。 Job的yaml示例： apiVersion: batch/v1 kind: Job metadata: name: echo-job spec: activeDeadlineSeconds: 15 backoffLimit: 2 completions: 4 parallelism: 2 template: spec: restartPolicy: OnFailure containers: - image: busybox name: echo-job imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/echo\u0026#34;] args: [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;] template定义了一个应用模版，里面用来嵌入Pod。 几个重要字段：\nactiveDeadlineSeconds，设置 Pod 运行的超时时间。 backoffLimit，设置 Pod 的失败重试次数。 completions，Job 完成需要运行多少个 Pod，默认是 1 个。 parallelism，它与 completions 相关，表示允许并发运行的 Pod 数量，避免过多占用资源 CronJob的yaml示例：\napiVersion: batch/v1 kind: CronJob metadata: name: echo-cj spec: # 这个spec是CronJob的配置 schedule: \u0026#39;*/* * * * *\u0026#39; # 定时，每分钟执行一次 jobTemplate: # 这个下面嵌套Job spec: template: # 这个下面嵌套Pod spec: restartPolicy: OnFailure containers: - image: busybox name: echo-cj imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/echo\u0026#34;] args: [\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;] ConfigMap和Secret ConfigMap和Secret用于保存配置信息，ConfigMap保存的是非敏感信息，如数据库连接信息，Secret保存的是敏感信息，如密码。这些信息都保存在etcd中。\nConfigMap的yaml示例：\napiVersion: v1 kind: ConfigMap metadata: name: info data: # 用来存储数据，kv结构 count: \u0026#39;10\u0026#39; debug: \u0026#39;on\u0026#39; path: \u0026#39;/etc/systemd\u0026#39; greeting: | say hello to kubernetes. Secret的yaml示例：\napiVersion: v1 kind: Secret metadata: name: user data: # 这些都是base64编码的， name: cm9vdA== # root pwd: MTIzNDU2 # 123456 db: bXlzY2Fw # mysql # 自己手动base64编码，-n是删除隐含的换行符 echo -n \u0026#34;123456\u0026#34; | base64 MTIzNDU2 使用方法： 1、环境变量的方式：利用pod.spec.containers.env.valueFrom以环境变量的形式注入Pod\napiVersion: v1 kind: Pod metadata: name: env-pod spec: containers: - env: - name: COUNT valueFrom: configMapKeyRef: #ConfigMap对象 name: info key: count - name: USERNAME valueFrom: secretKeyRef: #Secret对象 name: user key: name 2、Volume的方式：类似docker run -v，在Pod中挂载一个Volume，然后把Volume中的内容映射到容器中。\napiVersion: v1 kind: Pod metadata: name: vol-pod spec: # 定义两个 Volume，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol volumes: - name: cm-vol configMap: name: info - name: sec-vol secret: secretName: user # 然后在容器中挂载 Volume containers: - volumeMounts: - mountPath: /tmp/cm-items name: cm-vol - mountPath: /tmp/sec-items name: sec-vol image: busybox name: busy imagePullPolicy: IfNotPresent command: [\u0026#34;/bin/sleep\u0026#34;, \u0026#34;300\u0026#34;] Deployment Deployment是用来部署应用程序的，用来处理在线业务，让应用永不宕机。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: ngx-dep name: ngx-dep spec: replicas: 2 # 副本数，运行的Pod数量 selector: matchLabels: app: ngx-dep # 匹配标签，和下面的template中的标签一致 # 通过这种labels，解除了Deployment和Pod的强绑定，把组合关系变成了“弱引用” template: metadata: labels: app: ngx-dep spec: containers: - image: nginx:alpine name: nginx 通过scale命令来扩容： kubectl scale \u0026ndash;replicas=5 deploy ngx-dep\nDaemonset 用于在集群的每个节点上运行且仅运行一个Pod，主要用于日志收集、数据采集等场景。\napiVersion: apps/v1 kind: DaemonSet metadata: name: redis-ds labels: app: redis-ds spec: selector: matchLabels: name: redis-ds template: metadata: labels: name: redis-ds spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 污点（taint）和容忍度（toleration\nMaster默认不跑应用，需要配置taint和toleration来配置才能在master上运行pod。 方法一：去掉master节点的taint，末尾的-就是去除 kubectl taint node master node-role.kubernetes.io/master:NoSchedule-\n方法二：在创建节点的时候，添加toleration\ntolerations: - key: node-role.kubernetes.io/master effect: NoSchedule operator: Exists 静态Pod 默认在/etc/kubernetes/manifests下，Kubernetes 的 4 个核心组件 apiserver、etcd、scheduler、controller-manager都以静态 Pod 的形式存在的\nService 用于服务发现和负载均衡，通过iptables实现。\n生成yaml文件命令：kubectl expose deploy ngx-dep \u0026ndash;port=80 \u0026ndash;target-port=80 \u0026ndash;dry-run=client -o yaml\napiVersion: v1 kind: Service metadata: name: ngx-svc spec: selector: app: ngx-dep ports: - port: 80 # 外部端口 targetPort: 80 # 内部端口 protocol: TCP # 协议 type：ClusterIP # 还有“ExternalName”、“LoadBalancer”、\u0026#34;NodePort\u0026#34;，“NodePort”类型会创建一个专用映射端口，外部可访问 namespace\nkubectl get ns查看所有的namespace，默认情况下所有API对象都在defalult namespace下。\n基于 DNS 插件，可以以域名形式访问Service，Service对象的域名形式为：”对象. 名字空间.svc.cluster.local“，或者“对象. 名字空间”、“对象名”\nIngress Ingress是流量的总入口，统管集群的进出口数据，以及负载均衡（七层，Service是四层负载均衡）\nIngress Controller Ingress Controller是Ingress资源的实际执行者，负责监听Ingress资源的变化并根据规则配置负载均衡器。 常用的Ingress Controller实现有Nginx Ingress Controller。\n工作原理：\n用户创建Ingress资源定义路由规则 Ingress Controller持续监听API Server中的Ingress资源变化 当Ingress资源发生变化时，Controller更新其内部的负载均衡配置 外部流量通过Ingress Controller访问集群内的服务 IngreeClass 用于定义 Ingress 控制器的类型和配置，解决了集群中存在多个 Ingress 控制器时的路由冲突问题。\nPersistentVolume 用于管理存储资源，属于系统资源，与Node平级，Pod只拥有其使用权\nPersistentVolumeClaim，简称 PVC，用来向Kubernetes申请存储资源，由Pod使用，代表Pod向系统申请PV，申请成功后会将PV和PVC绑定（bind） StorageClass，在PV和PVC之间，帮助PVC找到合适的PV。 PV的yaml：\napiVersion: v1 kind: PersistentVolume metadata: name: host-10m-pv spec: storageClassName: host-test # 对应StorageClass # 访问模式，ReadWriteOnce（可读可写，只能被一个Pod使用）、ReadOnlyMany（只可读，可以被多个Pod使用）、ReadWriteMany（可读可写，可以被多个Pod使用） accessModes: - ReadWriteOnce capacity: # 存储容量 storage: 10Mi hostPath: # 存储卷的本地路径，在节点上创建的目录 path: /tmp/host-10m-pv/ PVC的yaml：\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: host-5m-pvc spec: storageClassName: host-test accessModes: - ReadWriteOnce resources: requests: storage: 5Mi # 请求的存储容量 在Pod中使用PVC：\napiVersion: v1 kind: Pod metadata: name: host-pvc-pod spec: volumes: - name: host-pvc-vol persistentVolumeClaim: claimName: host-5m-pvc # 绑定PVC containers: - name: ngx-pvc-pod image: nginx:alpine ports: - containerPort: 80 volumeMounts: - name: host-pvc-vol mountPath: /tmp HostPath 是最简单的一种 PV，数据存储在节点本地，速度快但不能跟随 Pod 迁移\n要实现跨节点数据共享，需要使用NFS或者Ceph等网络存储系统\n# NFS PV apiVersion: v1 kind: PersistentVolume metadata: name: nfs-1g-pv spec: storageClassName: nfs accessModes: - ReadWriteMany capacity: storage: 1Gi nfs: path: /tmp/nfs/1g-pv # 需事先创建好 server: 192.168.10.208 以上PV都是静态存储卷，需要手动创建，而Provisioner可以自动管理、创建PV，是动态存储卷。Provisioner以Pod形式运行，部署Provisioner有三个yaml文件，rbac.yaml、class.yaml和deployment.yaml。\n使用Provisioner时就不需要定义PV了，只需要在PVC中指定StorageClass，StorageClass再关联到Provisioner。 StorageClass的yaml：\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-client provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # Provisioner名称 parameters: archiveOnDelete: \u0026#34;false\u0026#34; # \u0026#34;false\u0026#34;为自动回收存储空间 StatefulSet Deployment只能管理无状态的应用，StatefulSet管理有状态的，可以看作Deployment的特例。\nStatefulSet启动的Pod是有顺序的，名字为XXXX-0, XXXX-1等，可以根据编号来决定依赖关系，\nStatefulSet的yaml：\napiVersion: apps/v1 kind: StatefulSet metadata: name: redis-sts spec: serviceName: redis-svc # 和Service中metadata.name一致 volumeClaimTemplates: # PVC，为每个Pod自动创建PVC - metadata: name: redis-100m-pvc spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 100Mi replicas: 2 selector: matchLabels: app: redis-sts template: metadata: labels: app: redis-sts spec: containers: - image: redis:5-alpine name: redis ports: - containerPort: 6379 volumeMounts: - name: redis-100m-pvc mountPath: /data 写 Service 对象的时候要小心一些，metadata.name 必须和StatefulSet 里的 serviceName 相同，selector 里的标签也必须和 StatefulSet 里的一致。StatefulSet创建的Pod的域名和其他Pod不同，格式为“Pod名.服务名.名字空间.svc.cluster.local”，简写”Pod名.服务名“。\nnamespace namespace是Kubernetes中的一个概念，可以将集群切分成多个区域，同时namespace也是一种API对象，简称ns。Kubernetes中默认有四个namespace：default、kube-system、kube-public、kube-node-lease。要将API对象放入指定namespace中可以在metadata里添加一个namespace字段。\nResourceQuota 用于为namespace进行资源配额，yaml文件示例：\napiVersion: v1 kind: ResourceQuota metadata: name: dev-qt namespace: dev-ns spec: hard: # 硬性全局限制 requests.cpu: 10 # 10个CPU requests.memory: 10Gi limits.cpu: 10 limits.memory: 20Gi requests.storage: 100Gi persistentvolumeclaims: 100 pods: 100 configmaps: 100 secrets: 100 services: 10 count/jobs.batch: 1 count/cronjobs.batch: 1 count/deployments.apps: 1 在添加资源配额后，要求所有在里面运行的 Pod 都必须用字段 resources 声明资源需求，否则就无法创建。可以用LimitRange自动为Pod加上资源限制，LimitRange的yaml：\napiVersion: v1 kind: LimitRange metadata: name: dev-limits namespace: dev-ns spec: limits: - type: Container defaultRequest: cpu: 200m memory: 50Mi default: cpu: 500m memory: 100Mi - type: Pod max: cpu: 800m memory: 200Mi HorizontalPodAutoscaler 基于Metrics Server，它从 Metrics Server 获取当前应用的运行指标，主要是 CPU 使用率，再依据预定的策略增加或者减少 Pod 的数量 yaml:\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: ngx-hpa spec: maxReplicas: 10 minReplicas: 2 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: ngx-hpa-dep targetCPUUtilizationPercentage: 5 在Deployment的spec下一定要写resources字段，否则无法实现自动扩缩容\nMetrics Server Metrics Server 是一个专门用来收集 Kubernetes 核心资源指标（metrics）的工具，它定时从所有节点的 kubelet 里采集信息,安装Metrics Server后可以使用kubectl top查看资源使用情况\nPrometheus 相对于Metrics Server，Prometheus的指标更全一些。Prometheus 是云原生监控领域的“事实标准”，用 PromQL 语言来查询数据，配合 Grafana可以展示直观的图形界面，方便监控。\nkubuadm——一键部署 使用方法：\n# 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join \u0026lt;Master 节点的 IP 和端口 \u0026gt; Kubernetes在部署时，它的每一个组件都是一个需要被执行的、单独的二进制文件。所以部署十分麻烦，kubeadm的方案是把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。\n滚动更新 Kubernetes中rsion使用Pod模版的hash值来作为版本号。 要实现滚动更新，主要有以下几个命令：\nkubectl apply -f xxx.yaml,修改yaml文件后应用即可，Kubernetes会进行滚动更新（逐步的创建新版本Pod，删除旧版本Pod，最后只剩新版本Pod，旧版本Pod全部删除） kubectl rollout pause暂停更新 kubectl rollout resume恢复更新 kubectl rollout history查看版本历史，加上参数\u0026ndash;revision x，查看指定版本详细信息 kubectl rollout undo，回滚到上一个版本，\u0026ndash;to-revision参数指定版本 Deployment 的 metadata添加字段annotations类似label，但是label用于外部对象，annotations用于内部对象，其中kubernetes.io/change-cause是更新版本说明。 网络模型 Docker有三种网络模式：null、host 和 bridge，但是只适用于单机，Kubernetes提出了自己的网络模型IP-per-pod：\n集群里的每个 Pod 都会有唯一的一个 IP 地址。 Pod 里的所有容器共享这个 IP 地址。 集群里的所有 Pod 都属于同一个网段。 Pod直接可以基于IP地址访问另一个Pod，不需要做网络地址转换（NAT） CNI（Container Networking Interface） CNI 为网络插件定义了一系列通用接口，CNI插件大致分为三种：\nOverlay：它构建了一个工作在真实底层网络之上的“逻辑网络”，把原始的 Pod 网络数据封包，再通过下层网络发送出去，到了目的地再拆包。因为这个特点，它对底层网络的要求低，适应性强，缺点就是有额外的传输成本，性能较低。 Route：在底层网络之上工作，但它没有封包和拆包，而是使用系统内置的路由功能来实现Pod 跨主机通信。它的好处是性能高，不过对底层网络的依赖性比较强，如果底层不支持就没办法工作了 Underlay： 直接用底层网络来实现 CNI，也就是说 Pod 和宿主机都在一个网络里，Pod和宿主机是平等的。它对底层的硬件和网络的依赖性是最强的，因而不够灵活，但性能最高。 常用的CNI插件：\nFlannel：最早是Overlay模式，后来用 Host-Gateway 技术支持了 Route 模式，简单易用、但是性能不是很好 Calico：Route模式使用BGP 协议来维护路由信息，性能要比 Flannel 好，而且支持多种网络策略，具备数据加密、安全隔离、流量整形等功能 Cilium：同时支持 Overlay 模式和 Route 模式，它的特点是深度使用了 Linux eBPF 技术，在内核层次操作网络数据，所以性能很高，可以灵活实现各种功能 参考资料： 《Kubernetes入门实战课》— 罗剑锋（极客时间）\n","permalink":"https://oyzg.github.io/archives/kubernetes/","summary":"容器编排","title":"Kubernetes"},{"content":"最近看了张宏杰的《曾国藩传》和周岭的《认知觉醒：开启自我改变的原动力》，读《认知觉醒》的过程中，很多能从曾国藩那儿学到的好习惯，在《认知觉醒》这本书中系统化地揭示了。\n比如曾国藩坚持写日记、读书，正和《认知觉醒》所说的每日反省、读书一样。\n《认知觉醒》 三重大脑 按照《认知觉醒》里面说的，大脑分为三个区域：本能脑、情绪脑、理智脑。本能脑和情绪脑占大多数，理智脑只有小部分，但正是理智脑最高级，能给予我们正确的指导，但是它太弱了，本能脑和情绪脑掌控着大脑，而本能脑和情绪脑又是倾向于低能耗、轻松、不用动脑的活动，所以导致了我们目光短浅、及时满足的天性。我们不用为我们有这种天性而烦恼，而是应该去学会如何控制他们，如果让本能脑和情绪脑为理智脑所控制。\n潜意识 理性可以理解为意识，感性可以理解为潜意识。我们不应该认为感性就是不好的，比如灵感，所以要学会去利用好两者，先用感性帮助自己选择，再用理性帮助自己思考。 我们很多时候会去回避痛苦，这可能会导致其转换为潜意识，所以在面对痛苦时，正确的做法是去正视它、解决它。\n元认知 元认知是一种更高维度的认知，是对自身思考过程的认知和理解。元认知就是自我反思+主动控制，跳出自身的思考，去思考我们的思考、我们的做法对还是不对，能不能做得更好，然后去控制自己\n专注力、学习力、行动力、情绪力 专注力：保持身心合一、专注当下 学习力：要在拉伸区练习，学会主动学习（输出而非输入），关联-\u0026gt;体系，好的学习策略应该是有反馈的，累了就休息，才能走的久 行动力：很多时候我们明明有想学习的东西，但是还没开始又在刷视频了，主要原因就是我们没有清晰的计划，所以行动力的关键在于清晰 情绪力：任何能造成压力的事情都会挤压我们的心智带宽，在这种情况下我们往往会做出不理智的决定，所以要保持冷静、保持理智 最简单的成长方法 早起 冥想 阅读 写作 跑步 ","permalink":"https://oyzg.github.io/archives/%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92/","summary":"不断反思，不断进步","title":"《认知觉醒》"},{"content":"课程地址：CS186\n缓存管理（Buffer Management） 我们在数据库中的读写操作都是在内存中的页（Page）上进行读写，缓存管理器则负责管理在内存中的页和处理来自文件和索引管理器的页请求。\n由于内存空间有限，我们不可能将所有页都保存在内存中，当内存满了而我们不存在我们需要的页时，我们就需要从磁盘中读取相应的页到内存中，同时将内存中的页写入磁盘中。\n在内存中，我们在缓存池（Buffer Pool）中，我们将空间划分为多个帧（Frame）来存放页，每一个帧中存放一个页。\n为了有效的管理缓存池中的帧，缓存管理器在内存中分配了额外的空间用来存放一张元数据表（Metadata table），表中包含四个字段：\nFrame ID：对应唯一的内存地址 Page ID：决定现在帧中包含的页 Dirty Bit：用来验证当前页是否被修改 Pin Count：用来记录当前页的请求者数量 处理请求 当请求的的页在内存中时，该帧的pin count加一，并且返回该页的内存地址 当请求的页不在但是buffer pool没有满时，找到这个页并读入内存，该页pin count设为1，并返回内存地址 当请求的页不在并且buffer pool没有空间时，就需要进行页面的置换 页面的置换策略很大程度上取决于页面访问模式，通过计算页面点击数来选择最佳策略。 Page hit就是请求的页在内存中，不需要读磁盘； Page miss就请求的页不在内存中，需要读磁盘，会有一个额外的IO消耗，所以好的置换策略是性能的关键因素。 Hit rate就是用Page hit数除以总的请求数。\n如果换出的页设置了dirty bit，需要写入磁盘来确保持久化，如果页在内存中有更新，dirty bit设置为1，写入磁盘后设置为0.\n当一个请求完成后会通知磁盘管理器减少该页的pin count。\nLRU（Least Recently Used-最近最少使用） 在LRU算法中，为了便于找到最近最少使用的页，我们需要在metadata table中加一列Last Used用来记录上一次使用的时间。\nLRU算法则是将pin count = 0 并且 last used最小的页换出，将需要的页换入。\n在具体计算中，我们可以按照如上方法进行计算，在数据量很少时，我们可以从当前插入的页往前，找到最久没有使用的页即可。\nMRU（Most Recently Used-最近最多使用） 在直觉上我们可能无法理解MRU算法，但是MRU的适用场景是这样的：比如我们在size=3的pool中循环查找（A、B、C、D），这种情况下，使用MRU的性能更佳，平均每size次只需置换一次。\n在MRU算法中则不需要添加额外的列，当需要置换时，我们只需将pin count = 0 并且最后换入的页换出即可\nClock Ploicy 在Clock Policy中，需要添加一列Ref bit来存储a Last Used value，并且Ref bit只需要1bit，而不是多个bit，它不同于LRU中的Last Used的地方在于最近使用的页为0，其他的页为1，因此节省的时间和空间。\n此外，Clock Policy还需要一个变量Clock Hand来记录考虑中的帧（这里为理解为下一次换出的页）\n在初始化时，所有ref bit为1，clock hand指向第一个unpinned（pin count = 0）的帧.\nClock Policy换出页面的程序如下：\n从0到最后循环遍历表中所有的帧，跳过pinned的帧，直到找到第一个unpinned且ref bit = 0的帧 如果当前遍历的帧 ref bit = 1，那么设置为0，并且将clock hand设置为下一个帧 找到后，移除这个页并且将ref bit设为1，将clock hand设为下一个帧 排序 不同与传统的排序算法，在数据库中对数据进行排序时，我们每次将页读入或者写出都是一次IO操作，因此我们对与时间复杂度的度量也不再是用O(),而是IO操作的次数。\nTwo Way External Merge Sort（二路归并算法） 为了有效的合并两个列表，我们需要保证两个列表都是有序的，而在最开始我们无法保证其是否有序，所以第一次阶段每一个列表都只有一个单独的页，我们称这个阶段为“conquer”。\n接着我们就可以开始进行合并排序，我们称合并的结果为“sorted runs”，一个“sorted runs”就是一个有序的页序列。\n直到我们只剩下一个“sorted runs”，排序才算是完成了。\n分析 在分析一个数据库算法时，最重要的指标就是这个算法消耗的IO次数。\n在二路归并算法中，我们假设有n个数据页，那么每次传递数据都需要2*n次IO，读写分别消耗一次IO。\n在整个排序的过程中，我们首先需要“conquer”，在这之后，我们需要log(n)次传递来进行合并，所以总共1+log(n)次传递，2n*(1+log(n))次IO操作。\n缓冲页（buffer page）或缓冲帧（buffer frame）是将页存储在内存中的一个槽。在合并的过程中，我们将两个页读入内存，分别占用一个buffer frame，在合并完成后，我们还需要一个buffer frame来存放合并完成的页，我们将合并前的buffer frame叫做“input buffer”，合并后的一个叫做“output buffer”。一旦这个页写满了，我们就需要将其写入磁盘，并开始构建新的页。在二路归并排序中，我们只需要3个buffer frame（两个input buffer， 一个output buffer）。\nFull External Sort（完全外部排序） 在二路归并排序中，我们的第一个阶段只对单个页进行排序，并且整个过程中，我们只用了3个buffer frame，而在实际情况中，buffer frame的个数不止3个，所以二路归并排序没有充分利用资源，并且传递次数更多。\n而完全外部排序能够充分利用buffer frame，假设有B个buffer frame，我们在“conquer”阶段，不是对单个页面进行排序了，而是对B个页进行排序，并得到一个“sorted runs”，在之后的合并阶段中，我们拿出一个buffer frame作为output buffer，剩下的B-1个作为input buffer。\n分析 假设我们需要对n个页进行排序，在“conquer”阶段，我们将n个页导入得到n/B个sorted runs，在后面的合并中，每一次传递都除以B-1，所以总共需要1+log_B-1(n/B)次传递，需要2n*(1+log_B-1(n/b))次IO操作。\n哈希 在数据库中，将类似的值分组在一起叫做hash。\n因为我们无法将所有的数据一次填入内存，所以我们需要构建多个不同的hash表并且将它们连接在一起。但这存在一个问题，如果同样的值存在两张hash表中，我们可以直接将两张hash表连接起来吗？ 答案当然是不可以，所以我们需要确保当一个值在内存中，其他相同的值也在内存中。\n在整个过程中，我们分为两个阶段，第一个阶段是“partitioning”（分片），第二个阶段是“conquer”。一个分片是用一个分片哈希函数得到值相同的集合。假设有B个buffer frame，在每次分片中，我们通过hash得到B-1个分片，也就是B-1个output buffer，因为有一个是作为input buffer。\n在完成分片后，适合内存（页数小于等于B）的分片就能进入hash表构建阶段，不适合的则递归地进行分片直到所有的分片最多只有B页，并且需要使用不同的hash函数。\n假设m为所需分片次数，r_i为第i次分片读入的页数，w_i为第i次分派你写出的页数，X为构建hash表的总页数，那么IO次数为m次分片所有的r_i和w_i的次数加上2X。\n此外hash还存在一些重要的属性：\nr_0 = N: 第一次分片需要读入所有的页 r_i \u0026lt;= w_i： 分片过程可能会创建额外的页 w_i \u0026gt;= r_(i+1)： 可能有的页进入构建阶段了，不需要再次分片 x \u0026gt;= N： 每次分片可能会增加页数 Joins (连接) 参考：知乎陈大炮笔记\n介绍 首先我们需要先了解join到底是什么，像“R INNER JOIN S ON R.name = S.name”的语句是如何执行的。完成一个Join是需要条件的，相当于把R和S两个关系根据匹配条件合并创建一个新关系，即，对于 R 中的每条记录 r_i 使用匹配条件找到 S 中的对应 s_j，并将\u0026rsquo;\u0026lt;r_i, s_j\u0026gt;\u0026lsquo;输出中作为新的行 (r的所有字段后面跟着s的所有字段)。\n在下面的连接算法中，我们不考虑将新关系写入磁盘的代价，因为我们假设join的新关系在稍后执行SQL查询涉及到的另一个操作符中会用到。\n常用两表连接算法 Simple Nested Loop Join 简单嵌套循环连接 简单嵌套循环连接是最简单的连接算法，假设我们有一个B页的缓存区，我们希望连接2个表，R和S，连接条件为θ，策略就是可以先获取 R 中的每条记录，然后再遍历 S 中的所有匹配项，最后产生对应匹配项。\n伪代码如下：\nfor each record ri in R: for each record sj in S: if θ(ri,sj): yield \u0026lt;ri, sj\u0026gt; 这是一个糟糕的算法，我们从 R 中读取每条记录时，需要读取 S 中的每一页来寻找匹配。产生的I/O开销是 [R]+|R|[S] ，其中 [R] 是 R 中的页数， |R| 是 S 中的记录数。\nPage Nested Loop Join 页嵌套循环连接 相对于简单嵌套循环，页嵌套循环不是对R中每一条记录去读S的每一页，而是对R中的每一页读S中的每一页，从而减少了读取S中每一页的次数。\nfor each page pr in R: for each page ps in S: for each record ri in pr: for each record sj in ps: if θ(ri, sj): yield \u0026lt;ri, sj\u0026gt; 因此，页嵌套循环的IO开销为[R]+[R][S]。\nBlock Nested Loop Join 块嵌套循环连接 在页嵌套循环中，我们只用了3个缓存页（一个用于R，一个用于S，一个为输出缓冲区），我们想要读S的次数越少越好，所以，我们可以将B-2个缓存页用于R，这B-2个页称为Chunk，将Chunk中的每一条记录与S中的每一条记录相匹配，这样就可以进一步减少读S的次数。\n这里的关键思想是，我们想利用 缓冲区 来降低 I/O 开销，所以我们可以在缓冲区里尽可能多存储 R 的页，因为我们每个 Chunk 针对 S 中页都只读一次，所以更大的 Chunk 意味着更少的 I/O。然后用 R 的每个块与 S 的每条记录进行匹配。\nfor each block of B−2 pages Br in R: for each page ps in S: for each record ri in Br: for each record sj in ps: if θ(ri,sj): yield \u0026lt;ri, sj\u0026gt; IO开销为[R] + [R/(b-2)][S]\nIndex Nested Loop Join 索引嵌套循环连接 当S上有一个对应字段的索引时，它可以非常快的在S中查找r_i的匹配，其IO开销为[R + [R]*(在S中查找匹配记录的成本)，其中在S中查找匹配记录的成本因索引的类型而不同。\nfor each record ri in R: for each record sj in S where θ(ri,sj)==true: yield \u0026lt;ri, sj\u0026gt; Hash Join 哈希连接 如果R的记录小于等于B-2页，则我们可以在构建一个哈希表，如何读取S，在R的哈希表中查找匹配的记录。这称为\u0026rsquo;Naive Hash Join\u0026rsquo;朴素哈希连接，成本为[R]+[S]。它依赖于R能够装入内存，但这通常不太可能。此外我们需要把R构建成哈希表，这里成本也很高。\n为了解决这个问题，我们可以重复地将 R 和 S 哈希到 B-1 缓冲区中，这样我们就可以得到 ≤B−2 页大小的分区，使我们能够将它们放入内存中并执行朴素哈希连接。\n更具体地说，考虑每一对相关的分区 Ri 和 Si (即R的分区i和S的分区i)。如果 Ri 和 Si 都是大于 B-2 页，则将两个分区哈希成更小的分区。否则，如果 Ri 或 Si 小于等于 B-2 页，则停止分区并将较小的分区加载到内存中，以构建内存中的哈希表，并与其较大的分区执行朴素哈希连接。\n这个过程被称为 Grace Hash Join 优雅哈希连接，它的I/O代价是：子节点上哈希的代价加上朴素哈希连接的代价。哈希的代价可以根据我们需要重复哈希多少个分区的次数而改变。对分区 P 进行哈希的代价包括读取 P 中的所有页所需的 I/O 以及在对分区 P 进行哈希后写入所有结果分区所需的 I/O。\nGrace Hash Join 是很好，但它对键倾斜非常敏感，所以在使用这个算法时要小心。当许多记录具有相同的键时，就会发生键倾斜。例如，如果我们对一个列进行哈希，这个列的值只有 yes，虽然我们可以强行构建，但不管我们使用哪个哈希函数，它们最终都会在同一个桶中。\nSort-Merge Join 排序合并连接 // TODO\nAn Important Refinement 排序合并连接的一个重要改进 // TODO\n","permalink":"https://oyzg.github.io/archives/cs186%E7%AC%94%E8%AE%B0-rookiedb/","summary":"CS186学习笔记.","title":"CS186笔记 RookieDB"},{"content":"贪心算法 基础 贪心的本质是选择每一阶段的局部最优，从而达到全局最优。\n所以要使用贪心算法，就得找出贪在哪里\n贪心算法没有套路，对于是否能用贪心，最好就是举反例，如果能举出来，则不能用\n贪心算法一般步骤：\n将问题分解为若干个子问题 找出适合的贪心策略 求解每一个子问题的最优解 将局部最优解堆叠成全局最优解 常见算法题 455. 分发饼干 // 小饼干喂小胃口，大饼干喂大胃口 // 先排序，然后按顺序找到满足胃口的最小饼干，可以从大到小，也可以从小到大 func findContentChildren(g []int, s []int) int { sort.Ints(g) sort.Ints(s) i, j := 0, 0 count := 0 for i \u0026lt; len(g) \u0026amp;\u0026amp; j \u0026lt; len(s) { if s[j] \u0026gt;= g[i] { j++ i++ count++ } else { j++ } } return count } 376. 摆动序列 // 删除一些元素，使其成为摆动序列，那么要找到这样一个子序列，就要找到所有峰值 func wiggleMaxLength(nums []int) int { n := len(nums) if n == 1 || (n == 2 \u0026amp;\u0026amp; nums[0] != nums[1]) { return n } pre := 0 cur := 0 count := 1 for i := 0; i \u0026lt; n-1; i++ { cur = nums[i+1] - nums[i] if (cur \u0026gt; 0 \u0026amp;\u0026amp; pre \u0026lt;= 0) || (cur \u0026lt; 0 \u0026amp;\u0026amp; pre \u0026gt;= 0) { pre = cur count++ } } return count } 53. 最大子数组和\n// 如果前面加起来小于0，那我就不加前面的就可以了 func maxSubArray(nums []int) int { sum := 0 res := nums[0] for _, val := range nums { if sum \u0026lt; 0 { sum = 0 } sum += val if sum \u0026gt; res { res = sum } } return res } 122. 买卖股票的最佳时机 II // 可以随意买入卖出，但最多持有一张股票，那只要前一天的比后一天的小就买入，然后再卖出，反正不要手续费 func maxProfit(prices []int) int { profit := 0 n := len(prices) for i := 1; i \u0026lt; n; i++ { if prices[i] - prices[i-1] \u0026gt; 0 { profit += prices[i] - prices[i-1] } } return profit } 55. 跳跃游戏 // 这题不要到底想跳几步，也不是跳得越大越好，应该想成我的跳跃范围能不能覆盖终点 func canJump(nums []int) bool { max := 0 // 最大的覆盖范围 for i, val := range nums { if i \u0026gt; max { // 如果不能跳到这里了 return false } if i + val \u0026gt; max { max = i + val } } return true } 45. 跳跃游戏 II // 求最小步数，那么就是目前覆盖范围内的下一个最大覆盖范围，然后这里算一步 // 比如目前我位置是0，值为3，所以覆盖范围是3，然后我求出[1,3]中的最大覆盖范围，假如为7， // 那么我从0到7就只要2步，所以我们得保存2个覆盖范围 func jump(nums []int) int { max := 0 // 最大的覆盖范围 premax := 0 // 上一段的最大覆盖范围 n := len(nums) count := 0 for i, val := range nums { if i \u0026gt;= n-1 { return count } if i + val \u0026gt; max { max = i+val } if i == premax { count++ premax = max } } return -1 } 1005. K 次取反后最大化的数组和 // 贪心：将最大的负数取反，如果没有负数了，且k为奇数，则将最小的数取反 func largestSumAfterKNegations(nums []int, k int) int { n := len(nums) sum := 0 sort.Ints(nums) // 先排序，从小到大，负数在前 for i := 0; i \u0026lt; n; i++ { if nums[i] \u0026lt; 0 \u0026amp;\u0026amp; k \u0026gt; 0 { // 将前k个负数取反 nums[i] *= -1 k-- } sum += nums[i] } if k % 2 == 0 { return sum } sort.Ints(nums) return sum - 2*nums[0] } 134. 加油站 func canCompleteCircuit(gas []int, cost []int) int { cur := 0 // 用于统计从某下标到当前位置的和 sum := 0 // 用于表示总和 start := 0 for i := 0; i \u0026lt; len(gas); i++ { cur += gas[i] - cost[i] sum += gas[i] - cost[i] if cur \u0026lt; 0 { // 如果cur小于0了，说明从之前的坐标开始不能走完，所以从i+1开始走 start = i+1 cur = 0 } } if sum \u0026gt;= 0 { // 如果总和大于0，才可能走完 return start } return -1 } 135. 分发糖果 // 思路：这题容易纠结从左到右还是从右到左，但是其实不需要纠结，两个方向我们都需要考虑 //\t所以可以先从一个方向，再从另一个方向 func candy(ratings []int) int { if len(ratings) \u0026lt;= 1 { return len(ratings) } candy := make([]int, len(ratings)) candy[0] = 1 // 先从左到右，如果比左边大就加一，否则等于1 for i := 1; i \u0026lt; len(ratings); i++ { if ratings[i] \u0026gt; ratings[i-1] { candy[i] = candy[i-1] + 1 } else { candy[i] = 1 } } // 再从右到左，如果比右边大，则比较candy[i]和candy[i+1]+1,取大值 for i := len(ratings)-2; i \u0026gt;= 0; i-- { if ratings[i] \u0026gt; ratings[i+1] \u0026amp;\u0026amp; candy[i+1]+1 \u0026gt; candy[i] { candy[i] = candy[i+1] + 1 } } // 最后求和即可 sum := 0 for _, val := range candy { sum += val } return sum } 860. 柠檬水找零 func lemonadeChange(bills []int) bool { money := []int{0, 0, 0} // 分别表示5,10,20的数量 for _, val := range bills { if val == 5 { // 当收到5时，直接收入即可 money[0]++ } else if val == 10 { // 收到10时，找5元 if money[0] \u0026lt;= 0 { // 如果没有5元，返回false return false } money[0]-- money[1]++ } else { // 如果是20 if money[1] \u0026gt; 0 \u0026amp;\u0026amp; money[0] \u0026gt; 0 { // 先看有没有一张10块一张5块 money[0]-- money[1]-- } else if money[0] \u0026gt;= 3 { // 再看有没有3张5块 money[0] -= 3 } else { // 都没有的话返回false return false } money[2]++ } } return true } 406. 根据身高重建队列 // 思路：这题要用贪心算法的话，必须先排序，但是按哪个排序呢 // 这里有2个维度（h和k），我们需要先确定一个维度，再考虑另一维度，这里我先按身高从大到小排序 func reconstructQueue(people [][]int) [][]int { // 按身高从到小排序 sort.Slice(people, func(i, j int) bool { if people[i][0] == people[j][0] { return people[i][1] \u0026lt; people[j][1] } return people[i][0] \u0026gt; people[j][0] }) res := make([][]int, 0) // 然后按照k来进行插入 for _, info := range people { res = append(res, info) copy(res[info[1]+1:], res[info[1]:]) res[info[1]] = info } return res } 452. 用最少数量的箭引爆气球 // 这个题也是一样，考虑按start排序还是按end排序 // 所以我们需要先确定一个维度，这里我按start排序 func findMinArrowShots(points [][]int) int { // 先按start从小到大排序 sort.Slice(points, func(i, j int) bool { return points[i][0] \u0026lt; points[j][0] }) res := 1 // 然后对当前start最小的气球，找到start小于等于它的end的气球，这段区间中的气球可以一次射爆 for i, _ := range points { if i \u0026gt; 0 \u0026amp;\u0026amp; points[i][0] \u0026gt; points[i-1][1] { res++ } else { if i \u0026gt; 0 \u0026amp;\u0026amp; points[i-1][1] \u0026lt; points[i][1] { points[i][1] = points[i-1][1] } } } return res } 435. 无重叠区间 // 这题也是一样，按start排序还是按end排序，其实都可以，只不过后序的遍历顺序不一样 func eraseOverlapIntervals(intervals [][]int) int { // 这里我按end排序 sort.Slice(intervals, func(i, j int) bool { return intervals[i][1] \u0026lt; intervals[j][1] }) end := intervals[0][1] count := 1 // 所有start小于等于当前end的区间，都要合并 for i := 1; i \u0026lt; len(intervals); i++ { if intervals[i][0] \u0026lt; end { continue } end = intervals[i][1] count++ } return len(intervals) - count } 763. 划分字母区间 // 这题比较简单，只需找到所有字母最后一次出现位置，然后遍历即可 func partitionLabels(s string) []int { end := make([]int, 26) for i,c := range s { end[int(c - \u0026#39;a\u0026#39;)] = i } cur := 0 pre := -1 res := []int{} for i,c := range s { if end[int(c - \u0026#39;a\u0026#39;)] \u0026gt; cur { cur = end[int(c - \u0026#39;a\u0026#39;)] } if i == cur { res = append(res, cur - pre) pre = cur cur = i+1 } } return res } 56. 合并区间 // 和删除重叠区间不同，我们需要实时更新边界，所以我们需要先确定一个边界 func merge(intervals [][]int) [][]int { res := [][]int{} // 按start排序 sort.Slice(intervals, func(i, j int) bool { return intervals[i][0] \u0026lt; intervals[j][0] }) for i := 0; i \u0026lt; len(intervals); i++ { // 更新右边界 if len(res) \u0026gt; 0 \u0026amp;\u0026amp; res[len(res)-1][1] \u0026gt;= intervals[i][0] { if res[len(res)-1][1] \u0026lt; intervals[i][1] { res[len(res)-1][1] = intervals[i][1] } } else { // 加入结果集中 res = append(res, intervals[i]) } } return res } 738. 单调递增的数字 func monotoneIncreasingDigits(n int) int { nums := []int{} for n \u0026gt; 0 { nums = append(nums, n%10) n /= 10 } flag := -1 // 从后往前遍历（这里是因为前面把顺序反过来了），如果前面大于后面，在后面一位变为9，前面一位减一 for i := 1; i \u0026lt; len(nums); i++ { if nums[i] \u0026gt; nums[i-1] { nums[i]-- flag = i-1 } } // 可某个位置变为了9，那么后面位置都变为9，因为要取最大 for i := 0; i \u0026lt;= flag; i++ { nums[i] = 9 } res := 0 for i := len(nums)-1; i \u0026gt;= 0; i-- { res *= 10 res += nums[i] } return res } 714. 买卖股票的最佳时机含手续费 // func maxProfit(prices []int, fee int) int { start := prices[0] // 买入位置 sum := 0 for _, p := range prices { if p \u0026lt;= start { // 如果小于买入位置 则将该处设为买入位置 start = p } else { // 对于一个点我们有时候会纠结现在买还是最高点买，我们想最高点买，但是不确定什么时候是最高点，通过下面这样，我们每次只要有盈利就买入，并且将start设为当前价格减去手续费，就不用纠结是不是最高点了 if p - fee \u0026gt; start { // 如果该处减去手续费大于买入位置，则可以买入 sum += p - fee - start start = p - fee } } } return sum } 968. 监控二叉树 // 在这个题中，每个节点都可能又3种状态，0没有被覆盖，1有摄像头，2有被覆盖 // 然后因为我们需要从下面开始，才能最大程度的减少摄像头数量（叶子节点不装摄像头可以节省叶子节点数，从上往下的话只能节省根节点这一个），所以我们使用后序遍历，最后遍历中节点 func minCameraCover(root *TreeNode) int { var dfs func(root *TreeNode) int count := 0 dfs = func(root * TreeNode) int { if root == nil { return 2 } left := dfs(root.Left) right := dfs(root.Right) // 如果是叶子节点，则没有被覆盖 if root.Left == nil \u0026amp;\u0026amp; root.Right == nil { return 0 } // 如果左右节点都被覆盖，则当前节点没有摄像头可以覆盖到 if left == 2 \u0026amp;\u0026amp; right == 2 { return 0 } // 如果左右节点中有没有被覆盖到的，那就需要安装摄像头 if left == 0 || right == 0 { count++ return 1 } // 如果左右节点中有安装摄像头，就不需要安装摄像头了 if left == 1 || right == 1 { return 2 } return -1 } if dfs(root) == 0 { count++ } return count } 总结 贪心算法的主要就是要找到贪在哪里 对于一些简单题，我们很容易就可以想到贪在哪里 但是对于中等或困难题则不简单，它可能有多个维度，这个时候我们需要逐个确定，慢慢来，先确定一个维度，再去想另一个 在确定维度时，可能先确定哪个维度都可以，但是它的遍历顺序则可能会不同 ","permalink":"https://oyzg.github.io/archives/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","summary":"贪心算法相关.","title":"贪心算法"},{"content":"回溯算法 基础 回溯算法也叫回溯搜索法，是一种搜索的方式\n回溯算法有时可以用于解决一些看起来比较复杂的问题，但是本质是穷举，所以效率还是会比较低\n回溯是从递归延伸出来的，只是在递归调用前后会做一些处理\n回溯一般可以解决以下几种问题：\n组合问题：N个数里面按一定规则找出k个数的集合 切割问题：一个字符串按一定规则有几种切割方式 子集问题：一个N个数的集合里有多少符合条件的子集 排列问题：N个数按一定规则全排列，有几种排列方式 棋盘问题：N皇后，解数独等等 回溯其实可以理解成树形结构，它是一种限高的树\n回溯算法的框架：\nvoid backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } 常见算法题 组合问题 77. 组合 func combine(n int, k int) [][]int { res := [][]int{} path := []int{} var travel func(n, k, startIndex int) travel = func(n, k, startIndex int) { if k == 0 { tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) return } for i := startIndex; i \u0026lt;= n-k+1; i++ { path = append(path, i) // 处理节点 travel(n, k-1, i+1) // 递归 path = path[:len(path)-1] // 回溯 } } travel(n, k, 1) return res } 216. 组合总和 III func combinationSum3(k int, n int) [][]int { res := [][]int{} path := []int{} var travel func(k, n, start int) travel = func(k, n, start int) { if k == 0 { if n == 0 { tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) } return } for i := start; i \u0026lt;= 9 - k + 1; i++ { path = append(path, i) travel(k-1, n-i, i+1) path = path[:len(path)-1] } } travel(k, n, 1) return res } 17. 电话号码的字母组合 func letterCombinations(digits string) []string { res := []string{} if len(digits) == 0 { return res } form := map[byte]string{\u0026#39;2\u0026#39;:\u0026#34;abc\u0026#34;, \u0026#39;3\u0026#39;:\u0026#34;def\u0026#34;, \u0026#39;4\u0026#39;:\u0026#34;ghi\u0026#34;, \u0026#39;5\u0026#39;:\u0026#34;jkl\u0026#34;, \u0026#39;6\u0026#39;:\u0026#34;mno\u0026#34;, \u0026#39;7\u0026#39;:\u0026#34;pqrs\u0026#34;, \u0026#39;8\u0026#39;:\u0026#34;tuv\u0026#34;,\u0026#39;9\u0026#39;:\u0026#34;wxyz\u0026#34;} combination := \u0026#34;\u0026#34; var travel func(digits string, start int) travel = func(digits string, start int) { if start == len(digits) { tmp := combination res = append(res, tmp) return } for _, v := range form[digits[start]] { combination += string(v) travel(digits, start+1) combination = combination[:len(combination)-1] } } travel(digits, 0) return res } 39. 组合总和 // 这题跟前面的题不同之处便在于可以无限次数的重复选取，所以要注意递归的地方start不要+1 func combinationSum(candidates []int, target int) [][]int { res := [][]int{} if len(candidates) == 0 { return res } combination := []int{} var backtracking func(candidates []int, target, start int) backtracking = func(candidates []int, target, start int) { if target == 0 { tmp := make([]int, len(combination)) copy(tmp, combination) res = append(res, tmp) } if target \u0026lt; 0 { return } for i := start; i \u0026lt; len(candidates); i++ { combination = append(combination, candidates[i]) backtracking(candidates, target - candidates[i], i) //不要+1，同时可以防止取前面的数，造成重复的答案 combination = combination[:len(combination)-1] } } backtracking(candidates, target, 0) return res } 40. 组合总和 II // 思路：这题与上一题的区别在于集合可重复，但是结果不能重复，对应到树上就是同树层不重复，树枝可重复 // 所以结果需要去重，但是用map的话可能会超时，所以需要使用别的方法对树层去重 // 树层去重的话需要对数组排序，并且使用used数组记录同一树层中上一个数是否使用过 func combinationSum2(candidates []int, target int) [][]int { res := [][]int{} if len(candidates) == 0 { return res } sort.Ints(candidates) used := make([]bool, len(candidates)) combination := []int{} var backtracking func(candidates []int, target, start int) backtracking = func(candidates []int, target, start int) { if target == 0 { tmp := make([]int, len(combination)) copy(tmp, combination) res = append(res, tmp) } if target \u0026lt; 0 { return } for i := start; i \u0026lt; len(candidates); i++ { if i \u0026gt; 0 \u0026amp;\u0026amp; candidates[i] == candidates[i-1] \u0026amp;\u0026amp; !used[i-1]{ continue // false说明上一个数在同一树层中未 } used[i] = true combination = append(combination, candidates[i]) backtracking(candidates, target - candidates[i], i+1) combination = combination[:len(combination)-1] used[i] = false } } backtracking(candidates, target, 0) return res } 分割问题(组合问题) 131. 分割回文串 // 思路和组合问题一样 func partition(s string) [][]string { res := [][]string{} path := []string{} var backtracking func(s string, start int) backtracking = func(s string, start int) { if start == len(s) { tmp := make([]string, len(path)) copy(tmp, path) res = append(res, tmp) return } for i := start; i \u0026lt; len(s); i++ { if isPartition(s, start, i) { path = append(path, s[start:i+1]) backtracking(s, i+1) path = path[:len(path)-1] } } } backtracking(s, 0) return res } //判断是否为回文 func isPartition(s string,startIndex,end int)bool{ left:=startIndex right:=end for ;left\u0026lt;right;{ if s[left]!=s[right]{ return false } //移动左右指针 left++ right-- } return true } 93. 复原 IP 地址 func restoreIpAddresses(s string) []string { res := []string{} path := \u0026#34;\u0026#34; var backtracking func(s string, start, count int) backtracking = func(s string, start, count int) { if count == 4 { if start == len(s) { tmp := path res = append(res, tmp[:len(tmp)-1]) } return } for i := start; i \u0026lt; start+3 \u0026amp;\u0026amp; i \u0026lt; len(s); i++ { tmp := s[start:i+1] if !check(tmp) { return } path += tmp+\u0026#34;.\u0026#34; backtracking(s, i+1, count+1) path = path[:len(path)-i+start-2] } } backtracking(s, 0, 0) return res } func check(s string) bool { if s[0] == \u0026#39;0\u0026#39; \u0026amp;\u0026amp; len(s) \u0026gt; 1 { return false } num, _ := strconv.Atoi(s) if num \u0026gt; 255 { return false } return true } 子集问题 78. 子集 func subsets(nums []int) [][]int { res := [][]int{} path := []int{} var backtracking func(nums []int, start int) backtracking = func(nums []int, start int) { if start \u0026gt; len(nums) { return } tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) for i := start; i \u0026lt; len(nums); i++ { path = append(path, nums[i]) backtracking(nums, i+1) path = path[:len(path)-1] } } backtracking(nums, 0) return res } 90. 子集 II func subsetsWithDup(nums []int) [][]int { res := [][]int{} path := []int{} used := make([]bool, len(nums)) sort.Ints(nums) // 这种用到used数组的一定要先排序 var backtracking func(nums []int, start int) backtracking = func(nums []int, start int) { if start \u0026gt; len(nums) { return } tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) for i := start; i \u0026lt; len(nums); i++ { if i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i-1] \u0026amp;\u0026amp; !used[i-1] { continue; } used[i] = true path = append(path, nums[i]) backtracking(nums, i+1) path = path[:len(path)-1] used[i] = false } } backtracking(nums, 0) return res } 491. 递增子序列 func findSubsequences(nums []int) [][]int { res := [][]int{} path := []int{} var backtracking func(nums []int, start int) backtracking = func(nums []int, start int) { if len(path) \u0026gt;= 2 { tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) } set := map[int]int{} // 因为是求子序列，不能排序，所以用map来去重（同树层去重） for i := start; i \u0026lt; len(nums); i++ { if _,ok := set[nums[i]]; ok || (len(path) \u0026gt; 0 \u0026amp;\u0026amp; nums[i] \u0026lt; path[len(path)-1]) { continue } set[nums[i]]++ path = append(path, nums[i]) backtracking(nums, i+1) path = path[:len(path)-1] } } backtracking(nums, 0) return res } 排列问题 46. 全排列 func permute(nums []int) [][]int { res := [][]int{} path := []int{} n := len(nums) used := make([]bool, n) // 使用used数组去重 var backtracking func(nums []int) backtracking = func(nums []int) { if len(path) == n { // 因为要取所有的排列，所以根据长度判断 tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) return } for i := 0; i \u0026lt; n; i++ { if used[i] { continue } used[i] = true path = append(path, nums[i]) backtracking(nums) path = path[:len(path)-1] used[i] = false } } backtracking(nums) return res } 47. 全排列 II func permuteUnique(nums []int) [][]int { res := [][]int{} path := []int{} n := len(nums) sort.Ints(nums) used := make([]bool, n) var backtracking func(nums []int) backtracking = func(nums []int) { if len(path) == n { tmp := make([]int, len(path)) copy(tmp, path) res = append(res, tmp) return } for i := 0; i \u0026lt; n; i++ { if used[i] || (i \u0026gt; 0 \u0026amp;\u0026amp; nums[i] == nums[i-1] \u0026amp;\u0026amp; !used[i-1]) { continue } used[i] = true path = append(path, nums[i]) backtracking(nums) path = path[:len(path)-1] used[i] = false } } backtracking(nums) return res }\t332. 重新安排行程 func findItinerary(tickets [][]string) []string { targets := map[string]pairs{} for _, ticket := range tickets { if targets[ticket[0]] == nil { targets[ticket[0]] = make(pairs, 0) } targets[ticket[0]] = append(targets[ticket[0]], \u0026amp;pair{target: ticket[1], visited: false,}) } for _, target := range targets { sort.Sort(target) } res := []string{\u0026#34;JFK\u0026#34;} n := len(tickets) var backtracking func() bool backtracking = func() bool { if len(res) == n+1 { return true } m := targets[res[len(res)-1]] for _, pair := range(m) { if pair.visited { continue } res = append(res, pair.target) pair.visited = true if backtracking() { return true } pair.visited = false res = res[:len(res)-1] } return false } backtracking() return res } type pair struct { target string visited bool } type pairs []*pair func (p pairs) Len() int { return len(p) } func (p pairs) Swap(i, j int) { p[i], p[j] = p[j], p[i] } func (p pairs) Less(i, j int) bool { return p[i].target \u0026lt; p[j].target } 51. N 皇后 func solveNQueens(n int) [][]string { res := [][]string{} path := [][]string{} for i := 0; i \u0026lt; n; i++ { s := []string{} for j := 0; j \u0026lt; n; j++ { s = append(s, \u0026#34;.\u0026#34;) } path = append(path, s) } var backtracking func(row int) backtracking = func(row int) { if row == n { tmp := make([]string, n) for i := 0; i \u0026lt; n; i++ { s := \u0026#34;\u0026#34; for j := 0; j \u0026lt; n; j++ { s += path[i][j] } tmp[i] = s } res = append(res, tmp) return } for i := 0; i \u0026lt; n; i++ { if !isValid(n, row, i, path) { continue } path[row][i] = \u0026#34;Q\u0026#34; backtracking(row+1) path[row][i] = \u0026#34;.\u0026#34; } } backtracking(0) return res } func isValid(n, row, col int, chessboard [][]string) bool { for i := 0; i \u0026lt; row; i++ { if chessboard[i][col] == \u0026#34;Q\u0026#34; { return false } } for i, j := row-1, col-1; i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026gt;= 0; i, j = i-1, j-1 { if chessboard[i][j] == \u0026#34;Q\u0026#34; { return false } } for i, j := row-1, col+1; i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; n; i, j = i-1, j+1 { if chessboard[i][j] == \u0026#34;Q\u0026#34; { return false } } return true } 37. 解数独 func solveSudoku(board [][]byte) { var backtracking func(board [][]byte) bool backtracking = func(board [][]byte) bool { for i := 0; i \u0026lt; 9; i++ { for j := 0; j \u0026lt; 9; j++ { if board[i][j] != \u0026#39;.\u0026#39; { continue } for v := \u0026#39;1\u0026#39;; v \u0026lt;= \u0026#39;9\u0026#39;; v++ { if check(i, j, byte(v), board) { board[i][j] = byte(v) if backtracking(board) { return true } board[i][j] = \u0026#39;.\u0026#39; } } return false } } return true } backtracking(board) } func check(row, col int, val byte, board [][]byte) bool { for i := 0; i \u0026lt; 9; i++ { if board[i][col] == val { return false } } for i := 0; i \u0026lt; 9; i++ { if board[row][i] == val { return false } } a := row/3 b := col/3 for i := 0; i \u0026lt; 3; i++ { for j := 0; j \u0026lt; 3; j++ { if board[a*3+i][b*3+j] == val { return false } } } return true } 总结 回溯算法其实不难，但是它可以解决一些有点难度的题，从上面的题目中也可以看出来，基本上都是中等题，但是它效率不高，可以说是中等题困难题的入门算法吧\n回溯算法有一个模版，大多数回溯算法的题都可以在模版的基础上进行修改即可\nvoid backtracking(参数) { if (终止条件) { 存放结果; return; } for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) { 处理节点; backtracking(路径，选择列表); // 递归 回溯，撤销处理结果 } } 对于一些简单的题，套用上面模版基本上就可以解决了，但是对于一些稍微困难一点点的题，它会加一些条件，比如集合中可以重复，集合中的元素可无限次选取等等\n集合可以重复的，而结果不能重复的情况：需要注意结果去重，这个去重指的是同一树层的去重 集合中元素可无限次选取（39.组合总和）：递归时start不需要+1 去重：去重可以分为同一树层去重和同一树枝去重\n同一树枝去重的话有两种方法： 排序+used数组：比如（40.组合总和II) set或者map去重：对于（491.递增子序列）这种不能排序的题则只能用set或map来去重了 一般来说：组合问题和排列问题是在树形结构的叶子节点上收集结果，而子集问题就是取树上所有节点的结果\n解题时注意一步一步慢慢来，像N皇后、解数独这种困难题，先把判断的方法写出来，剩下的其实和其他中等题差不多\n","permalink":"https://oyzg.github.io/archives/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/","summary":"回溯算法相关.","title":"回溯算法"},{"content":"二叉树 基础 先来看看定义：\n// Java public class TreeNode { int val; // 值 TreeNode left; // 左节点 TreeNode right;// 右节点 TreeNode() {} TreeNode(int val) { this.val = val; } } // Go type TreeNode struct { Val int Left *TreeNode Right *TreeNode } 二叉树在结构上其实很简单，就是有一个左节点和一个右节点，相比于链表多了一个左节点，如果左节点都为null，那就成了链表。\n再来看看二叉树的存储方式\n二叉树的存储结构有两种，一种链式存储，一种顺序存储，平时我们看到的可能都是链式存储结构，顺序存储也很简单，就是二叉树的层次遍历结果\n再来看看二叉树的遍历顺序：\n二叉树的遍历顺序有两种：\n深度优先遍历（DFS） 前序遍历 中序遍历 后序遍历 广度优先遍历（BFS） 深度优先遍历的三种顺序指的是中间节点的遍历顺序，比如中序遍历就是左中右，中在中间遍历，所以是中序遍历；此外这三种遍历都有两种实现方法，递归法和迭代法，递归的本质就是栈，所以迭代法其实就是用栈实现\n广度优先遍历比较简单，它的实现只有一种方法，迭代法，是用队列来实现的\n最后来看看几个特别的二叉树：\n满二叉树：\n完全二叉树：\n完全二叉树是完全按照从上到下，从左到右的顺序来摆放的，所以不可能有上面某个节点为空，或者左边某个节点为空的情况\n二叉搜索树：\n左子树的所有节点都小于根节点，右子树的所有节点都大于根节点，中序遍历结果是递增的\n平衡二叉搜索树（AVL）：\n在二叉搜索树的基础上，左右子树的高度差不超过1\n二叉树的遍历 前序遍历\n中序遍历\n后序遍历\n递归遍历 递归三要素：\n确定递归函数的参数和返回值 确定终止条件 确定单层递归的逻辑 /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ // 前序遍历 func preorderTraversal(root *TreeNode) []int { res := []int{} var preorder func(root *TreeNode) preorder = func(root * TreeNode) { if root == nil { return } res = append(res, root.Val) preorder(root.Left) preorder(root.Right) } preorder(root) return res } // 中序遍历 func inorderTraversal(root *TreeNode) []int { res := []int{} var inorder func(root *TreeNode) inorder = func(root * TreeNode) { if root == nil { return } inorder(root.Left) res = append(res, root.Val) inorder(root.Right) } inorder(root) return res } // 后序遍历 func postorderTraversal(root *TreeNode) []int { res := []int{} var postorder func(root *TreeNode) postorder = func(root * TreeNode) { if root == nil { return } postorder(root.Left) postorder(root.Right) res = append(res, root.Val) } postorder(root) return res } 迭代遍历 迭代法就是用栈来模拟递归，因为前序遍历和后序遍历都可以通过先遍历中间节点来实现，因为栈是先进后出，所以需要反着来，前序遍历入栈的时候先放入右节点，再放入左节点，而中间节点直接加入到结果中；而后序遍历就是反过来，先放入左节点，再放入右节点，最后的结果是中右左，所以最后需要将结果反转；而中序遍历则不能和前2种一样，需要使用指针来遍历\n// 前序遍历 func preorderTraversal(root *TreeNode) []int { res := []int{} if root == nil { return res } stack := []*TreeNode{root} for len(stack) \u0026gt; 0 { node := stack[len(stack)-1] stack = stack[:len(stack)-1] res = append(res, node.Val) if node.Right != nil { stack = append(stack, node.Right) } if node.Left != nil { stack = append(stack, node.Left) } } return res } // 中序遍历 func inorderTraversal(root *TreeNode) []int { res := []int{} cur := root stack := []*TreeNode{} for cur != nil || len(stack) \u0026gt; 0 { if cur != nil { stack = append(stack, cur) cur = cur.Left } else { cur = stack[len(stack)-1] stack = stack[:len(stack)-1] res = append(res, cur.Val) cur = cur.Right } } return res } // 后序遍历 func postorderTraversal(root *TreeNode) []int { res := []int{} if root == nil { return res } stack := []*TreeNode{root} for len(stack) \u0026gt; 0 { node := stack[len(stack)-1] stack = stack[:len(stack)-1] res = append(res, node.Val) if node.Left != nil { stack = append(stack, node.Left) } if node.Right != nil { stack = append(stack, node.Right) } } reverse(res) return res } func reverse(a []int) { l, r := 0, len(a) - 1 for l \u0026lt; r { a[l], a[r] = a[r], a[l] l, r = l+1, r-1 } } 此外迭代法还有统一的写法：\n/** type Element struct { // 元素保管的值 Value interface{} // 内含隐藏或非导出字段 } func (l *List) Back() *Element 前序遍历：中左右 压栈顺序：右左中 **/ func preorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u0026gt;0{ e := stack.Back() stack.Remove(e)//弹出元素 if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右左中 if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) } return res } //中序遍历：左中右 //压栈顺序：右中左 func inorderTraversal(root *TreeNode) []int { if root==nil{ return nil } stack:=list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u0026gt;0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：右中左 if node.Right!=nil{ stack.PushBack(node.Right) } stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Left!=nil{ stack.PushBack(node.Left) } } return res } //后续遍历：左右中 //压栈顺序：中右左 func postorderTraversal(root *TreeNode) []int { if root == nil { return nil } var stack = list.New()//栈 res:=[]int{}//结果集 stack.PushBack(root) var node *TreeNode for stack.Len()\u0026gt;0{ e := stack.Back() stack.Remove(e) if e.Value==nil{// 如果为空，则表明是需要处理中间节点 e=stack.Back()//弹出元素（即中间节点） stack.Remove(e)//删除中间节点 node=e.Value.(*TreeNode) res=append(res,node.Val)//将中间节点加入到结果集中 continue//继续弹出栈中下一个节点 } node = e.Value.(*TreeNode) //压栈顺序：中右左 stack.PushBack(node)//中间节点压栈后再压入nil作为中间节点的标志符 stack.PushBack(nil) if node.Right!=nil{ stack.PushBack(node.Right) } if node.Left!=nil{ stack.PushBack(node.Left) } } return res } 层序遍历 [102. 二叉树的层序遍历] func levelOrder(root *TreeNode) [][]int { res := [][]int{} if root == nil { return res } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) level := []int{} for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] level = append(level, node.Val) if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } res = append(res, level) } return res } 107.二叉树的层次遍历II // 思路：将层次遍历结果反转即可 func levelOrderBottom(root *TreeNode) [][]int { res := [][]int{} if root == nil { return res } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) level := []int{} for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] level = append(level, node.Val) if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } res = append(res, level) } reverse(res) return res } func reverse(nums [][]int) { i, j := 0, len(nums)-1 for i \u0026lt; j { nums[i], nums[j] = nums[j], nums[i] i++ j-- } } 199.二叉树的右视图 // 思路：取右视图即取层次遍历结果每层的最后一个节点即可 func rightSideView(root *TreeNode) []int { res := []int{} if root == nil { return res } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] if i == size-1 { res = append(res, node.Val) } if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } } return res } 637. 二叉树的层平均值 // BFS：每层取平均值即可 func averageOfLevels(root *TreeNode) []float64 { res := []float64{} if root == nil { return res } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) sum := 0 for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] sum += node.Val if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } res = append(res, float64(sum)/float64(size)) } return res } 429. N 叉树的层序遍历 // BFS func levelOrder(root *Node) [][]int { res := [][]int{} if root == nil { return res } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) level := []int{} for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] level = append(level, node.Val) for _,child := range node.Children { if child != nil { queue = append(queue, child) } } } res = append(res, level) } return res } 515. 在每个树行中找最大值 // BFS func largestValues(root *TreeNode) []int { res := []int{} if root == nil { return res } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) max := queue[0].Val for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] if node.Val \u0026gt; max { max = node.Val } if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } res = append(res, max) } return res } 116. 填充每个节点的下一个右侧节点指针 // BFS func connect(root *Node) *Node { if root == nil { return root } queue := []*Node{root} for len(queue) \u0026gt; 0 { size := len(queue) var pre *Node = nil for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] if pre != nil { pre.Next = node } pre = node if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } } return root } 117. 填充每个节点的下一个右侧节点指针 II // 同上 // TODO：其他解法 func connect(root *Node) *Node { if root == nil { return root } queue := []*Node{root} for len(queue) \u0026gt; 0 { size := len(queue) var pre *Node = nil for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] if pre != nil { pre.Next = node } pre = node if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } } return root } 104. 二叉树的最大深度 func maxDepth(root *TreeNode) int { if root == nil { return 0 } depth := 0 queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } depth++ } return depth } 111. 二叉树的最小深度 func minDepth(root *TreeNode) int { if root == nil { return 0 } depth := 0 queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) depth++ for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] if node.Left == nil \u0026amp;\u0026amp; node.Right == nil { return depth } if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } } return depth } 常见算法题 226. 翻转二叉树 // 递归，左右节点互换即可 func invertTree(root *TreeNode) *TreeNode { if root != nil { root.Left, root.Right = invertTree(root.Right), invertTree(root.Left) } return root } // 迭代法，使用BFS即可 func invertTree(root *TreeNode) *TreeNode { if root == nil { return root } queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] node.Left, node.Right = node.Right, node.Left if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } } return root } 101. 对称二叉树 // 递归法 func isSymmetric(root *TreeNode) bool { return defs(root.Left, root.Right) } // 将左右节点作为参数分别传入 func defs(left *TreeNode, right *TreeNode) bool { if left == nil \u0026amp;\u0026amp; right == nil { return true; }; if left == nil || right == nil { return false; }; if left.Val != right.Val { return false; } return defs(left.Left, right.Right) \u0026amp;\u0026amp; defs(right.Left, left.Right); } // 迭代法：在加入队列时将相对应的一起加入，取出时也一起取出 func isSymmetric(root *TreeNode) bool { queue := []*TreeNode{root.Left, root.Right} for len(queue) \u0026gt; 0 { left := queue[0] right := queue[1] queue = queue[2:] if left == nil \u0026amp;\u0026amp; right == nil { continue } if left == nil || right == nil || left.Val != right.Val { return false } queue = append(queue, left.Left, right.Right, left.Right, right.Left) } return true } 559. N 叉树的最大深度 // BFS func maxDepth(root *Node) int { if root == nil { return 0 } depth := 0 queue := []*Node{root} for len(queue) \u0026gt; 0 { size := len(queue) for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] for _, child := range node.Children { queue = append(queue, child) } } depth++ } return depth } 222. 完全二叉树的节点个数 func countNodes(root *TreeNode) int { if root == nil { return 0 } count := 0 queue := []*TreeNode{root} for len(queue) \u0026gt; 0 { size := len(queue) for i := 0; i \u0026lt; size; i++ { node := queue[0] queue = queue[1:] count++ if node.Left != nil { queue = append(queue, node.Left) } if node.Right != nil { queue = append(queue, node.Right) } } } return count } 110. 平衡二叉树\n// 递归法 func isBalanced(root *TreeNode) bool { return getHeight(root) != -1 } func getHeight(root *TreeNode) int { if root == nil { return 0 } left := getHeight(root.Left) right := getHeight(root.Right) if left == -1 || right == -1 || math.Abs(float64(left-right)) \u0026gt; 1 { return -1 } return int(math.Max(float64(left), float64(right))) + 1 } 257. 二叉树的所有路径 // 递归法 func binaryTreePaths(root *TreeNode) []string { res := []string{} var travel func(root *TreeNode, s string) travel = func(root *TreeNode, s string) { if root.Left == nil \u0026amp;\u0026amp; root.Right == nil { res = append(res, s + strconv.Itoa(root.Val)) return } s += strconv.Itoa(root.Val) + \u0026#34;-\u0026gt;\u0026#34; if root.Left != nil { travel(root.Left, s) } if root.Right != nil { travel(root.Right, s) } } travel(root, \u0026#34;\u0026#34;) return res } 100. 相同的树 // 递归 func isSameTree(p *TreeNode, q *TreeNode) bool { if p == nil \u0026amp;\u0026amp; q == nil { return true } if p == nil || q == nil || p.Val != q.Val { return false } return isSameTree(p.Left, q.Left) \u0026amp;\u0026amp; isSameTree(p.Right, q.Right) } 404. 左叶子之和 func sumOfLeftLeaves(root *TreeNode) int { if root == nil { return 0 } sum := 0 if root.Left != nil \u0026amp;\u0026amp; root.Left.Left == nil \u0026amp;\u0026amp; root.Left.Right == nil { sum += root.Left.Val } sum += sumOfLeftLeaves(root.Left) + sumOfLeftLeaves(root.Right) return sum } 513. 找树左下角的值 // 递归 func findBottomLeftValue(root *TreeNode) int { maxLen := 1 maxValue := root.Val var travel func(root *TreeNode, length int) travel = func(root *TreeNode, length int) { if root.Left == nil \u0026amp;\u0026amp; root.Right == nil { if (length \u0026gt; maxLen) { maxLen = length maxValue = root.Val return } } if root.Left != nil { travel(root.Left, length+1) } if root.Right != nil { travel(root.Right, length+1) } } travel(root, 1) return maxValue } 112. 路径总和 func hasPathSum(root *TreeNode, targetSum int) bool { if root == nil { return false } if targetSum == root.Val \u0026amp;\u0026amp; root.Left == nil \u0026amp;\u0026amp; root.Right == nil { return true } return hasPathSum(root.Left, targetSum - root.Val) || hasPathSum(root.Right, targetSum - root.Val) } 113. 路径总和 II func pathSum(root *TreeNode, targetSum int) [][]int { res := [][]int{} var travel func(root *TreeNode, targetSum int, path []int) travel = func(root *TreeNode, targetSum int, path []int) { if root == nil { return } path = append(path, root.Val) if root.Left == nil \u0026amp;\u0026amp; root.Right == nil \u0026amp;\u0026amp; root.Val == targetSum { p := make([]int, len(path)) copy(p, path) // 注意此处，需要复制一个slice res = append(res, p) return } if root.Left != nil { travel(root.Left, targetSum - root.Val, path) } if root.Right != nil { travel(root.Right, targetSum - root.Val, path) } } travel(root, targetSum, []int{}) return res } 106. 从中序与后序遍历序列构造二叉树 func buildTree(inorder []int, postorder []int) *TreeNode { n := len(inorder) if n \u0026lt;= 0 { return nil } mid := postorder[n-1] index := 0 for i := 0; i \u0026lt; n; i++ { // 找到root节点在inorder中的下标 if inorder[i] == mid { index = i break } } root := \u0026amp;TreeNode { Val: mid, Left: buildTree(inorder[:index], postorder[:index]), Right: buildTree(inorder[index+1:], postorder[index:n-1]), } return root } 105. 从前序与中序遍历序列构造二叉树 func buildTree(preorder []int, inorder []int) *TreeNode { n := len(inorder) if n \u0026lt;= 0 { return nil } mid := preorder[0] index := 0 for i := 0; i \u0026lt; n; i++ { // 找到root节点在inorder中的下标 if inorder[i] == mid { index = i break } } root := \u0026amp;TreeNode { Val: mid, Left: buildTree(preorder[1:index+1], inorder[:index]), Right: buildTree(preorder[index+1:], inorder[index+1:]), } return root } 654. 最大二叉树 func constructMaximumBinaryTree(nums []int) *TreeNode { if len(nums) \u0026lt;= 0 { return nil } maxNum := nums[0] maxIndex := 0 for i, v := range nums { if v \u0026gt; maxNum { maxNum = v maxIndex = i } } return \u0026amp;TreeNode { Val: maxNum, Left: constructMaximumBinaryTree(nums[:maxIndex]), Right: constructMaximumBinaryTree(nums[maxIndex+1:]), } } 617. 合并二叉树 func mergeTrees(root1 *TreeNode, root2 *TreeNode) *TreeNode { if root1 == nil \u0026amp;\u0026amp; root2 == nil { return nil } root := \u0026amp;TreeNode {} if root1 == nil { root = root2 } else if root2 == nil { root = root1 } else { root = \u0026amp;TreeNode { Val: root1.Val+root2.Val, Left: mergeTrees(root1.Left, root2.Left), Right: mergeTrees(root1.Right, root2.Right), } } return root } 700. 二叉搜索树中的搜索 func searchBST(root *TreeNode, val int) *TreeNode { if root == nil { return nil } if root.Val == val { return root } node := searchBST(root.Left, val) if node != nil { return node } node = searchBST(root.Right, val) return node } 98. 验证二叉搜索树 func isValidBST(root *TreeNode) bool { var verify func(root *TreeNode, min, max int64) bool verify = func(root *TreeNode, min, max int64) bool { if root == nil { return true } if min \u0026gt;= int64(root.Val) || max \u0026lt;= int64(root.Val) { return false } // 分别对左子树和右子树递归判断，如果左子树和右子树都符合则返回true return verify(root.Right,int64(root.Val),max) \u0026amp;\u0026amp; verify(root.Left,min,int64(root.Val)) } return verify(root, math.MinInt64, math.MaxInt64) } 530. 二叉搜索树的最小绝对差 // 递归 func getMinimumDifference(root *TreeNode) int { var dfs func(node *TreeNode) min := 100000 pre := -1 dfs = func(node *TreeNode) { if node == nil { return } dfs(node.Left) if pre != -1 \u0026amp;\u0026amp; node.Val - pre \u0026lt; min { min = node.Val - pre } pre = node.Val dfs(node.Right) } dfs(root) return min } 501. 二叉搜索树中的众数 // 计数 func findMode(root *TreeNode) []int { var dfs func(node *TreeNode) res := []int{} maxCount := 0 count := 0 cur := 0 dfs = func(node *TreeNode) { if node == nil { return } dfs(node.Left) if node.Val == cur { count++ } else { count = 1 } cur = node.Val if count \u0026gt; maxCount { maxCount = count res = []int{} } if count == maxCount { res = append(res, cur) } dfs(node.Right) } dfs(root) return res } 236. 二叉树的最近公共祖先 func lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root == nil { return nil } if root == p || root == q { return root } left := lowestCommonAncestor(root.Left, p, q) right := lowestCommonAncestor(root.Right, p, q) if left != nil \u0026amp;\u0026amp; right != nil { return root } if left != nil { return left } if right != nil { return right } return nil } 235. 二叉搜索树的最近公共祖先\nfunc lowestCommonAncestor(root, p, q *TreeNode) *TreeNode { if root == nil { return nil } if root == q || root == p || (root.Val \u0026gt;= p.Val \u0026amp;\u0026amp; root.Val \u0026lt;= q.Val) || (root.Val \u0026lt;= p.Val \u0026amp;\u0026amp; root.Val \u0026gt;= q.Val) { return root } if root.Val \u0026gt; p.Val \u0026amp;\u0026amp; root.Val \u0026gt; q.Val { return lowestCommonAncestor(root.Left, p, q) } if root.Val \u0026lt; p.Val \u0026amp;\u0026amp; root.Val \u0026lt; q.Val { return lowestCommonAncestor(root.Right, p, q) } return nil } 701. 二叉搜索树中的插入操作 // 利用二叉搜索树的性质找到要插入的位置，注意root可能为nil func insertIntoBST(root *TreeNode, val int) *TreeNode { if root == nil { return \u0026amp;TreeNode{ Val: val, } } cur := root for cur != nil { if cur.Val \u0026gt; val { if cur.Left == nil { cur.Left = \u0026amp;TreeNode{ Val: val, } return root } cur = cur.Left } else { if cur.Right == nil { cur.Right = \u0026amp;TreeNode{ Val: val, } return root } cur = cur.Right } } return root } 450. 删除二叉搜索树中的节点 func deleteNode(root *TreeNode, key int) *TreeNode { if root == nil { return nil } if root.Val == key { if root.Left == nil \u0026amp;\u0026amp; root.Right == nil { return nil } else if root.Right == nil { return root.Left } else if root.Left == nil { return root.Right } else { cur := root.Right for cur.Left != nil { cur = cur.Left } cur.Left = root.Left return root.Right } } else if root.Val \u0026gt; key { root.Left = deleteNode(root.Left, key) } else { root.Right = deleteNode(root.Right, key) } return root } 669. 修剪二叉搜索树 func trimBST(root *TreeNode, low int, high int) *TreeNode { if root == nil { return nil } else if root.Val \u0026lt; low { return trimBST(root.Right, low, high) } else if root.Val \u0026gt; high { return trimBST(root.Left, low, high) } else { root.Left = trimBST(root.Left, low, high) root.Right = trimBST(root.Right, low, high) } return root } 108. 将有序数组转换为二叉搜索树 func sortedArrayToBST(nums []int) *TreeNode { n := len(nums) if n == 0 { return nil } mid := n/2 root := \u0026amp;TreeNode { Val: nums[mid], Left: sortedArrayToBST(nums[:mid]), Right: sortedArrayToBST(nums[mid+1:]), } return root } 538. 把二叉搜索树转换为累加树 func convertBST(root *TreeNode) *TreeNode { sum := 0 var dfs func(node *TreeNode) dfs = func(node *TreeNode) { if node == nil { return } dfs(node.Right) node.Val = sum + node.Val sum = node.Val dfs(node.Left) } dfs(root) return root } ","permalink":"https://oyzg.github.io/archives/%E4%BA%8C%E5%8F%89%E6%A0%91/","summary":"二叉树相关.","title":"二叉树"},{"content":"栈和队列 基础 栈：先进后出 队列：先进先出 栈和队列在不同语言不同集合中的实现方式是不同的，有数组实现的，有链表实现的，但是最基本的就是以上两个特征。\n实现方式 Java 在Java中，Stack类继承自Vector类，Queue则是一个接口，最常见的实现方式是LinkedList，此外还有Dueue双端队列，Priority优先队列\nStack主要方法： 1. boolean empty() // 判断栈是否为空 2. E peek() // 返回栈顶对象，不移除 3. E pop()\t// 返回栈顶对象，并移除 4. E push(E item)\t// 压入栈顶 5. int search(Object o) //返回对象在栈的位置 Queue主要方法： 1. boolean add(E e)\t// 向队列中添加元素 2. E element()\t// 返回队列的头，且不移除 3. boolean offer(E e)\t// 向队列中添加元素 4. E peek()\t// 返回队列的头，且不移除 5. E poll()\t// 返回队列的头，且移除 6. E remove()\t// 返回队列的头，且移除 Go 在Golang中栈和队列最常用的实现方式是slice，对于优先队列（堆），在go中有一个heap接口，可以用于实现优先队列，要实现heap接口，只需定义一个type，实现一下方法即可，具体使用方法可参考347.前 K 个高频元素\ntype Interface interface { sort.Interface Push(x interface{}) // 将x添加至元素Len()的位置 Pop() interface{} // 移除并且返回元素Len()-1 } // 其中sort.Interface为 type Interface interface { Len() int // Len是集合中的元素个数。 Less(i, j int) bool Swap(i, j int) // Swap交换索引i和索引j的元素 } 常见算法题 232.用栈实现队列 力扣题目链接\ntype MyQueue struct { s1 []int s2 []int } func Constructor() MyQueue { return MyQueue{ s1: make([]int, 0), s2: make([]int, 0), } } func (this *MyQueue) Push(x int) { this.s1 = append(this.s1, x) } func (this *MyQueue) Pop() int { for len(this.s1) \u0026gt; 0 { this.s2 = append(this.s2, this.s1[len(this.s1) - 1]) this.s1 = this.s1[:len(this.s1)-1] } res := this.s2[len(this.s2)-1] this.s2 = this.s2[:len(this.s2)-1] for len(this.s2) \u0026gt; 0 { this.s1 = append(this.s1, this.s2[len(this.s2)-1]) this.s2 = this.s2[:len(this.s2)-1] } return res } func (this *MyQueue) Peek() int { return this.s1[0] } func (this *MyQueue) Empty() bool { return len(this.s1) == 0 \u0026amp;\u0026amp; len(this.s2) == 0 } /** * Your MyQueue object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Peek(); * param_4 := obj.Empty(); */ 225. 用队列实现栈 力扣题目链接\ntype MyStack struct { q1 []int q2 []int } func Constructor() MyStack { return MyStack{ q1: make([]int, 0), q2: make([]int, 0), } } func (this *MyStack) Push(x int) { this.q1 = append(this.q1, x) } func (this *MyStack) Pop() int { for len(this.q1) \u0026gt; 0 { this.q2 = append(this.q2, this.q1[0]) this.q1 = this.q1[1:] } res := this.q2[len(this.q2)-1] this.q2 = this.q2[:len(this.q2)-1] for len(this.q2) \u0026gt; 0 { this.q1 = append(this.q1, this.q2[0]) this.q2 = this.q2[1:] } return res } func (this *MyStack) Top() int { return this.q1[len(this.q1)-1] } func (this *MyStack) Empty() bool { return len(this.q1) == 0 } /** * Your MyStack object will be instantiated and called as such: * obj := Constructor(); * obj.Push(x); * param_2 := obj.Pop(); * param_3 := obj.Top(); * param_4 := obj.Empty(); */ 20. 有效的括号 力扣题目链接\nfunc isValid(s string) bool { stack := make([]rune, 0) for _, v := range s { if len(stack) == 0 || v == \u0026#39;(\u0026#39; || v == \u0026#39;[\u0026#39; || v == \u0026#39;{\u0026#39; { stack = append(stack, v) } else { if (v == \u0026#39;)\u0026#39; \u0026amp;\u0026amp; stack[len(stack)-1] == \u0026#39;(\u0026#39;) || (v == \u0026#39;]\u0026#39; \u0026amp;\u0026amp; stack[len(stack)-1] == \u0026#39;[\u0026#39;) || (v == \u0026#39;}\u0026#39; \u0026amp;\u0026amp; stack[len(stack)-1] == \u0026#39;{\u0026#39;) { stack = stack[:len(stack)-1] } else { stack = append(stack, v) } } } return len(stack) == 0 } 1047. 删除字符串中的所有相邻重复项 力扣题目链接\nfunc removeDuplicates(s string) string { stack := make([]rune, 0) for _, v := range s { if len(stack) != 0 \u0026amp;\u0026amp; v == stack[len(stack)-1] { stack = stack[0:len(stack)-1] } else { stack = append(stack, v) } } var build strings.Builder for len(stack) != 0 { build.WriteString(string(stack[0])) stack = stack[1:] } return build.String() } 150. 逆波兰表达式求值 力扣题目链接\nfunc evalRPN(tokens []string) int { stack := []int{} for _, token := range tokens { val, err := strconv.Atoi(token) if err == nil { stack = append(stack, val) } else { num1, num2 := stack[len(stack)-2], stack[(len(stack))-1] stack = stack[:len(stack)-2] switch token { case \u0026#34;+\u0026#34;: stack = append(stack, num1+num2) case \u0026#34;-\u0026#34;: stack = append(stack, num1-num2) case \u0026#34;*\u0026#34;: stack = append(stack, num1*num2) case \u0026#34;/\u0026#34;: stack = append(stack, num1/num2) } } } return stack[0] } 239. 滑动窗口最大值 力扣题目链接\n// 基本思路： // 1.使用单调队列（递减），push时将小的移除再push func maxSlidingWindow(nums []int, k int) []int { queue := make([]int, 0) // 单调队列 for i := 0; i \u0026lt; k; i++ { //先将前k个放入队列 for len(queue) \u0026gt; 0 \u0026amp;\u0026amp; queue[len(queue)-1] \u0026lt; nums[i] { // 如果queue最后一个比放入的值小，就把先把最后一个移除，之后再放入，这样来保持队列单调递减 queue = queue[:len(queue)-1] } queue = append(queue, nums[i]) } res := []int{queue[0]} for i := k; i \u0026lt; len(nums); i++ { if len(queue) \u0026gt; 0 \u0026amp;\u0026amp; queue[0] == nums[i-k] { // 如果queue第一个等于目前窗口第一个，则移除 queue = queue[1:] } for len(queue) \u0026gt; 0 \u0026amp;\u0026amp; queue[len(queue)-1] \u0026lt; nums[i] { queue = queue[:len(queue)-1] } queue = append(queue, nums[i]) res = append(res, queue[0]) } return res } 347.前 K 个高频元素 力扣题目链接\nfunc topKFrequent(nums []int, k int) []int { map_num := map[int]int{} for _, v := range nums { map_num[v]++ } h := \u0026amp;IHeap{} heap.Init(h) for key, value := range map_num { heap.Push(h, [2]int{key, value}) if h.Len() \u0026gt; k { heap.Pop(h) } } res := make([]int, k) for i := 0; i \u0026lt; k; i++ { res[k-i-1] = heap.Pop(h).([2]int)[0] } return res } type IHeap [][2]int func (h IHeap) Len() int { return len(h) } func (h IHeap) Less(i, j int) bool { return h[i][1] \u0026lt; h[j][1] } func (h IHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h *IHeap) Push(x interface{}) { *h = append(*h, x.([2]int)) } func (h *IHeap) Pop() interface{} { old := *h res := old[len(old)-1] *h = old[:len(old)-1] return res } ","permalink":"https://oyzg.github.io/archives/%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97/","summary":"数组、二分查找、双指针相关.","title":"栈和队列"},{"content":"哈希表（散列表） 基础 散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构\n通过哈希函数来将key和value映射起来\n哈希函数 将key转化成hashcode，然后对hash table的大小进行取模就可以得到key在hash table中的下标了\n既然是取模来得到下标，就可能会存在冲突的可能，这个就叫hash冲突（hash碰撞）\nHash碰撞 一般hash碰撞有两种方法：拉链法和线性探测法\n拉链法 拉链法即将哈希冲突的值存储在一个链表中\n但是如果链表过长， 就会影响哈希表的性能，所以哈希表需要选择合适的大小\n线性探测法 见名知义，即如果发生hash冲突，就从该点往后寻找空闲的位置\n各语言中常用哈希表\nC++：\n映射 底层实现 是否有序 数值是否可以重复 能否更改数值 查询效率 增删效率 std::map 红黑树 key有序 key不可重复 key不可修改 O(logn) O(logn) std::multimap 红黑树 key有序 key可重复 key不可修改 O(log n) O(log n) std::unordered_map 哈希表 key无序 key不可重复 key不可修改 O(1) O(1)集合 集合 底层实现 是否有序 数值是否可以重复 能否更改数值 查询效率 增删效率 std::set 红黑树 有序 否 否 O(log n) O(log n) std::multiset 红黑树 有序 是 否 O(logn) O(logn) std::unordered_set 哈希表 无序 否 否 O(1) O(1) Java：\n底层 有序否 键值对能否为Null 遍历 线程安全 哈希Code Hashmap 数组+链表 无序 都可null iterator 不安全 内部hash方法 Hashtable 数组+链表 无序 都不可null Enumeration（iterator） 安全 Key自己的 TreeMap 红黑树 有序 仅value能null iterator 不安全 / Go：\nMap\n常见算法题 哈希表相关的算法题会稍微简单一点，需要注意的点就是，因为哈希表是用空间换取了时间，所以哈希表的空间消耗会比较大，对于一些情况，比如key为26个字母，我们可以用数组来代替哈希表，key换成(int)key-‘a’即可\n242.有效的字母异位词 力扣题目链接\n方法：\n暴力法（两层for循环） 哈希表 用数组代替哈希表 func isAnagram(s string, t string) bool { if len(s) != len(t) { return false } record := [26]int{} // m := make(map[rune]int) for _, v := range s { record[v-rune(\u0026#39;a\u0026#39;)]++ //m[v]++ } for _, v := range t { record[v-rune(\u0026#39;a\u0026#39;)]-- // m[v]--; // if m[v] \u0026lt; 0 { // return false // } } return record == [26]int{} // return true } 349. 两个数组的交集 力扣题目链接\nfunc intersection(nums1 []int, nums2 []int) []int { m := make(map[int]int) for _, v := range nums1 { m[v] = 1 } res := make([]int, 0) for _, v := range nums2 { if _, ok := m[v]; ok \u0026amp;\u0026amp; m[v] \u0026gt; 0 { res = append(res, v) m[v]-- } } return res } 第202题. 快乐数 力扣题目链接\nfunc isHappy(n int) bool { m := make(map[int]int) m[n] = 1 for n != 1 { tmp := 0 for n \u0026gt; 0 { a := n%10 tmp += a*a n /= 10 } n = tmp if _, ok := m[n]; ok { return false } m[n] = 1 } return true } 1. 两数之和 力扣题目链接\nfunc twoSum(nums []int, target int) []int { m := make(map[int]int) for i, v := range nums { if _, ok := m[target-v]; ok { return []int{i, m[target-v]} } m[v] = i } return nil } 第454题.四数相加II 力扣题目链接\nfunc fourSumCount(nums1 []int, nums2 []int, nums3 []int, nums4 []int) int { m := make(map[int]int) for _, v1 := range nums1 { for _, v2 := range nums2 { m[v1+v2]++ } } res := 0 for _, v1 := range nums3 { for _, v2 := range nums4 { if _, ok := m[0-v1-v2]; ok { res += m[0-v1-v2] } } } return res } 383. 赎金信 力扣题目链接\nfunc canConstruct(ransomNote string, magazine string) bool { m := make(map[rune]int) for _, v := range magazine { m[v]++ } for _, v:= range ransomNote { if _, ok := m[v]; ok \u0026amp;\u0026amp; m[v] \u0026gt; 0 { m[v]-- } else { return false } } return true } ","permalink":"https://oyzg.github.io/archives/%E5%93%88%E5%B8%8C%E8%A1%A8/","summary":"哈希表相关.","title":"哈希表"},{"content":"链表 基础 链表在内存上分散，通过指针的方式连接 链表读O(n),写O(1) 链表的类型 单链表 双链表 循环链表 结构 // 单链表 class ListNode { int val; // 节点上存储的元素 ListNode next; // 指向下一个节点 ListNode(int x) { this.val = x; this.next = null; } // 节点的构造函数 }; // 双链表 class ListNode { int val; // 节点上存储的元素 ListNode pre; // 指向上一个节点 ListNode next; // 指向下一个节点 ListNode(int x) { this.val = x; this.pre = null; this.next = null; } // 节点的构造函数 }; 常考算法题 203.移除链表元素 基本思路：\n​\t因为链表题很多时候需要考虑头结点，所以很多情况都需要设置一个虚拟头节点pre(previous)\n​\t对链表进行遍历，对每一个pre.Next等于val的情况，将pre.Next删除，即使pre.Next指向pre.Next.Next\npublic ListNode removeElements(ListNode head, int val) { ListNode cur = new ListNode(); ListNode res = cur; cur.next = head; while (cur != null \u0026amp;\u0026amp; cur.next != null) { if (cur.next.val == val) { cur.next = cur.next.next; } else { cur = cur.next; } } return res.next; } func removeElements(head *ListNode, val int) *ListNode { pre := \u0026amp;ListNode{Next: head} for tmp := pre; tmp.Next != nil; { if tmp.Next.Val == val { tmp.Next = tmp.Next.Next } else { tmp = tmp.Next } } return pre.Next } 707.设计链表 力扣题目链接\n单链表或双链表\n206.反转链表 力扣题目链接\n// 基本思路： // 设置两个变量pre和cur,每次将pre -\u0026gt; cur 变为 pre \u0026lt;- cur // 并提前设置一个临时变量tmp来表示原来的cur.Next // 反转之后，将pre设为cur，将cur设为tmp // 反转完成后需要将原来的head.Next设为nil，否则前2个节点会形成循环链表 func reverseList(head *ListNode) *ListNode { if head == nil { // 判断头结点是否为空 return head } cur := head.Next pre := head for cur != nil \u0026amp;\u0026amp; pre != nil { // 遍历链表 tmp := cur.Next // 记录cur.Next cur.Next = pre // 反转 pre = cur cur = tmp } head.Next = nil // 将原来的head（现在的tail）的Next设为nil return pre } 24. 两两交换链表中的节点 力扣题目链接\n// 基本思路： // 1.判断头结点是否为空 // 2.设置一个虚拟头结点dummpHead // 3.遍历链表，对每2个节点进行反转 // 3.1 设置两个节点pre，cur为需要进行反转的2个节点 // 3.2 将pre下一节点指向cur.Next // 3.3 tmp -\u0026gt; cur //\t3.4 pre \u0026lt;- cur // 4.最后返回虚拟头节点的下一节点即可 func swapPairs(head *ListNode) *ListNode { if head == nil || head.Next == nil{ // 判空 return head } dummpHead := \u0026amp;ListNode{ // 设置虚拟头节点 Val: -1, Next: head, } tmp := dummpHead for tmp.Next != nil \u0026amp;\u0026amp; tmp.Next.Next != nil{ pre := tmp.Next cur := pre.Next pre.Next = cur.Next //以下三步进行反转 tmp.Next = cur cur.Next = pre tmp = pre } return dummpHead.Next } 19.删除链表的倒数第N个节点 力扣题目链接\n// 基本思路： // 1.因为可能删除的是头节点，所以设置一个虚拟头节点 // 2.找到第n个节点 // 3.两个指针(pre, end)同时往后移，直到end为空 // 4.此时pre为要删除节点的前一节点，将pre.Next指向下下节点即可 // 5.最后返回虚拟头节点的下一节点即可 func removeNthFromEnd(head *ListNode, n int) *ListNode { end := head dummpHead := \u0026amp;ListNode{ // 设置一个虚拟头节点 Val: -1, Next: head, } pre := dummpHead for i := 0; i \u0026lt; n \u0026amp;\u0026amp; end != nil; i++ { // 将end移至第n个节点 end = end.Next } for end != nil { // 将pre和end同时后移 end = end.Next pre = pre.Next } pre.Next = pre.Next.Next // 删除指定节点 return dummpHead.Next } 面试题 02.07. 链表相交 力扣题目链接\n哈希表法：\n// 基本思路： // 1.思路很简单，用map来记录已经遍历过的节点 // 2.如果该节点存在于map中说明这个节点即为公共节点 // 3.先遍历一条链表再遍历另一条链表 // 4.如果没有公共节点，返回nil即可 func getIntersectionNode(headA, headB *ListNode) *ListNode { m := make(map[*ListNode]bool) for headA != nil { // 遍历链表A m[headA] = true headA = headA.Next } for headB != nil { // 遍历链表B if m[headB] { // 如果有公共节点，返回即可 return headB } m[headB] = true headB = headB.Next } return nil // 如果没有返回nil即可 } 双指针法：\n// 基本思路： // 1.假设链表A长度为a，链表B长度为b，重合部分为c // 2.设置两个节点遍历两个链表，遍历完后遍历另一条链表 // 3.当遍历到a+b-c时，即遇到第一个重合节点，最后返回该节点即可 // 4.如果没有重合部分，那么最后都遍历了a+b，最后都为nil，返回nil即可 func getIntersectionNode(headA, headB *ListNode) *ListNode { curA := headA curB := headB for curA != curB \u0026amp;\u0026amp; (curA != nil || curB != nil) { // 分别遍历 if curA == nil { curA = headB } else { curA = curA.Next } if curB == nil { curB = headA } else { curB = curB.Next } } if curA != nil { return curA } return nil } 142.环形链表II 力扣题目链接\n// 基本思路：（快慢指针） // 1.设置一个快指针，每次走2步，一个慢指针，每次走1步 // 2.判断有没有环： // 2.1 如果相遇即有环 // *为什么有环一定相遇？ // fast相对于slow每次多走一步，在进入环后一定会相遇 // 2.2 如果最后fast为nil，则没环 // 3.相遇后如何找到环入口： // 3.1 设起点到入口距离x，入口到相遇点距离y，相遇点到入口距离z（环的另一遍） // 3.2 slow走的距离为x+y，fast走的距离为x+y+n*(y+z)，fast走的距离是slow的2倍 // 3.3 所以2(x+y) = x+y+n*(y+z) ==\u0026gt; x+y = n*(y+z) ==\u0026gt; x = (n-1)(y+z) + z // 3.4 所以n \u0026gt;= 1 // 3.4.1 n为1时，fast走了一圈，化简得x = z //\t3.4.2 n \u0026gt; 1时，也是一样，只不过要多走n-1圈 // 3.5 所以只需slow和head同时移动，便会在入口相遇 func detectCycle(head *ListNode) *ListNode { fast := head slow := head for fast != nil \u0026amp;\u0026amp; fast.Next != nil { fast = fast.Next.Next slow = slow.Next if fast == slow { for slow != head { slow = slow.Next head = head.Next } return head } } return nil } ","permalink":"https://oyzg.github.io/archives/%E9%93%BE%E8%A1%A8/","summary":"链表相关算法,常考算法题.","title":"链表"},{"content":"数组 数组是最基础的一种数据结构，定义：数组是存放在连续内存空间上的相同类型数据的集合\n特点 数组在内存上是连续的 数组下标从0开始 数组支持随机访问，即在相同时间内访问任意元素 读操作和添加操作O(1)，插入删除操作O(n) 各语言版本 Type[] array;//定义 array = new Type[size];//初始化，必须指定长度 array = new Type[]{x1, x2, x3};//初始化 array[i]//读取或修改指定下标的值 Type array [size];//声明 Type array [] = {x1, x2, x3};//初始化 array[i]//读取或修改指定下标的值 var variable_name [SIZE] variable_type;//声明 var balance = [5]float32{1000.0, 2.0, 3.4, 7.0, 50.0}//初始化 var salary float32 = balance[9]//访问元素 动态数组 在平时的使用中，数组的长度不可变造成了很多不方便，所以很多语言中有类似动态数组的数据结构存在\nJava：ArrayList C++：Vertor Go：slice（切片） 经典算法题 [二分查找]:https://leetcode-cn.com/problems/binary-search/ 二分查找有两种写法：\n左闭右闭即[left, right] 左闭右开即[left, right) 第一种：左闭右闭\nwhile (left \u0026lt;= right) 要使用 \u0026lt;= ，因为left == right是有意义的，所以使用 \u0026lt;= if (nums[middle] \u0026gt; target) right 要赋值为 middle - 1，因为当前这个nums[middle]一定不是target，那么接下来要查找的左区间结束下标位置就是 middle - 1 class Solution { public int search(int[] nums, int target) { int left = 0; int right = nums.length-1; while (left \u0026lt;= right) { int mid = left + ((right - left) / 2); if (nums[mid] \u0026gt; target) { right = mid-1; } else if (nums[mid] \u0026lt; target){ left = mid+1; } else { return mid; } } return -1; } } 第二种：左闭右开\nwhile (left \u0026lt; right)，这里使用 \u0026lt; ,因为left == right在区间[left, right)是没有意义的 if (nums[middle] \u0026gt; target) right 更新为 middle，因为当前nums[middle]不等于target，去左区间继续寻找，而寻找区间是左闭右开区间，所以right更新为middle，即：下一个查询区间不会去比较nums[middle] class Solution { public int search(int[] nums, int target) { int left = 0, right = nums.length; while (left \u0026lt; right) { int mid = left + ((right - left) \u0026gt;\u0026gt; 1); if (nums[mid] == target) return mid; else if (nums[mid] \u0026lt; target) left = mid + 1; else if (nums[mid] \u0026gt; target) right = mid; } return -1; } } 左右边界版本 左闭右开：\nint left_bound(int[] nums, int target) { if (nums.length == 0) return -1; int left = 0; int right = nums.length; // 注意 while (left \u0026lt; right) { // 注意 int mid = (left + right) / 2; //if (nums[mid] == target) { // right = mid; //} else if (nums[mid] \u0026lt; target) { // left = mid + 1; //} else if (nums[mid] \u0026gt; target) { // right = mid; // 注意 //} //因为要找目标数的左边界 对于相等的情况 也将right=mid //由于相对简洁，普通的二分查找也可以写成这种版本 if (nums[mid] \u0026gt;= target) { right = mid; } else { left = mid+1; } } // 最后要检查 left 越界的情况 if (left \u0026gt;= nums.length || nums[left] != target) return -1; return left; } 左闭右闭：\nclass Solution { public int search(int[] nums, int target) { int n = nums.length; int left = 0, right = n - 1; while (left \u0026lt; right) { int mid = left + right + 1 \u0026gt;\u0026gt; 1; if (nums[mid] \u0026gt;= target) right = mid; else left = mid + 1; } return nums[right] == target ? right : -1; } } 练习题 双指针 双指针 排序 滑动窗口 模拟 小结 数组在内存中连续，读快写慢 常用解题方法： 二分查找 双指针法 滑动窗口 模拟行为 ","permalink":"https://oyzg.github.io/archives/%E6%95%B0%E7%BB%84/","summary":"数组、二分查找、双指针相关.","title":"数组"},{"content":"算法性能分析 时间复杂度 概念 时间复杂度：是一个与算法规模n相关的函数，用来描述算法的运行时间与规模之间的关系 大O：通常来讲，大O用来表示一般情况的时间复杂度，在《算法导论》中为上界 常见时间复杂度 O(n) O(logn) O(n^2) O(nlogn) 计算时间复杂度时，我们往往是忽略常数的，包括n前的常数，以及logn的底数\n递归的时间复杂度 递归算法的时间复杂度本质上是要看: 递归的次数 * 每次递归的时间复杂度\n示例1：\nint function2(int x, int n) { if (n == 0) { return 1; // return 1 同样是因为0次方是等于1的 } return function2(x, n - 1) * x; } 在这个例子中，每次递归都是-1，很容易看出来复杂度为O(n)\n示例2：\nint function3(int x, int n) { if (n == 0) { return 1; } if (n % 2 == 1) { return function3(x, n / 2) * function3(x, n / 2)*x; } return function3(x, n / 2) * function3(x, n / 2); } 在这个例子中，节点数为2^3+2^2+2^1+2^0=15，根据推导可以得出=n-1，所以复杂度为O(n)\n示例3：\nint function4(int x, int n) { if (n == 0) { return 1; } int t = function4(x, n / 2);// 这里相对于function3，是把这个递归操作抽取出来 if (n % 2 == 1) { return t * t * x; } return t * t; } 在该例子中，每次调用都是n/2，所以复杂度为O(logn)\n方法 分析递归算法的时间复杂度的关键点就在于每次递归时 参数的变化\n对于一分为一的递归，我们可以根据参数变化转换为循环 如每次调用为f(n/2)，可以看成for(int i = x; i \u0026lt; l; i*=2) 如每次调用为f(n-x)，可以看成for(int i = x; i \u0026lt; l; i += x) 对于一分为多的递归，我们可以将递归想象成一个多叉树进行分析 空间复杂度 空间复杂度和时间复杂度大同小异\n在一些简单的算法中，看程序中定义了多少数组、map这些即可\n在递归算法中，递归算法的空间复杂度 = 每次递归的空间复杂度 * 递归深度\n总结 递归算法 时间复杂度：每次递归的时间复杂度 * 递归的次数 空间复杂度：每次递归的空间复杂度 * 递归深度 ","permalink":"https://oyzg.github.io/archives/%E7%AE%97%E6%B3%95%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","summary":"算法时间复杂度、空间复杂度分析.","title":"算法性能分析"},{"content":"MySQL 存储引擎 MySQL 常用存储引擎为 MyISAM 和 InnoDB 两种 在 MySQL 5.5.5 版本开始默认存储引擎从 MyISAM 改为 InnoDB\n存储引擎相关命令\n查看提供的所有存储引擎：show engines 查看默认存储引擎：show variables like '%storage_engine%'; 查看表的存储引擎：show table status like \u0026quot;table_name\u0026quot; ; MyISAM与InnoDB的区别\n锁级别：MyISAM支持表级锁，不支持行级锁，InnoDB都支持 事务：MyISAM不支持事务，InnoDB支持事务，可提交和回滚 外键：MyISAM不支持外键，InnoDB支持外键 安全恢复：InnoDB可使用事务日志进行自动恢复，MyISAM不支持 性能：InnoDB具有更高的写入速度，处理大量数据性能更高，MyISAM读取速度更快 缓存和索引：InnoDB支持缓存数据和索引的大型缓冲区池，不支持全文搜索，MyISAM密钥缓冲区仅用于索引，支持全文搜索 事务 在逻辑上为一个整体的多个操作，即要么都执行，要么都不执行\n四大特性ACID 原子性（Atomicity） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 实现原理 MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。 MySQL InnoDB 引擎通过 锁机制、MVCC 等手段来保证事务的隔离性（ 默认支持的隔离级别是 REPEATABLE-READ ）。 保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障 并发事务带来的问题 丢失修改：两个事务对同一个数据进行修改，后一个覆盖了前一个 脏读：读到另一个事务没有提交的数据 不可重复读：在一个事务中，对一组数据多次读取，得到的结果不一致 幻读：在一个事务中读取数据，读取的数据条数不一致 隔离级别 读未提交：允许读取其他事务未提交的数据，存在脏读、不可重复读、幻读问题 读已提交：只允许读其他事务已提交的数据，存在不可重复读、幻读问题 可重复读：对同一数据多次读取结果一致，存在幻读问题，InnoDB默认隔离级别 可串行化：所有事务逐个执行，可解决以上所有问题 索引 索引是一种用于快速查询和检索数据的数据结构\nMySQL索引使用的数据结构为B+树\nB+树的优点 与Hash表相比\nHash存在Hash冲突问题 Hash不支持顺序查找和范围查找 与B树相比\nB+树的key和data都存放在叶子节点 B+树的叶子节点有一条引用链指向与它相邻的叶子节点 B+树的检索效率更稳定 与红黑树相比：\n有更低的层数，因此磁盘访问次数更少 索引类型 聚簇索引和非聚簇索引 聚簇索引：存储记录是物理上连续存在，物理存储按照索引排序，有利于范围查询，一张表中只能有一个聚簇索引。 非聚簇索引：非聚集索引是逻辑上的连续，物理存储并不连续，物理存储不按照索引排序。 覆盖索引 一个索引如果包含需要查询的字段，就是覆盖索引\n比如name字段有索引，select name from student where name = ‘xxxx’\n创建索引注意点 选择合适的字段 不为NULL的 频繁查询的 频繁需要排序的 频繁用于连接的 频繁作为条件查询的 频繁更新的需谨慎 尽可能建立联合索引 字符串字段使用前缀索引，占用空间更新 避免冗余索引 日志 redo log 是InnoDB所独有的，让MySQL拥有了崩溃恢复能力\nundo log bin log ","permalink":"https://oyzg.github.io/archives/mysql/","summary":"MySQL面试相关内容.","title":"MySQL"},{"content":"Linux Linux常用命令\n文件目录操作 ls：查看目录下的文件 cd：进入指定目录 pwd：查看当前目录 mkdir：创建目录 rm：删除文件或目录，-rf删除目录 rmdir：删除目录 mv：移动文件，或改名 cp：复制文件 touch：查看文件或目录的日期时间 cat：显示文件内容，-n显示行号 nl：输出文件内容自动加上行号 more：显示文件内容，与cat区别为按页显示 less：显示文件内容，与more区别为more仅能向前移动，不能向后移动，且less查看之前不会加载整个文件 head：显示文件开头 tail：显示指定文件末尾内容 文件查找 which：搜索某个系统命令位置 whereis：定位可执行文件、源代码文件、帮助文件在文件系统中的位置 locate：快速的搜索系统内是否有指定的文件 find：沿着文件层次结构向下遍历，匹配符合条件的文件，并执行相应操作 文件打包上传和下载 tar：压缩解压文件，-zcvf压缩，-zxvf解压 -z：支持gzip解压文件 -c：建立新的压缩文件 -x：从压缩文件中提取文件 -v：显示操作过程 -f：压缩指定文件 gzip：压缩文件，为.gz文件 文件权限 chmod：改变文件访问权限 chgrp：改变文件所属群组 chown：改变文件的所有者和群组 磁盘存储 df：显示指定磁盘文件的可用空间 du：显示每个文件和目录的磁盘使用空间 性能监控和优化 top：显示系统当前正在执行的进程的相关信息，包括进程id，内存使用率，cpu占用率等 free：显示系统使用和空闲的内存情况，包括物理内存、交互区内存（swap）和内核缓存区内存 vmstat：用来显示虚拟内存信息 iostat：查看cpu、网卡、磁盘等设备的活动情况、负载信息 lsof：查看某个文件相关的进程 网络命令 ifconfig：查看配置网络 route：用于操作基于内核ip路由表 ping：确定主机与外部主机的状态 netstat：检验各端口网络连接情况 telnet：开启终端机阶段作业，并登入远端主机 其他命令 grep：文本搜索工具 ps：显示当前进程的状态 ","permalink":"https://oyzg.github.io/archives/linux/","summary":"Linux面试相关内容.","title":"Linux"},{"content":"操作系统理论知识 内存管理 内存管理主要负责内存的分配与回收（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做的事情。\n虚拟内存 操作系统提供的一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来\n程序中使用到的内存地址为虚拟内存地址 实际中硬件中的空间地址为物理内存地址 为什么要有虚拟内存？\n虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间），这样会更加有效地管理内存并减少出错。 虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间 局部性原理\n时间局部性 ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。 虚拟内存（虚拟存储器）的技术实现？\n请求分页存储管理 ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。 请求分段存储管理 ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。 请求段页式存储管理 请求分页与分页存储管理，两者有何不同呢？\n请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因\n它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。\n内存分段 程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。\n内存分段的不足之处：\n内存碎片 外部碎片：产生了多个不连续的小物理内存，可用内存交换（Swap）解决 内部碎片：程序所有内存都被装载到了物理内存，但一部分内存可能不常用造成了浪费 内存交换效率低：内存交换造成的，因为Swap需要写入硬盘，硬盘访问速度慢 内存分页 分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。在 Linux 下，每一页的大小为 4KB。\n不足：\n页表会非常大，使用多级页表解决 多级页表 如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表\n段页式内存管理 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制； 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页； 段页式地址变换中要得到物理地址须经过三次内存访问：\n第一次访问段表，得到页表起始地址； 第二次访问页表，得到物理页号； 第三次将物理页号与页内位移组合，得到物理地址。 页面置换算法 最佳页面置换算法 先进先出置换算法 最近最久未使用的页面置换算法 时钟页面置换算法 最不常用页面置换算法 磁盘调度算法 先来先服务算法 最短寻道时间优先算法 扫描算法 循环扫描算法 LOOK与C-LOOK算法 进程管理 线程 运行中的程序叫线程\n进程的状态？\n创建态：正在被创建的状态 就绪态：可运行，但是其他进程在运行 运行态：占用CPU在运行 阻塞态：不满足可运行的条件，在等待某一事件发生 终止态：正在从系统中消失的状态 在七状态模型中还有两种状态：\n阻塞挂起状态：进程在外存且等待某一事件发生 就绪挂起状态：进程在外存，只要进入内存就可以立刻运行 进程创建的过程？\n为新进程分配分配唯一的进程标识符，申请一个PCB 为进程分配资源 初始化PCB 如果进程的调度队列能够接纳，就插入到就绪队列 进程控制块PCB PCB是进程的唯一标识，包含以下信息：\n进程描述信息：进程标识符、用户标识符 进程控制和管理信息：进程状态、进程优先级 资源分配清单 CPU相关信息 PCB是如何组织的？\nPCB是通过链表的方式进行组织的，把具有相同状态的进程链接在一起，组成各种队列\n进程间通信方式 管道：输出数据是单向的，Linux中的| 命名管道FIFO：可在不相关的进程间进行相互通信 管道的通信方式效率低，不适合进程间频繁的交换数据 消息队列：保存在内核中的消息链表，不适合大数据的传输，存在用户态和内核态之间的数据拷贝开销 共享内存：拿出一块虚拟地址空间来，映射到相同的物理内存中 信号量：一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据 P操作减一，V操作加一 信号：进程间通信机制唯一的异步通信机制 Socket 线程 线程是进程当中的一条执行流程\n线程的优缺点？\n优点：\n一个进程可以同时存在多个进程 各个线程之间可以并发执行； 各个线程之间可以共享地址空间和文件等资源； 缺点：\n当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线程奔溃不会造成进程崩溃） 进程线程对比 进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位； 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈； 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系； 线程能减少并发执行的时间和空间开销； 调度算法 先来先服务算法 最短作业优先调度算法 高响应比优先调度算法（响应比=（等待时间+服务时间）/服务时间） 时间片轮转调度算法 最高优先级调度算法 多级反馈队列调度算法 ","permalink":"https://oyzg.github.io/archives/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","summary":"操作系统面试相关内容.","title":"操作系统"},{"content":"计算机网络分层 目前主要使用的网络模型为五层网络模型\n五层网络模型 应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等协议。数据单位为报文。 传输层 ：为进程提供通用数据传输服务。运输层包括两种协议：TCP， UDP。 网络层 ：为主机提供通信服务，路由选择。网络层有四个协议：ARP协议，IP协议，ICMP协议，IGMP协议 数据链路层 ：为同一链路的主机提供数据传输服务。将分组封装成帧。协议包括：Ethernet，PPP，[CSMA/CD](https://github.com/CyC2018/CS-Notes/blob/master/notes/计算机网络 - 链路层.md#csmacd-协议) 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 七层网络模型 四层网络模型 应用层 传输层 网络层 网络接口层 应用层协议 HTTP（超文本传输协议） 首部字段 Host：请求的服务器域名 Content-Length：响应数据长度 Connection ：Keep-Alive为长连接 Content-Type：响应体数据格式 Content-Encoding：响应体数据压缩方式 特点 优点：\n简单 灵活易于扩展 应用广泛，跨平台 缺点：\n无状态双刃剑 明文传输双刃剑 不安全（明文可能被窃听，不验证身份可能被伪装，无法证明报文完整性可能被篡改） 状态码 1xx:提示信息 2xx：成功 200：OK，表示一切正常 204：No content，与200的区别在于响应头没有body数据 206：Partial Content，表示body为资源的一部分（应用于HTTP分块下载或断点续传） 3xx：重定向，301和302含字段Location表示跳转URL 301：永久重定向，请求资源已不存在 302：临时重定向，请求资源还存在 304：缓存重定向，资源未修改，重定向到已存在的缓存文件 4xx：客户端错误 400：请求报文错误，笼统的错误 403：服务器禁止访问资源 404：请求找不到资源 5xx：服务端错误 500：笼统的错误，并不知道发生了什么错误 501：客户端请求功能还不支持 502：服务器作为网关或代理，服务器自身正常，访问后端服务器发生了错误 503：服务器繁忙 Get与Post的区别 Get用于请求资源，Post用于对资源做修改处理 Get是幂等和安全的，Post不是幂等和安全的（幂等指多次执行结果相同） Get请求参数位于URL中，只支持ASCLL，长度有限制，Post请求数据在body中，无限制 HTTP1.1 提出了长连接的通信方式，减少了TCP重复连接断开所带来的开销，减轻了服务器端的负载 管道网络传输：不需要等待前一个请求响应再发送，减少了整体的响应时间 队头阻塞：服务器端仍然是按照顺序响应，如果发生阻塞，会导致所有请求阻塞 HTTP2 头部压缩：使用HPACK算法进行压缩 静态表编码 动态表编码 二进制帧 并发传输：多条stream复用一条TCP连接 服务器主动推送资源 HTTP3 基于UDP协议实现了QUIC协议，有以下优点 无队头阻塞 更快地连接建立 连接迁移 HTTPS 与HTTP的区别 在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，更加安全 在TCP三次握手基础上还需进行SSL/TLS 的握手 HTTP 的端口号是 80，HTTPS 的端口号是 443 HTTPS需向CA申请数字证书 如何解决HTTP安全问题 信息加密（混合加密）实现机密性，解决窃听问题 效验机制（摘要算法）实现完整性，解决篡改问题 数字证书解决冒充问题 HTTPS通信过程 首先是服务器将自己的公钥注册到CA，CA用自己的私钥对服务器的公钥签名并颁发数字证书 服务器将数字证书发送给浏览器（事先置入了CA的公钥），浏览器对数字证书进行解密得到服务器的公钥 浏览器用服务器的公钥对报文进行加密发送 服务器用自己的私钥对报文进行解密 SSL/TLS握手 客户端发送clientHello请求，包含版本，随机数（用于生产会话密钥），密码套件列表等信息 服务端响应serverHello，包含确认的版本和密码套件列表，随机数（用户生产会话密钥），数字证书 客户端回应一个加密的随机数，加密通信算法改变通知和客户端握手结束通知 服务端通过加密算法得出会话密钥，最后回应加密通信算法改变通知和服务端握手结束通知 传输层协议 TCP 头部格式 ACK：为1表示确认应答报文 RST：为1表示出现异常时必须断开连接 SYN：为1表示希望建立连接 FIN： 为1表示希望断开连接 三次握手 客户端发送一个SYN为1的报文，序列号为一随机数，客户端进入SYN_SENT状态 服务端收到SYN报文后，回复一个ACK和SYN都为1的报文，序列号为一随机数，确认应答号为收到的SYN报文的序列号+1，并从LISTEN状态进入SYN_RCVD状态 客户端收到报文后，回复一个ACK报文，确认应答号为收到的报文的序列号+1，并进入ESTABLISHED状态，服务端收到后也进入ESTABLISHED状态 为什么是三次握手？\n保证双方具有接收和发送的能力 可以阻止重复历史连接的初始化（主要原因） 可以同步双方的初识序列号 可以避免资源浪费 四次挥手 客户端打算关闭连接，所以发送一个FIN报文，并从ESTABLISHED状态进入FIN_WAIT_1状态 服务端收到报文后回复一个ACK报文，进入CLOSED_WAIT状态，客户端收到后会进入FIN_WAIT_2状态 服务端处理完数据之后也发送一个FIN报文，之后进入LAST_ACK状态 客户端收到后回复一个ACK报文，并进入TIME_WAIT状态，在等待2MSL后关闭连接 TIME_WAIT状态为什么是2MSL？\n防止第四次挥手的报文丢失，2MSL指的是第四次挥手报文以及如果第四次挥手报文丢失后，被动结束方重传的FIN报文\nTCP可靠性保证 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ 协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。 协议 自动重传请求\n停止等待ARQ协议：每发完一个分组就停止发送，等待对方确认 连续ARQ协议：发送方维持一个发送窗口，接收方采取累积确认 重传机制 超时重传：发送数据时设置一个定时器，当超过指定时间后没有收到ACK报文就重传 超时重传时间RTO应略大于往返时延RTT 快速重传：当多次重复收到同一个ACK报文（累计应答），就重复后面的报文 SACK：在TCP头部加一个SACK，接收方可以缓存接收到哪些数据，并发送给发送方 D-SACK：用于告诉发送方有哪些数据被重复接收了 滑动窗口 引入窗口这个概念，解决往返时间过长造成的通信效果减低\n窗口大小指无需等待确认应答，而可以继续发送数据的最大值\n窗口可以分为4部分：1.已发送且已收到应答；2.已发送但未收到应答；3.未发送但总大小在接收方处理范围内（可用窗口）；4.未发送但大小超过接收方处理范围\n接收窗口和发送窗口大小相等吗？\n并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。\n因为滑动窗口并不是一成不变的\n流量控制 TCP提供的可以让发送方根据接收方的实际接收能力控制发送的数据量的机制\n当接收窗口收缩到0时，接收方发送消息通知发送方，发送窗口也收缩为0，当接收窗口不为0时发送一个窗口非0的报文，如果报文丢失就会造成死锁，TCP为每个连接设置了一个持续定时器，只要一方收到零窗口通知，就会启动计时器，如果计时器超时，就会发送窗口探测报文\n拥塞控制 为了避免网络出现拥堵时，发送方持续重传数据，从而造成恶性循环，在发送方设置一个拥塞窗口cwnd\n什么情况是发生了拥塞？\n只要发生了超时重传，就认为出现了拥塞\n拥塞控制四个算法？\n慢启动：当发送方每收到一个ACK，拥塞窗口cwnd的大小就加1，呈指数增长 当cwnd\u0026gt;=ssthresh（慢启动门限），就使用拥塞避免算法 拥塞避免算法：每收到一个ACK，cwnd就增加1/cwnd，呈线性增长 当触发了重传机制，就使用拥塞发生算法 拥塞发生 如果是超时重传，ssthresh设为cwnd/2，cwnd设置为1 如果是快速重传，cwnd设置为原来的一半，ssthresh设置为cwnd，进入快速恢复算法 快速恢复 cwnd设置为ssthresh+x(x为收到的重复确认应答数) 重传丢失的数据包 如果再收到重复的ACK，cwnd+1； 如果收到新的ACK，cwnd设置为第一步中ssthresh的值 拥塞控制和流量控制的区别？\n流量控制是避免发送方的数据填满接收方的缓存 UDP 头部格式 TCP和UDP的区别？\n连接：TCP是面向连接的，UDP是无连接的 服务对象：TCP只支持一对一，UDP支持一对一，一对多，多对多 可靠性：TCP是可靠交付的，UDP是尽最大努力交付 拥塞控制、流量控制：TCP有拥塞控制、流量控制机制来保证数据传输的安全性，UDP没有 首部开销：TCP最小20字节，UDP首部只有8字节 传输方式：TCP是流式传输，UDP是一个包一个包的传输 分片不同：TCP数据大小如果大于MSS大小，会在传输层进行分片、组装，UDP则在IP层 应用场景：TCP应用于FTP文件传输、HTTP等，UDP应用于视频、音频通信等 网络层协议 IP IP地址分类？\n无地址分类CIDR 不再有地址分类，而把地址分为网络号和主机号\n子网掩码与IP地址按位与得到网络号\nDNS域名解析 域名解析流程\n首先会先查询浏览器和操作系统缓存 浏览器发送DNS请求后，会先发给本地域名服务器，如果有则返回对应的IP地址 如果没有则询问根域名服务器，根域名服务器会给你顶级域名服务器的地址 然后去询问顶级域名服务器，顶级域名服务器会给你权威DNS服务器的地址 然后去权威域名服务器查询IP地址，本地域名服务器得到IP地址后再返回给客户端 ARP 通过广播ARP请求，设备拿到ARP请求包后如果与自己的IP地址相同，就将自己的MAC地址放入ARP响应包中，返回给主机\nRARP 已知MAC地址求IP地址\nDHCP 用于给设备动态分配IP地址\n步骤：\n客户端首先发起DHCP发现报文，全程使用UDP广播通信 DHCP服务器收到后，回复一个DHCP提供报文，报文携带可租约的IP地址、租期等信息 客户端收到后选择一个服务器，向其发送DHCP请求报文 DHCP服务器收到后用DHCP ACK报文进行响应 NAT 网络地址转换协议：用于缓解IP地址不足的问题\nICMP 互联网控制报文协议，功能包括：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。\nIGMP IGMP 是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间\nPing的工作原理\n主机A Ping 主机B：主机A发送都会先用ARP协议来获得MAC地址，如果无法到达主机B，就会返回一个ICMP目标不可达报文\n","permalink":"https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","summary":"计算机网络面试相关内容.","title":"计算机网络"}]