[{"content":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.\nFree Courses  Machine Learning Crash Course It\u0026rsquo;s a Google\u0026rsquo;s fast-paced, practical introduction to Machine Learning. NLP series Course In Chinese Its content includes text preprocessing, word embedding, RNN/LSTM, seq2seq model, and attention/self-attention mechanism. Audio Signal Processing for Machine Learning An awesome course about audio processing for machine learning.  Discussions and Questions  Kaggle is the world\u0026rsquo;s largest data science community with powerful tools and resources. Learn AI Together is a group chat about artificial intelligence in Discord. Machine Learning , Learn Machine Learning are communities about machine learning in Reddit.  Libraries and Tools  Scikit-learn is a free machine learning library. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. NLTK is a library for building Python programs to work with human language data. It has many useful tools like text tokenization, word lemmatization, and so on. Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.  Online Labs  Embedding Projector is a tool for visualizing high-dimensional data. The website has an example that shows how word2vec looks like.  Datasets  Tatoeba is a large database of sentences and translations. Its content is ever-growing and results from the voluntary contributions of thousands of members.  Others  Papers With Code is a navigation of Machine Learning papers, Code, Dataset, and their leaderboards.  ","permalink":"https://oyzg.github.io/archives/some-useful-website-or-resources-for-learning-ai/","summary":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.","title":"Some Valuable Websites or Resources for Learning AI"},{"content":"计算机网络分层 目前主要使用的网络模型为五层网络模型\n五层网络模型  应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等协议。数据单位为报文。 传输层 ：为进程提供通用数据传输服务。运输层包括两种协议：TCP， UDP。 网络层 ：为主机提供通信服务，路由选择。网络层有四个协议：ARP协议，IP协议，ICMP协议，IGMP协议 数据链路层 ：为同一链路的主机提供数据传输服务。将分组封装成帧。协议包括：Ethernet，PPP，[CSMA/CD](https://github.com/CyC2018/CS-Notes/blob/master/notes/计算机网络 - 链路层.md#csmacd-协议) 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。  七层网络模型 四层网络模型  应用层 传输层 网络层 网络接口层  应用层协议 HTTP（超文本传输协议） 首部字段  Host：请求的服务器域名 Content-Length：响应数据长度 Connection ：Keep-Alive为长连接 Content-Type：响应体数据格式 Content-Encoding：响应体数据压缩方式  特点 优点：\n 简单 灵活易于扩展 应用广泛，跨平台  缺点：\n 无状态双刃剑 明文传输双刃剑 不安全（明文可能被窃听，不验证身份可能被伪装，无法证明报文完整性可能被篡改）  状态码  1xx:提示信息 2xx：成功  200：OK，表示一切正常 204：No content，与200的区别在于响应头没有body数据 206：Partial Content，表示body为资源的一部分（应用于HTTP分块下载或断点续传）   3xx：重定向，301和302含字段Location表示跳转URL  301：永久重定向，请求资源已不存在 302：临时重定向，请求资源还存在 304：缓存重定向，资源未修改，重定向到已存在的缓存文件   4xx：客户端错误  400：请求报文错误，笼统的错误 403：服务器禁止访问资源 404：请求找不到资源   5xx：服务端错误  500：笼统的错误，并不知道发生了什么错误 501：客户端请求功能还不支持 502：服务器作为网关或代理，服务器自身正常，访问后端服务器发生了错误 503：服务器繁忙    Get与Post的区别  Get用于请求资源，Post用于对资源做修改处理 Get是幂等和安全的，Post不是幂等和安全的（幂等指多次执行结果相同） Get请求参数位于URL中，只支持ASCLL，长度有限制，Post请求数据在body中，无限制  HTTP1.1  提出了长连接的通信方式，减少了TCP重复连接断开所带来的开销，减轻了服务器端的负载 管道网络传输：不需要等待前一个请求响应再发送，减少了整体的响应时间 队头阻塞：服务器端仍然是按照顺序响应，如果发生阻塞，会导致所有请求阻塞  HTTP2  头部压缩：使用HPACK算法进行压缩 静态表编码 动态表编码 二进制帧 并发传输：多条stream复用一条TCP连接 服务器主动推送资源  HTTP3  基于UDP协议实现了QUIC协议，有以下优点  无队头阻塞 更快地连接建立 连接迁移    HTTPS 与HTTP的区别  在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，更加安全 在TCP三次握手基础上还需进行SSL/TLS 的握手 HTTP 的端口号是 80，HTTPS 的端口号是 443 HTTPS需向CA申请数字证书  如何解决HTTP安全问题  信息加密（混合加密）实现机密性，解决窃听问题 效验机制（摘要算法）实现完整性，解决篡改问题 数字证书解决冒充问题  HTTPS通信过程  首先是服务器将自己的公钥注册到CA，CA用自己的私钥对服务器的公钥签名并颁发数字证书 服务器将数字证书发送给浏览器（事先置入了CA的公钥），浏览器对数字证书进行解密得到服务器的公钥 浏览器用服务器的公钥对报文进行加密发送 服务器用自己的私钥对报文进行解密  SSL/TLS握手  客户端发送clientHello请求，包含版本，随机数（用于生产会话密钥），密码套件列表等信息 服务端响应serverHello，包含确认的版本和密码套件列表，随机数（用户生产会话密钥），数字证书 客户端回应一个加密的随机数，加密通信算法改变通知和客户端握手结束通知 服务端通过加密算法得出会话密钥，最后回应加密通信算法改变通知和服务端握手结束通知  传输层协议 TCP 头部格式  ACK：为1表示确认应答报文 RST：为1表示出现异常时必须断开连接 SYN：为1表示希望建立连接 FIN： 为1表示希望断开连接  三次握手  客户端发送一个SYN为1的报文，序列号为一随机数，客户端进入SYN_SENT状态 服务端收到SYN报文后，回复一个ACK和SYN都为1的报文，序列号为一随机数，确认应答号为收到的SYN报文的序列号+1，并从LISTEN状态进入SYN_RCVD状态 客户端收到报文后，回复一个ACK报文，确认应答号为收到的报文的序列号+1，并进入ESTABLISHED状态，服务端收到后也进入ESTABLISHED状态   为什么是三次握手？\n  保证双方具有接收和发送的能力 可以阻止重复历史连接的初始化（主要原因） 可以同步双方的初识序列号 可以避免资源浪费  四次挥手  客户端打算关闭连接，所以发送一个FIN报文，并从ESTABLISHED状态进入FIN_WAIT_1状态 服务端收到报文后回复一个ACK报文，进入CLOSED_WAIT状态，客户端收到后会进入FIN_WAIT_2状态 服务端处理完数据之后也发送一个FIN报文，之后进入LAST_ACK状态 客户端收到后回复一个ACK报文，并进入TIME_WAIT状态，在等待2MSL后关闭连接   TIME_WAIT状态为什么是2MSL？\n 防止第四次挥手的报文丢失，2MSL指的是第四次挥手报文以及如果第四次挥手报文丢失后，被动结束方重传的FIN报文\nTCP可靠性保证  应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ 协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。  ARP协议 自动重传请求\n 停止等待ARQ协议：每发完一个分组就停止发送，等待对方确认 连续ARQ协议：发送方维持一个发送窗口，接收方采取累积确认  重传机制  超时重传：发送数据时设置一个定时器，当超过指定时间后没有收到ACK报文就重传  超时重传时间RTO应略大于往返时延RTT   快速重传：当多次重复收到同一个ACK报文（累计应答），就重复后面的报文 SACK：在TCP头部加一个SACK，接收方可以缓存接收到哪些数据，并发送给发送方 D-SACK：用于告诉发送方有哪些数据被重复接收了  滑动窗口 引入窗口这个概念，解决往返时间过长造成的通信效果减低\n窗口大小指无需等待确认应答，而可以继续发送数据的最大值\n窗口可以分为4部分：1.已发送且已收到应答；2.已发送但未收到应答；3.未发送但总大小在接收方处理范围内（可用窗口）；4.未发送但大小超过接收方处理范围\n 接收窗口和发送窗口大小相等吗？\n 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。\n因为滑动窗口并不是一成不变的\n流量控制 TCP提供的可以让发送方根据接收方的实际接收能力控制发送的数据量的机制\n当接收窗口收缩到0时，接收方发送消息通知发送方，发送窗口也收缩为0，当接收窗口不为0时发送一个窗口非0的报文，如果报文丢失就会造成死锁，TCP为每个连接设置了一个持续定时器，只要一方收到零窗口通知，就会启动计时器，如果计时器超时，就会发送窗口探测报文\n拥塞控制 为了避免网络出现拥堵时，发送方持续重传数据，从而造成恶性循环，在发送方设置一个拥塞窗口cwnd\n 什么情况是发生了拥塞？\n 只要发生了超时重传，就认为出现了拥塞\n 拥塞控制四个算法？\n  慢启动：当发送方每收到一个ACK，拥塞窗口cwnd的大小就加1，呈指数增长  当cwnd\u0026gt;=ssthresh（慢启动门限），就使用拥塞避免算法   拥塞避免算法：每收到一个ACK，cwnd就增加1/cwnd，呈线性增长  当触发了重传机制，就使用拥塞发生算法   拥塞发生  如果是超时重传，ssthresh设为cwnd/2，cwnd设置为1 如果是快速重传，cwnd设置为原来的一半，ssthresh设置为cwnd，进入快速恢复算法   快速恢复  cwnd设置为ssthresh+x(x为收到的重复确认应答数) 重传丢失的数据包 如果再收到重复的ACK，cwnd+1； 如果收到新的ACK，cwnd设置为第一步中ssthresh的值     拥塞控制和流量控制的区别？\n  流量控制是避免发送方的数据填满接收方的缓存  UDP 头部格式  TCP和UDP的区别？\n  连接：TCP是面向连接的，UDP是无连接的 服务对象：TCP只支持一对一，UDP支持一对一，一对多，多对多 可靠性：TCP是可靠交付的，UDP是尽最大努力交付 拥塞控制、流量控制：TCP有拥塞控制、流量控制机制来保证数据传输的安全性，UDP没有 首部开销：TCP最小20字节，UDP首部只有8字节 传输方式：TCP是流式传输，UDP是一个包一个包的传输 分片不同：TCP数据大小如果大于MSS大小，会在传输层进行分片、组装，UDP则在IP层 应用场景：TCP应用于FTP文件传输、HTTP等，UDP应用于视频、音频通信等  网络层协议 IP  IP地址分类？\n 无地址分类CIDR 不再有地址分类，而把地址分为网络号和主机号\n子网掩码与IP地址按位与得到网络号\nDNS域名解析  域名解析流程\n  首先会先查询浏览器和操作系统缓存 浏览器发送DNS请求后，会先发给本地域名服务器，如果有则返回对应的IP地址 如果没有则询问根域名服务器，根域名服务器会给你顶级域名服务器的地址 然后去询问顶级域名服务器，顶级域名服务器会给你权威DNS服务器的地址 然后去权威域名服务器查询IP地址，本地域名服务器得到IP地址后再返回给客户端  ARP 通过广播ARP请求，设备拿到ARP请求包后如果与自己的IP地址相同，就将自己的MAC地址放入ARP响应包中，返回给主机\nRARP 已知MAC地址求IP地址\nDHCP 用于给设备动态分配IP地址\n步骤：\n 客户端首先发起DHCP发现报文，全程使用UDP广播通信 DHCP服务器收到后，回复一个DHCP提供报文，报文携带可租约的IP地址、租期等信息 客户端收到后选择一个服务器，向其发送DHCP请求报文 DHCP服务器收到后用DHCP ACK报文进行响应  NAT 网络地址转换协议：用于缓解IP地址不足的问题\nICMP 互联网控制报文协议，功能包括：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。\nIGMP IGMP 是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间\n Ping的工作原理\n 主机A Ping 主机B：主机A发送都会先用ARP协议来获得MAC地址，如果无法到达主机B，就会返回一个ICMP目标不可达报文\n","permalink":"https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","summary":"计算机网络面试相关内容.","title":"计算机网络"},{"content":"Introduction This article is a summary of the notebook about Quora insincere questions classification in Kaggle.\nThe link of kaggle notebook is below.\n https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification\n The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.\n Important: The complete code of examples below can be found in Github or Colab.\n Key Points Text Preprocessing Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\nExamples of tokenization with NLTK:\nfrom nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize  corpus = \u0026#39;\u0026#39;\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;\u0026#39;\u0026#39;  print(sent_tokenize(corpus)) print(word_tokenize(corpus))  output:\n [\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\u0026#39;, \u0026#39;One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;]\r[\u0026#39;Tokenization\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;process\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokenizing\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;splitting\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokens\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;One\u0026#39;, \u0026#39;can\u0026#39;, \u0026#39;think\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;parts\u0026#39;, \u0026#39;like\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;word\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;paragraph\u0026#39;, \u0026#39;.\u0026#39;] See also: https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\nLemmatization is a normalization technique in the field of natural language processing. There have a related technique called stemming. In some languages, the word has difference forms in difference contexts. The goal of both stemming and lemmatization is to reduce various forms to a common base form.\nExamples of lemmatization with NLTK:\nfrom nltk.stem import WordNetLemmatizer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] lemmatizer = WordNetLemmatizer()  print([lemmatizer.lemmatize(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] The result confused me. Why the output of gone and better isn\u0026rsquo;t the base form of them? Because we don\u0026rsquo;t provide the second parameter parts-of-speech to lemmatize. The parameter default value is n means a noun.\n Lemmatization needs the context to know what the parts-of-speech of the word is, a noun, verb or adjective so that can work well.\n Usage:\ndef lemmatize_sent(text):  pos_dict = {\u0026#39;NN\u0026#39;:\u0026#39;n\u0026#39;, \u0026#39;JJ\u0026#39;:\u0026#39;a\u0026#39;, \u0026#39;VB\u0026#39;:\u0026#39;v\u0026#39;, \u0026#39;RB\u0026#39;:\u0026#39;r\u0026#39;}  word_list = []  for word, tag in pos_tag(word_tokenize(text)):  pos = pos_dict[tag[0:2]] if tag[0:2] in pos_dict else \u0026#39;n\u0026#39;  word_list.append(lemmatizer.lemmatize(word, pos=pos))  return word_list  sentence = \u0026#39;He is walking to school\u0026#39; lemmatization_words = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)] print(\u0026#39;lemmatize word by word: \u0026#39;, lemmatization_words) print(\u0026#39;lemmatize with context: \u0026#39;, lemmatize_sent(sentence))  output:\n lemmatize word by word: [\u0026#39;He\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;walking\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;]\rlemmatize with context: [\u0026#39;He\u0026#39;, \u0026#39;be\u0026#39;, \u0026#39;walk\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;] Stemming shortens words with some rules and can be used without context, so it\u0026rsquo;s more efficient than Lemmatization.\nExamples of Stemming with NLTK:\nfrom nltk.stem import PorterStemmer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;going\u0026#39;, \u0026#39;history\u0026#39;] stemmer = PorterStemmer() print([stemmer.stem(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;go\u0026#39;, \u0026#39;histori\u0026#39;] As you can see, the stemming outputs may not be actual words.\n Time-consuming Test:\n from nltk.corpus import gutenberg  @timing def stemming(text):  [stemmer.stem(w) for w in word_tokenize(sentence)]  @timing def lemmatize(text):  lemmatize_sent(text)  @timing def lemmatize_without_context(text):  [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)]  book = gutenberg.raw(\u0026#34;austen-sense.txt\u0026#34;)  stemming(book) lemmatize(book) lemmatize_without_context(book)  output:\n stemming : 0.22 ms\rlemmatize : 5980.39 ms\rlemmatize_without_context : 0.17 ms See also: https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\nStop words usually refers to the most common words in a language, such as the, is, at, which and a in English. We remove the stop words before processing the text data because these words usually are no enough valuable.\nExamples of Stop words with NLTK:\nfrom nltk.corpus import stopwords  corpus = [\u0026#39;I\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;boy\u0026#39;] print([w for w in corpus if w not in set(stopwords.words(\u0026#39;english\u0026#39;))])  output:\n [\u0026#39;I\u0026#39;, \u0026#39;boy\u0026#39;] See also: https://www.nltk.org/book/ch02.html#stopwords_index_term\nText Vectorization is the process of converting text into a numerical vector that can be fed into a neural network or machine learning model. The most used methods to do that including Bag of Words, TF-IDF vectorization and Word2vec.\nExamples of Bag of Words with scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer  vectorizer = CountVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0 0 1 1 0 0 1]\r[0 1 0 0 0 1 0]\r[1 0 0 1 1 1 0]] The default configuration tokenizes the string by extracting words of at least 2 letters. so the word a has been discarded. The vector representation table of words is below.\nV(also) : [1 0 0 0 0 0 0]\rV(am) : [0 1 0 0 0 0 0]\rV(he) : [0 0 1 0 0 0 0]\rV(is) : [0 0 0 1 0 0 0]\rV(she) : [0 0 0 0 1 0 0]\rV(student) : [0 0 0 0 0 1 0]\rV(teacher) : [0 0 0 0 0 0 1]\rV(He is a teacher) = V(He) + V(is) + V(teacher) = [0 0 1 1 0 0 1]\r... TF means term-frequency. $$\\text{TF(t, d)}=\\frac{\\text{number of term t occurs in the document d}}{\\text{number of terms in the document d}}$$\nIDF means inverse document-frequency.\n$$\\text{IDF(t)}=\\log\\frac{\\text{number of documents in the corpus}}{\\text{number of document where the term t occurs}}$$\nTF-IDF means term-frequency times inverse document-frequency.\n$$\\text{TF-IDF(t, d)}=\\text{TF(t, d)} \\times \\text{IDF(t)}$$\nExamples of TF-IDF Verctorization with scikit-learn:\nfrom sklearn.feature_extraction.text import TfidfVectorizer  vectorizer = TfidfVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0. 0. 0.62276601 0.4736296 0. 0. 0.62276601]\r[0. 0.79596054 0. 0. 0. 0.60534851 0. ]\r[0.5628291 0. 0. 0.42804604 0.5628291 0.42804604 0. ]]  Note: The calculational formula of TF-IDF has some changes in the TfidfVectorizer.\n Word2vec is an algorithm that uses a shallow feedforward neural network to learn word embedding from a large corpus of text. There have two methods for producing a distributed representation of words.\n Continuous Bag-of-Words which was usually called CBOW predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). Continuous Skip-gram uses the current word to predict the surrounding window of context words.  See also: https://www.tensorflow.org/tutorials/text/word2vec\nExamples of Word2vec with gensim:\nimport gensim.downloader from gensim.models import Word2Vec  word2vec = gensim.downloader.load(\u0026#39;word2vec-google-news-300\u0026#39;) print(word2vec.most_similar(\u0026#39;car\u0026#39;)) print(word2vec.word_vec(\u0026#39;car\u0026#39;))  output:\n [(\u0026#39;vehicle\u0026#39;, 0.7821096181869507),\r(\u0026#39;cars\u0026#39;, 0.7423830032348633),\r(\u0026#39;SUV\u0026#39;, 0.7160962820053101),\r(\u0026#39;minivan\u0026#39;, 0.6907036304473877),\r(\u0026#39;truck\u0026#39;, 0.6735789775848389),\r(\u0026#39;Car\u0026#39;, 0.6677608489990234),\r(\u0026#39;Ford_Focus\u0026#39;, 0.667320191860199),\r(\u0026#39;Honda_Civic\u0026#39;, 0.662684977054596),\r(\u0026#39;Jeep\u0026#39;, 0.6511331796646118),\r(\u0026#39;pickup_truck\u0026#39;, 0.64414381980896)]\r[ 0.13085938 0.00842285 0.03344727 -0.05883789 0.04003906 -0.14257812\r0.04931641 -0.16894531 0.20898438 0.11962891 0.18066406 -0.25\r...\r0.04248047 0.12792969 -0.27539062 0.28515625 -0.04736328 0.06494141\r-0.11230469 -0.02575684 -0.04125977 0.22851562 -0.14941406 -0.15039062] See also: https://radimrehurek.com/gensim/models/word2vec.html\nLSTM network LSTM is an artificial recurrent neural network architecture. Its full name is long short-term memory, it is well-suited to classifying, processing and making predictions based on time series data.\nA common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\nSee also: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe following is an explanation of some parameters that were being used in the model.\nThe metrics parameter is used to evaluate the model during training and testing. It uses F1-score as the final evaluation metric in this task,. $$\\text{F1-score}=2\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}$$ Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.\nRecall is the number of true positive results divided by the number of all samples that should have been identified as positive.\nSee also: https://en.wikipedia.org/wiki/F-score\nThe bias_initializer parameter can influence the initial output of the layer. E.g. You have an imbalanced dataset of the ratio of positives to negatives is 1: 9. The model will predict 50% positive and 50% negative at initialization (PS: before training) if you don\u0026rsquo;t set the bias_initializer parameter. But if you have set the parameter properly, it will predict 10% positive and 90% negative at initialization, it is more reasonable. In addition, setting it correctly will speed up convergence.\nThe calculation of the bias_initializer parameter depends on what activation function the layer used. The correct bias to set can be derived from below if the activation function is sigmoid. $$p_0=\\frac{\\text{positive}}{\\text{positive}+\\text{negative}}=\\frac{1}{1+\\exp(-b_0)}$$ then $$b_0=-\\log(\\frac{1}{p_0}-1)=\\log(\\frac{\\text{positive}}{\\text{negative}})$$ See also: Classification on imbalanced data\nYou can refer to this question if the activation function is softmax.\nThe class weight parameter is also important for a model during training if the dataset is imbalanced. Giving a heavier weight to an under-represented class will cause the model to pay more attention to it.\nFinal Thoughts When I finish the article, I learned more than when I begin to write. I found some approaches more convenient to preprocess text, such as TextVectorization in tensorflow. I hava a more in-depth understand of Word2Vec. I just knew it was a distributed representation of words. Now I know there have two methods to implement it, CBOW and skip-gram. The skip-gram model uses the technique called negative sampling to maximize the probability of predicting context words when given the target word.\nReferences  Stemming and Lemmatization Basic NLP with NLTK NLP | How tokenizing text, sentence, words works Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text Word2Vec - Tensorflow Long short-term memory - Wikipedia  ","permalink":"https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/","summary":"The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.","title":"[Kaggle Summary] Quora Insincere Questions Classification"}]