[{"content":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.\nFree Courses  Machine Learning Crash Course It\u0026rsquo;s a Google\u0026rsquo;s fast-paced, practical introduction to Machine Learning. NLP series Course In Chinese Its content includes text preprocessing, word embedding, RNN/LSTM, seq2seq model, and attention/self-attention mechanism. Audio Signal Processing for Machine Learning An awesome course about audio processing for machine learning.  Discussions and Questions  Kaggle is the world\u0026rsquo;s largest data science community with powerful tools and resources. Learn AI Together is a group chat about artificial intelligence in Discord. Machine Learning , Learn Machine Learning are communities about machine learning in Reddit.  Libraries and Tools  Scikit-learn is a free machine learning library. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. NLTK is a library for building Python programs to work with human language data. It has many useful tools like text tokenization, word lemmatization, and so on. Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.  Online Labs  Embedding Projector is a tool for visualizing high-dimensional data. The website has an example that shows how word2vec looks like.  Datasets  Tatoeba is a large database of sentences and translations. Its content is ever-growing and results from the voluntary contributions of thousands of members.  Others  Papers With Code is a navigation of Machine Learning papers, Code, Dataset, and their leaderboards.  ","permalink":"https://oyzg.github.io/archives/some-useful-website-or-resources-for-learning-ai/","summary":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.","title":"Some Valuable Websites or Resources for Learning AI"},{"content":"Linux  Linux常用命令\n  文件目录操作  ls：查看目录下的文件 cd：进入指定目录 pwd：查看当前目录 mkdir：创建目录 rm：删除文件或目录，-rf删除目录 rmdir：删除目录 mv：移动文件，或改名 cp：复制文件 touch：查看文件或目录的日期时间 cat：显示文件内容，-n显示行号 nl：输出文件内容自动加上行号 more：显示文件内容，与cat区别为按页显示 less：显示文件内容，与more区别为more仅能向前移动，不能向后移动，且less查看之前不会加载整个文件 head：显示文件开头 tail：显示指定文件末尾内容   文件查找  which：搜索某个系统命令位置 whereis：定位可执行文件、源代码文件、帮助文件在文件系统中的位置 locate：快速的搜索系统内是否有指定的文件 find：沿着文件层次结构向下遍历，匹配符合条件的文件，并执行相应操作   文件打包上传和下载  tar：压缩解压文件，-zcvf压缩，-zxvf解压  -z：支持gzip解压文件 -c：建立新的压缩文件 -x：从压缩文件中提取文件 -v：显示操作过程 -f：压缩指定文件   gzip：压缩文件，为.gz文件   文件权限  chmod：改变文件访问权限 chgrp：改变文件所属群组 chown：改变文件的所有者和群组   磁盘存储  df：显示指定磁盘文件的可用空间 du：显示每个文件和目录的磁盘使用空间   性能监控和优化  top：显示系统当前正在执行的进程的相关信息，包括进程id，内存使用率，cpu占用率等 free：显示系统使用和空闲的内存情况，包括物理内存、交互区内存（swap）和内核缓存区内存 vmstat：用来显示虚拟内存信息 iostat：查看cpu、网卡、磁盘等设备的活动情况、负载信息 lsof：查看某个文件相关的进程   网络命令  ifconfig：查看配置网络 route：用于操作基于内核ip路由表 ping：确定主机与外部主机的状态 netstat：检验各端口网络连接情况 telnet：开启终端机阶段作业，并登入远端主机   其他命令  grep：文本搜索工具 ps：显示当前进程的状态    ","permalink":"https://oyzg.github.io/archives/linux/","summary":"Linux面试相关内容.","title":"Linux"},{"content":"操作系统理论知识 内存管理 内存管理主要负责内存的分配与回收（malloc 函数：申请内存，free 函数：释放内存），另外地址转换也就是将逻辑地址转换成相应的物理地址等功能也是操作系统内存管理做的事情。\n虚拟内存 操作系统提供的一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来\n 程序中使用到的内存地址为虚拟内存地址 实际中硬件中的空间地址为物理内存地址   为什么要有虚拟内存？\n  虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间），这样会更加有效地管理内存并减少出错。 虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，并且 把内存扩展到硬盘空间   局部性原理\n  时间局部性 ：如果程序中的某条指令一旦执行，不久以后该指令可能再次执行；如果某数据被访问过，不久以后该数据可能再次被访问。产生时间局部性的典型原因，是由于在程序中存在着大量的循环操作。 空间局部性 ：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，这是因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。   虚拟内存（虚拟存储器）的技术实现？\n  请求分页存储管理 ：建立在分页管理之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。请求分页存储管理系统中，在作业开始运行之前，仅装入当前要执行的部分段即可运行。假如在作业运行的过程中发现要访问的页面不在内存，则由处理器通知操作系统按照对应的页面置换算法将相应的页面调入到主存，同时操作系统也可以将暂时不用的页面置换到外存中。 请求分段存储管理 ：建立在分段存储管理之上，增加了请求调段功能、分段置换功能。请求分段储存管理方式就如同请求分页储存管理方式一样，在作业开始运行之前，仅装入当前要执行的部分段即可运行；在执行过程中，可使用请求调入中断动态装入要访问但又不在内存的程序段；当内存空间已满，而又需要装入新的段时，根据置换功能适当调出某个段，以便腾出空间而装入新的段。 请求段页式存储管理   请求分页与分页存储管理，两者有何不同呢？\n 请求分页存储管理建立在分页管理之上。他们的根本区别是是否将程序全部所需的全部地址空间都装入主存，这也是请求分页存储管理可以提供虚拟内存的原因\n它们之间的根本区别在于是否将一作业的全部地址空间同时装入主存。请求分页存储管理不要求将作业全部地址空间同时装入主存。基于这一点，请求分页存储管理可以提供虚存，而分页存储管理却不能提供虚存。\n内存分段 程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。不同的段是有不同的属性的，所以就用分段（Segmentation）的形式把这些段分离出来。\n内存分段的不足之处：\n 内存碎片  外部碎片：产生了多个不连续的小物理内存，可用内存交换（Swap）解决 内部碎片：程序所有内存都被装载到了物理内存，但一部分内存可能不常用造成了浪费   内存交换效率低：内存交换造成的，因为Swap需要写入硬盘，硬盘访问速度慢  内存分页 分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。在 Linux 下，每一页的大小为 4KB。\n不足：\n 页表会非常大，使用多级页表解决  多级页表 如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表\n段页式内存管理  先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制； 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；  段页式地址变换中要得到物理地址须经过三次内存访问：\n 第一次访问段表，得到页表起始地址； 第二次访问页表，得到物理页号； 第三次将物理页号与页内位移组合，得到物理地址。  页面置换算法  最佳页面置换算法 先进先出置换算法 最近最久未使用的页面置换算法 时钟页面置换算法 最不常用页面置换算法  磁盘调度算法  先来先服务算法 最短寻道时间优先算法 扫描算法 循环扫描算法 LOOK与C-LOOK算法  进程管理 线程 运行中的程序叫线程\n 进程的状态？\n  创建态：正在被创建的状态 就绪态：可运行，但是其他进程在运行 运行态：占用CPU在运行 阻塞态：不满足可运行的条件，在等待某一事件发生 终止态：正在从系统中消失的状态  在七状态模型中还有两种状态：\n 阻塞挂起状态：进程在外存且等待某一事件发生 就绪挂起状态：进程在外存，只要进入内存就可以立刻运行   进程创建的过程？\n  为新进程分配分配唯一的进程标识符，申请一个PCB 为进程分配资源 初始化PCB 如果进程的调度队列能够接纳，就插入到就绪队列  进程控制块PCB PCB是进程的唯一标识，包含以下信息：\n 进程描述信息：进程标识符、用户标识符 进程控制和管理信息：进程状态、进程优先级 资源分配清单 CPU相关信息   PCB是如何组织的？\n PCB是通过链表的方式进行组织的，把具有相同状态的进程链接在一起，组成各种队列\n进程间通信方式  管道：输出数据是单向的，Linux中的| 命名管道FIFO：可在不相关的进程间进行相互通信  管道的通信方式效率低，不适合进程间频繁的交换数据   消息队列：保存在内核中的消息链表，不适合大数据的传输，存在用户态和内核态之间的数据拷贝开销 共享内存：拿出一块虚拟地址空间来，映射到相同的物理内存中 信号量：一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据  P操作减一，V操作加一   信号：进程间通信机制唯一的异步通信机制 Socket  线程 线程是进程当中的一条执行流程\n 线程的优缺点？\n 优点：\n 一个进程可以同时存在多个进程 各个线程之间可以并发执行； 各个线程之间可以共享地址空间和文件等资源；  缺点：\n 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线程奔溃不会造成进程崩溃）  进程线程对比  进程是资源（包括内存、打开的文件等）分配的单位，线程是 CPU 调度的单位； 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈； 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系； 线程能减少并发执行的时间和空间开销；  调度算法  先来先服务算法 最短作业优先调度算法 高响应比优先调度算法（响应比=（等待时间+服务时间）/服务时间） 时间片轮转调度算法 最高优先级调度算法 多级反馈队列调度算法  ","permalink":"https://oyzg.github.io/archives/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","summary":"操作系统面试相关内容.","title":"操作系统"},{"content":"计算机网络分层 目前主要使用的网络模型为五层网络模型\n五层网络模型  应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等协议。数据单位为报文。 传输层 ：为进程提供通用数据传输服务。运输层包括两种协议：TCP， UDP。 网络层 ：为主机提供通信服务，路由选择。网络层有四个协议：ARP协议，IP协议，ICMP协议，IGMP协议 数据链路层 ：为同一链路的主机提供数据传输服务。将分组封装成帧。协议包括：Ethernet，PPP，[CSMA/CD](https://github.com/CyC2018/CS-Notes/blob/master/notes/计算机网络 - 链路层.md#csmacd-协议) 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。  七层网络模型 四层网络模型  应用层 传输层 网络层 网络接口层  应用层协议 HTTP（超文本传输协议） 首部字段  Host：请求的服务器域名 Content-Length：响应数据长度 Connection ：Keep-Alive为长连接 Content-Type：响应体数据格式 Content-Encoding：响应体数据压缩方式  特点 优点：\n 简单 灵活易于扩展 应用广泛，跨平台  缺点：\n 无状态双刃剑 明文传输双刃剑 不安全（明文可能被窃听，不验证身份可能被伪装，无法证明报文完整性可能被篡改）  状态码  1xx:提示信息 2xx：成功  200：OK，表示一切正常 204：No content，与200的区别在于响应头没有body数据 206：Partial Content，表示body为资源的一部分（应用于HTTP分块下载或断点续传）   3xx：重定向，301和302含字段Location表示跳转URL  301：永久重定向，请求资源已不存在 302：临时重定向，请求资源还存在 304：缓存重定向，资源未修改，重定向到已存在的缓存文件   4xx：客户端错误  400：请求报文错误，笼统的错误 403：服务器禁止访问资源 404：请求找不到资源   5xx：服务端错误  500：笼统的错误，并不知道发生了什么错误 501：客户端请求功能还不支持 502：服务器作为网关或代理，服务器自身正常，访问后端服务器发生了错误 503：服务器繁忙    Get与Post的区别  Get用于请求资源，Post用于对资源做修改处理 Get是幂等和安全的，Post不是幂等和安全的（幂等指多次执行结果相同） Get请求参数位于URL中，只支持ASCLL，长度有限制，Post请求数据在body中，无限制  HTTP1.1  提出了长连接的通信方式，减少了TCP重复连接断开所带来的开销，减轻了服务器端的负载 管道网络传输：不需要等待前一个请求响应再发送，减少了整体的响应时间 队头阻塞：服务器端仍然是按照顺序响应，如果发生阻塞，会导致所有请求阻塞  HTTP2  头部压缩：使用HPACK算法进行压缩 静态表编码 动态表编码 二进制帧 并发传输：多条stream复用一条TCP连接 服务器主动推送资源  HTTP3  基于UDP协议实现了QUIC协议，有以下优点  无队头阻塞 更快地连接建立 连接迁移    HTTPS 与HTTP的区别  在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，更加安全 在TCP三次握手基础上还需进行SSL/TLS 的握手 HTTP 的端口号是 80，HTTPS 的端口号是 443 HTTPS需向CA申请数字证书  如何解决HTTP安全问题  信息加密（混合加密）实现机密性，解决窃听问题 效验机制（摘要算法）实现完整性，解决篡改问题 数字证书解决冒充问题  HTTPS通信过程  首先是服务器将自己的公钥注册到CA，CA用自己的私钥对服务器的公钥签名并颁发数字证书 服务器将数字证书发送给浏览器（事先置入了CA的公钥），浏览器对数字证书进行解密得到服务器的公钥 浏览器用服务器的公钥对报文进行加密发送 服务器用自己的私钥对报文进行解密  SSL/TLS握手  客户端发送clientHello请求，包含版本，随机数（用于生产会话密钥），密码套件列表等信息 服务端响应serverHello，包含确认的版本和密码套件列表，随机数（用户生产会话密钥），数字证书 客户端回应一个加密的随机数，加密通信算法改变通知和客户端握手结束通知 服务端通过加密算法得出会话密钥，最后回应加密通信算法改变通知和服务端握手结束通知  传输层协议 TCP 头部格式  ACK：为1表示确认应答报文 RST：为1表示出现异常时必须断开连接 SYN：为1表示希望建立连接 FIN： 为1表示希望断开连接  三次握手  客户端发送一个SYN为1的报文，序列号为一随机数，客户端进入SYN_SENT状态 服务端收到SYN报文后，回复一个ACK和SYN都为1的报文，序列号为一随机数，确认应答号为收到的SYN报文的序列号+1，并从LISTEN状态进入SYN_RCVD状态 客户端收到报文后，回复一个ACK报文，确认应答号为收到的报文的序列号+1，并进入ESTABLISHED状态，服务端收到后也进入ESTABLISHED状态   为什么是三次握手？\n  保证双方具有接收和发送的能力 可以阻止重复历史连接的初始化（主要原因） 可以同步双方的初识序列号 可以避免资源浪费  四次挥手  客户端打算关闭连接，所以发送一个FIN报文，并从ESTABLISHED状态进入FIN_WAIT_1状态 服务端收到报文后回复一个ACK报文，进入CLOSED_WAIT状态，客户端收到后会进入FIN_WAIT_2状态 服务端处理完数据之后也发送一个FIN报文，之后进入LAST_ACK状态 客户端收到后回复一个ACK报文，并进入TIME_WAIT状态，在等待2MSL后关闭连接   TIME_WAIT状态为什么是2MSL？\n 防止第四次挥手的报文丢失，2MSL指的是第四次挥手报文以及如果第四次挥手报文丢失后，被动结束方重传的FIN报文\nTCP可靠性保证  应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和： TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制： TCP 连接的每一方都有固定大小的缓冲空间，TCP 的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制： 当网络拥塞时，减少数据的发送。 ARQ 协议： 也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传： 当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。  协议 自动重传请求\n 停止等待ARQ协议：每发完一个分组就停止发送，等待对方确认 连续ARQ协议：发送方维持一个发送窗口，接收方采取累积确认  重传机制  超时重传：发送数据时设置一个定时器，当超过指定时间后没有收到ACK报文就重传  超时重传时间RTO应略大于往返时延RTT   快速重传：当多次重复收到同一个ACK报文（累计应答），就重复后面的报文 SACK：在TCP头部加一个SACK，接收方可以缓存接收到哪些数据，并发送给发送方 D-SACK：用于告诉发送方有哪些数据被重复接收了  滑动窗口 引入窗口这个概念，解决往返时间过长造成的通信效果减低\n窗口大小指无需等待确认应答，而可以继续发送数据的最大值\n窗口可以分为4部分：1.已发送且已收到应答；2.已发送但未收到应答；3.未发送但总大小在接收方处理范围内（可用窗口）；4.未发送但大小超过接收方处理范围\n 接收窗口和发送窗口大小相等吗？\n 并不是完全相等，接收窗口的大小是约等于发送窗口的大小的。\n因为滑动窗口并不是一成不变的\n流量控制 TCP提供的可以让发送方根据接收方的实际接收能力控制发送的数据量的机制\n当接收窗口收缩到0时，接收方发送消息通知发送方，发送窗口也收缩为0，当接收窗口不为0时发送一个窗口非0的报文，如果报文丢失就会造成死锁，TCP为每个连接设置了一个持续定时器，只要一方收到零窗口通知，就会启动计时器，如果计时器超时，就会发送窗口探测报文\n拥塞控制 为了避免网络出现拥堵时，发送方持续重传数据，从而造成恶性循环，在发送方设置一个拥塞窗口cwnd\n 什么情况是发生了拥塞？\n 只要发生了超时重传，就认为出现了拥塞\n 拥塞控制四个算法？\n  慢启动：当发送方每收到一个ACK，拥塞窗口cwnd的大小就加1，呈指数增长  当cwnd\u0026gt;=ssthresh（慢启动门限），就使用拥塞避免算法   拥塞避免算法：每收到一个ACK，cwnd就增加1/cwnd，呈线性增长  当触发了重传机制，就使用拥塞发生算法   拥塞发生  如果是超时重传，ssthresh设为cwnd/2，cwnd设置为1 如果是快速重传，cwnd设置为原来的一半，ssthresh设置为cwnd，进入快速恢复算法   快速恢复  cwnd设置为ssthresh+x(x为收到的重复确认应答数) 重传丢失的数据包 如果再收到重复的ACK，cwnd+1； 如果收到新的ACK，cwnd设置为第一步中ssthresh的值     拥塞控制和流量控制的区别？\n  流量控制是避免发送方的数据填满接收方的缓存  UDP 头部格式  TCP和UDP的区别？\n  连接：TCP是面向连接的，UDP是无连接的 服务对象：TCP只支持一对一，UDP支持一对一，一对多，多对多 可靠性：TCP是可靠交付的，UDP是尽最大努力交付 拥塞控制、流量控制：TCP有拥塞控制、流量控制机制来保证数据传输的安全性，UDP没有 首部开销：TCP最小20字节，UDP首部只有8字节 传输方式：TCP是流式传输，UDP是一个包一个包的传输 分片不同：TCP数据大小如果大于MSS大小，会在传输层进行分片、组装，UDP则在IP层 应用场景：TCP应用于FTP文件传输、HTTP等，UDP应用于视频、音频通信等  网络层协议 IP  IP地址分类？\n 无地址分类CIDR 不再有地址分类，而把地址分为网络号和主机号\n子网掩码与IP地址按位与得到网络号\nDNS域名解析  域名解析流程\n  首先会先查询浏览器和操作系统缓存 浏览器发送DNS请求后，会先发给本地域名服务器，如果有则返回对应的IP地址 如果没有则询问根域名服务器，根域名服务器会给你顶级域名服务器的地址 然后去询问顶级域名服务器，顶级域名服务器会给你权威DNS服务器的地址 然后去权威域名服务器查询IP地址，本地域名服务器得到IP地址后再返回给客户端  ARP 通过广播ARP请求，设备拿到ARP请求包后如果与自己的IP地址相同，就将自己的MAC地址放入ARP响应包中，返回给主机\nRARP 已知MAC地址求IP地址\nDHCP 用于给设备动态分配IP地址\n步骤：\n 客户端首先发起DHCP发现报文，全程使用UDP广播通信 DHCP服务器收到后，回复一个DHCP提供报文，报文携带可租约的IP地址、租期等信息 客户端收到后选择一个服务器，向其发送DHCP请求报文 DHCP服务器收到后用DHCP ACK报文进行响应  NAT 网络地址转换协议：用于缓解IP地址不足的问题\nICMP 互联网控制报文协议，功能包括：确认 IP 包是否成功送达目标地址、报告发送过程中 IP 包被废弃的原因和改善网络设置等。\nIGMP IGMP 是因特网组管理协议，工作在主机（组播成员）和最后一跳路由之间\n Ping的工作原理\n 主机A Ping 主机B：主机A发送都会先用ARP协议来获得MAC地址，如果无法到达主机B，就会返回一个ICMP目标不可达报文\n","permalink":"https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","summary":"计算机网络面试相关内容.","title":"计算机网络"},{"content":"Introduction This article is a summary of the notebook about Quora insincere questions classification in Kaggle.\nThe link of kaggle notebook is below.\n https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification\n The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.\n Important: The complete code of examples below can be found in Github or Colab.\n Key Points Text Preprocessing Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\nExamples of tokenization with NLTK:\nfrom nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize  corpus = \u0026#39;\u0026#39;\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;\u0026#39;\u0026#39;  print(sent_tokenize(corpus)) print(word_tokenize(corpus))  output:\n [\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\u0026#39;, \u0026#39;One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;]\r[\u0026#39;Tokenization\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;process\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokenizing\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;splitting\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokens\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;One\u0026#39;, \u0026#39;can\u0026#39;, \u0026#39;think\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;parts\u0026#39;, \u0026#39;like\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;word\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;paragraph\u0026#39;, \u0026#39;.\u0026#39;] See also: https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\nLemmatization is a normalization technique in the field of natural language processing. There have a related technique called stemming. In some languages, the word has difference forms in difference contexts. The goal of both stemming and lemmatization is to reduce various forms to a common base form.\nExamples of lemmatization with NLTK:\nfrom nltk.stem import WordNetLemmatizer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] lemmatizer = WordNetLemmatizer()  print([lemmatizer.lemmatize(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] The result confused me. Why the output of gone and better isn\u0026rsquo;t the base form of them? Because we don\u0026rsquo;t provide the second parameter parts-of-speech to lemmatize. The parameter default value is n means a noun.\n Lemmatization needs the context to know what the parts-of-speech of the word is, a noun, verb or adjective so that can work well.\n Usage:\ndef lemmatize_sent(text):  pos_dict = {\u0026#39;NN\u0026#39;:\u0026#39;n\u0026#39;, \u0026#39;JJ\u0026#39;:\u0026#39;a\u0026#39;, \u0026#39;VB\u0026#39;:\u0026#39;v\u0026#39;, \u0026#39;RB\u0026#39;:\u0026#39;r\u0026#39;}  word_list = []  for word, tag in pos_tag(word_tokenize(text)):  pos = pos_dict[tag[0:2]] if tag[0:2] in pos_dict else \u0026#39;n\u0026#39;  word_list.append(lemmatizer.lemmatize(word, pos=pos))  return word_list  sentence = \u0026#39;He is walking to school\u0026#39; lemmatization_words = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)] print(\u0026#39;lemmatize word by word: \u0026#39;, lemmatization_words) print(\u0026#39;lemmatize with context: \u0026#39;, lemmatize_sent(sentence))  output:\n lemmatize word by word: [\u0026#39;He\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;walking\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;]\rlemmatize with context: [\u0026#39;He\u0026#39;, \u0026#39;be\u0026#39;, \u0026#39;walk\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;] Stemming shortens words with some rules and can be used without context, so it\u0026rsquo;s more efficient than Lemmatization.\nExamples of Stemming with NLTK:\nfrom nltk.stem import PorterStemmer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;going\u0026#39;, \u0026#39;history\u0026#39;] stemmer = PorterStemmer() print([stemmer.stem(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;go\u0026#39;, \u0026#39;histori\u0026#39;] As you can see, the stemming outputs may not be actual words.\n Time-consuming Test:\n from nltk.corpus import gutenberg  @timing def stemming(text):  [stemmer.stem(w) for w in word_tokenize(sentence)]  @timing def lemmatize(text):  lemmatize_sent(text)  @timing def lemmatize_without_context(text):  [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)]  book = gutenberg.raw(\u0026#34;austen-sense.txt\u0026#34;)  stemming(book) lemmatize(book) lemmatize_without_context(book)  output:\n stemming : 0.22 ms\rlemmatize : 5980.39 ms\rlemmatize_without_context : 0.17 ms See also: https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\nStop words usually refers to the most common words in a language, such as the, is, at, which and a in English. We remove the stop words before processing the text data because these words usually are no enough valuable.\nExamples of Stop words with NLTK:\nfrom nltk.corpus import stopwords  corpus = [\u0026#39;I\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;boy\u0026#39;] print([w for w in corpus if w not in set(stopwords.words(\u0026#39;english\u0026#39;))])  output:\n [\u0026#39;I\u0026#39;, \u0026#39;boy\u0026#39;] See also: https://www.nltk.org/book/ch02.html#stopwords_index_term\nText Vectorization is the process of converting text into a numerical vector that can be fed into a neural network or machine learning model. The most used methods to do that including Bag of Words, TF-IDF vectorization and Word2vec.\nExamples of Bag of Words with scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer  vectorizer = CountVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0 0 1 1 0 0 1]\r[0 1 0 0 0 1 0]\r[1 0 0 1 1 1 0]] The default configuration tokenizes the string by extracting words of at least 2 letters. so the word a has been discarded. The vector representation table of words is below.\nV(also) : [1 0 0 0 0 0 0]\rV(am) : [0 1 0 0 0 0 0]\rV(he) : [0 0 1 0 0 0 0]\rV(is) : [0 0 0 1 0 0 0]\rV(she) : [0 0 0 0 1 0 0]\rV(student) : [0 0 0 0 0 1 0]\rV(teacher) : [0 0 0 0 0 0 1]\rV(He is a teacher) = V(He) + V(is) + V(teacher) = [0 0 1 1 0 0 1]\r... TF means term-frequency. $$\\text{TF(t, d)}=\\frac{\\text{number of term t occurs in the document d}}{\\text{number of terms in the document d}}$$\nIDF means inverse document-frequency.\n$$\\text{IDF(t)}=\\log\\frac{\\text{number of documents in the corpus}}{\\text{number of document where the term t occurs}}$$\nTF-IDF means term-frequency times inverse document-frequency.\n$$\\text{TF-IDF(t, d)}=\\text{TF(t, d)} \\times \\text{IDF(t)}$$\nExamples of TF-IDF Verctorization with scikit-learn:\nfrom sklearn.feature_extraction.text import TfidfVectorizer  vectorizer = TfidfVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0. 0. 0.62276601 0.4736296 0. 0. 0.62276601]\r[0. 0.79596054 0. 0. 0. 0.60534851 0. ]\r[0.5628291 0. 0. 0.42804604 0.5628291 0.42804604 0. ]]  Note: The calculational formula of TF-IDF has some changes in the TfidfVectorizer.\n Word2vec is an algorithm that uses a shallow feedforward neural network to learn word embedding from a large corpus of text. There have two methods for producing a distributed representation of words.\n Continuous Bag-of-Words which was usually called CBOW predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). Continuous Skip-gram uses the current word to predict the surrounding window of context words.  See also: https://www.tensorflow.org/tutorials/text/word2vec\nExamples of Word2vec with gensim:\nimport gensim.downloader from gensim.models import Word2Vec  word2vec = gensim.downloader.load(\u0026#39;word2vec-google-news-300\u0026#39;) print(word2vec.most_similar(\u0026#39;car\u0026#39;)) print(word2vec.word_vec(\u0026#39;car\u0026#39;))  output:\n [(\u0026#39;vehicle\u0026#39;, 0.7821096181869507),\r(\u0026#39;cars\u0026#39;, 0.7423830032348633),\r(\u0026#39;SUV\u0026#39;, 0.7160962820053101),\r(\u0026#39;minivan\u0026#39;, 0.6907036304473877),\r(\u0026#39;truck\u0026#39;, 0.6735789775848389),\r(\u0026#39;Car\u0026#39;, 0.6677608489990234),\r(\u0026#39;Ford_Focus\u0026#39;, 0.667320191860199),\r(\u0026#39;Honda_Civic\u0026#39;, 0.662684977054596),\r(\u0026#39;Jeep\u0026#39;, 0.6511331796646118),\r(\u0026#39;pickup_truck\u0026#39;, 0.64414381980896)]\r[ 0.13085938 0.00842285 0.03344727 -0.05883789 0.04003906 -0.14257812\r0.04931641 -0.16894531 0.20898438 0.11962891 0.18066406 -0.25\r...\r0.04248047 0.12792969 -0.27539062 0.28515625 -0.04736328 0.06494141\r-0.11230469 -0.02575684 -0.04125977 0.22851562 -0.14941406 -0.15039062] See also: https://radimrehurek.com/gensim/models/word2vec.html\nLSTM network LSTM is an artificial recurrent neural network architecture. Its full name is long short-term memory, it is well-suited to classifying, processing and making predictions based on time series data.\nA common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\nSee also: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe following is an explanation of some parameters that were being used in the model.\nThe metrics parameter is used to evaluate the model during training and testing. It uses F1-score as the final evaluation metric in this task,. $$\\text{F1-score}=2\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}$$ Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.\nRecall is the number of true positive results divided by the number of all samples that should have been identified as positive.\nSee also: https://en.wikipedia.org/wiki/F-score\nThe bias_initializer parameter can influence the initial output of the layer. E.g. You have an imbalanced dataset of the ratio of positives to negatives is 1: 9. The model will predict 50% positive and 50% negative at initialization (PS: before training) if you don\u0026rsquo;t set the bias_initializer parameter. But if you have set the parameter properly, it will predict 10% positive and 90% negative at initialization, it is more reasonable. In addition, setting it correctly will speed up convergence.\nThe calculation of the bias_initializer parameter depends on what activation function the layer used. The correct bias to set can be derived from below if the activation function is sigmoid. $$p_0=\\frac{\\text{positive}}{\\text{positive}+\\text{negative}}=\\frac{1}{1+\\exp(-b_0)}$$ then $$b_0=-\\log(\\frac{1}{p_0}-1)=\\log(\\frac{\\text{positive}}{\\text{negative}})$$ See also: Classification on imbalanced data\nYou can refer to this question if the activation function is softmax.\nThe class weight parameter is also important for a model during training if the dataset is imbalanced. Giving a heavier weight to an under-represented class will cause the model to pay more attention to it.\nFinal Thoughts When I finish the article, I learned more than when I begin to write. I found some approaches more convenient to preprocess text, such as TextVectorization in tensorflow. I hava a more in-depth understand of Word2Vec. I just knew it was a distributed representation of words. Now I know there have two methods to implement it, CBOW and skip-gram. The skip-gram model uses the technique called negative sampling to maximize the probability of predicting context words when given the target word.\nReferences  Stemming and Lemmatization Basic NLP with NLTK NLP | How tokenizing text, sentence, words works Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text Word2Vec - Tensorflow Long short-term memory - Wikipedia  ","permalink":"https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/","summary":"The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.","title":"[Kaggle Summary] Quora Insincere Questions Classification"}]