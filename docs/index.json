[{"content":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.\nFree Courses  Machine Learning Crash Course It\u0026rsquo;s a Google\u0026rsquo;s fast-paced, practical introduction to Machine Learning. NLP series Course In Chinese Its content includes text preprocessing, word embedding, RNN/LSTM, seq2seq model, and attention/self-attention mechanism. Audio Signal Processing for Machine Learning An awesome course about audio processing for machine learning.  Discussions and Questions  Kaggle is the world\u0026rsquo;s largest data science community with powerful tools and resources. Learn AI Together is a group chat about artificial intelligence in Discord. Machine Learning , Learn Machine Learning are communities about machine learning in Reddit.  Libraries and Tools  Scikit-learn is a free machine learning library. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. NLTK is a library for building Python programs to work with human language data. It has many useful tools like text tokenization, word lemmatization, and so on. Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.  Online Labs  Embedding Projector is a tool for visualizing high-dimensional data. The website has an example that shows how word2vec looks like.  Datasets  Tatoeba is a large database of sentences and translations. Its content is ever-growing and results from the voluntary contributions of thousands of members.  Others  Papers With Code is a navigation of Machine Learning papers, Code, Dataset, and their leaderboards.  ","permalink":"https://oyzg.github.io/archives/some-useful-website-or-resources-for-learning-ai/","summary":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.","title":"Some Valuable Websites or Resources for Learning AI"},{"content":"计算机网络分层 目前主要使用的网络模型为五层网络模型\n五层网络模型  应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等协议。数据单位为报文。 传输层 ：为进程提供通用数据传输服务。运输层包括两种协议：TCP， UDP。 网络层 ：为主机提供通信服务，路由选择。网络层有四个协议：ARP协议，IP协议，ICMP协议，IGMP协议 数据链路层 ：为同一链路的主机提供数据传输服务。将分组封装成帧。协议包括：Ethernet，PPP，[CSMA/CD](https://github.com/CyC2018/CS-Notes/blob/master/notes/计算机网络 - 链路层.md#csmacd-协议) 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。  七层网络模型 四层网络模型  应用层 传输层 网络层 网络接口层  应用层协议 HTTP（超文本传输协议） 首部字段  Host：请求的服务器域名 Content-Length：响应数据长度 Connection ：Keep-Alive为长连接 Content-Type：响应体数据格式 Content-Encoding：响应体数据压缩方式  特点 优点：\n 简单 灵活易于扩展 应用广泛，跨平台  缺点：\n 无状态双刃剑 明文传输双刃剑 不安全（明文可能被窃听，不验证身份可能被伪装，无法证明报文完整性可能被篡改）  状态码  1xx:提示信息 2xx：成功  200：OK，表示一切正常 204：No content，与200的区别在于响应头没有body数据 206：Partial Content，表示body为资源的一部分（应用于HTTP分块下载或断点续传）   3xx：重定向，301和302含字段Location表示跳转URL  301：永久重定向，请求资源已不存在 302：临时重定向，请求资源还存在 304：缓存重定向，资源未修改，重定向到已存在的缓存文件   4xx：客户端错误  400：请求报文错误，笼统的错误 403：服务器禁止访问资源 404：请求找不到资源   5xx：服务端错误  500：笼统的错误，并不知道发生了什么错误 501：客户端请求功能还不支持 502：服务器作为网关或代理，服务器自身正常，访问后端服务器发生了错误 503：服务器繁忙    Get与Post的区别  Get用于请求资源，Post用于对资源做修改处理 Get是幂等和安全的，Post不是幂等和安全的（幂等指多次执行结果相同） Get请求参数位于URL中，只支持ASCLL，长度有限制，Post请求数据在body中，无限制  HTTP1.1  提出了长连接的通信方式，减少了TCP重复连接断开所带来的开销，减轻了服务器端的负载 管道网络传输：不需要等待前一个请求响应再发送，减少了整体的响应时间 队头阻塞：服务器端仍然是按照顺序响应，如果发生阻塞，会导致所有请求阻塞  HTTP2  头部压缩：使用HPACK算法进行压缩 静态表编码 动态表编码 二进制帧 并发传输：多条stream复用一条TCP连接 服务器主动推送资源  HTTP3  基于UDP协议实现了QUIC协议，有以下优点  无队头阻塞 更快地连接建立 连接迁移    HTTPS 与HTTP的区别  在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，更加安全 在TCP三次握手基础上还需进行SSL/TLS 的握手 HTTP 的端口号是 80，HTTPS 的端口号是 443 HTTPS需向CA申请数字证书  如何解决HTTP安全问题  信息加密（混合加密）实现机密性，解决窃听问题 效验机制（摘要算法）实现完整性，解决篡改问题 数字证书解决冒充问题  HTTPS通信过程  首先是服务器将自己的公钥注册到CA，CA用自己的私钥对服务器的公钥签名并颁发数字证书 服务器将数字证书发送给浏览器（事先置入了CA的公钥），浏览器对数字证书进行解密得到服务器的公钥 浏览器用服务器的公钥对报文进行加密发送 服务器用自己的私钥对报文进行解密  SSL/TLS握手  客户端发送clientHello请求，包含版本，随机数（用于生产会话密钥），密码套件列表等信息 服务端响应serverHello，包含确认的版本和密码套件列表，随机数（用户生产会话密钥），数字证书 客户端回应一个加密的随机数，加密通信算法改变通知和客户端握手结束通知 服务端通过加密算法得出会话密钥，最后回应加密通信算法改变通知和服务端握手结束通知  传输层协议 TCP 头部格式 ","permalink":"https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","summary":"计算机网络面试相关内容.","title":"计算机网络"},{"content":"Introduction This article is a summary of the notebook about Quora insincere questions classification in Kaggle.\nThe link of kaggle notebook is below.\n https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification\n The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.\n Important: The complete code of examples below can be found in Github or Colab.\n Key Points Text Preprocessing Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\nExamples of tokenization with NLTK:\nfrom nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize  corpus = \u0026#39;\u0026#39;\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;\u0026#39;\u0026#39;  print(sent_tokenize(corpus)) print(word_tokenize(corpus))  output:\n [\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\u0026#39;, \u0026#39;One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;]\r[\u0026#39;Tokenization\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;process\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokenizing\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;splitting\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokens\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;One\u0026#39;, \u0026#39;can\u0026#39;, \u0026#39;think\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;parts\u0026#39;, \u0026#39;like\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;word\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;paragraph\u0026#39;, \u0026#39;.\u0026#39;] See also: https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\nLemmatization is a normalization technique in the field of natural language processing. There have a related technique called stemming. In some languages, the word has difference forms in difference contexts. The goal of both stemming and lemmatization is to reduce various forms to a common base form.\nExamples of lemmatization with NLTK:\nfrom nltk.stem import WordNetLemmatizer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] lemmatizer = WordNetLemmatizer()  print([lemmatizer.lemmatize(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] The result confused me. Why the output of gone and better isn\u0026rsquo;t the base form of them? Because we don\u0026rsquo;t provide the second parameter parts-of-speech to lemmatize. The parameter default value is n means a noun.\n Lemmatization needs the context to know what the parts-of-speech of the word is, a noun, verb or adjective so that can work well.\n Usage:\ndef lemmatize_sent(text):  pos_dict = {\u0026#39;NN\u0026#39;:\u0026#39;n\u0026#39;, \u0026#39;JJ\u0026#39;:\u0026#39;a\u0026#39;, \u0026#39;VB\u0026#39;:\u0026#39;v\u0026#39;, \u0026#39;RB\u0026#39;:\u0026#39;r\u0026#39;}  word_list = []  for word, tag in pos_tag(word_tokenize(text)):  pos = pos_dict[tag[0:2]] if tag[0:2] in pos_dict else \u0026#39;n\u0026#39;  word_list.append(lemmatizer.lemmatize(word, pos=pos))  return word_list  sentence = \u0026#39;He is walking to school\u0026#39; lemmatization_words = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)] print(\u0026#39;lemmatize word by word: \u0026#39;, lemmatization_words) print(\u0026#39;lemmatize with context: \u0026#39;, lemmatize_sent(sentence))  output:\n lemmatize word by word: [\u0026#39;He\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;walking\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;]\rlemmatize with context: [\u0026#39;He\u0026#39;, \u0026#39;be\u0026#39;, \u0026#39;walk\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;] Stemming shortens words with some rules and can be used without context, so it\u0026rsquo;s more efficient than Lemmatization.\nExamples of Stemming with NLTK:\nfrom nltk.stem import PorterStemmer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;going\u0026#39;, \u0026#39;history\u0026#39;] stemmer = PorterStemmer() print([stemmer.stem(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;go\u0026#39;, \u0026#39;histori\u0026#39;] As you can see, the stemming outputs may not be actual words.\n Time-consuming Test:\n from nltk.corpus import gutenberg  @timing def stemming(text):  [stemmer.stem(w) for w in word_tokenize(sentence)]  @timing def lemmatize(text):  lemmatize_sent(text)  @timing def lemmatize_without_context(text):  [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)]  book = gutenberg.raw(\u0026#34;austen-sense.txt\u0026#34;)  stemming(book) lemmatize(book) lemmatize_without_context(book)  output:\n stemming : 0.22 ms\rlemmatize : 5980.39 ms\rlemmatize_without_context : 0.17 ms See also: https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\nStop words usually refers to the most common words in a language, such as the, is, at, which and a in English. We remove the stop words before processing the text data because these words usually are no enough valuable.\nExamples of Stop words with NLTK:\nfrom nltk.corpus import stopwords  corpus = [\u0026#39;I\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;boy\u0026#39;] print([w for w in corpus if w not in set(stopwords.words(\u0026#39;english\u0026#39;))])  output:\n [\u0026#39;I\u0026#39;, \u0026#39;boy\u0026#39;] See also: https://www.nltk.org/book/ch02.html#stopwords_index_term\nText Vectorization is the process of converting text into a numerical vector that can be fed into a neural network or machine learning model. The most used methods to do that including Bag of Words, TF-IDF vectorization and Word2vec.\nExamples of Bag of Words with scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer  vectorizer = CountVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0 0 1 1 0 0 1]\r[0 1 0 0 0 1 0]\r[1 0 0 1 1 1 0]] The default configuration tokenizes the string by extracting words of at least 2 letters. so the word a has been discarded. The vector representation table of words is below.\nV(also) : [1 0 0 0 0 0 0]\rV(am) : [0 1 0 0 0 0 0]\rV(he) : [0 0 1 0 0 0 0]\rV(is) : [0 0 0 1 0 0 0]\rV(she) : [0 0 0 0 1 0 0]\rV(student) : [0 0 0 0 0 1 0]\rV(teacher) : [0 0 0 0 0 0 1]\rV(He is a teacher) = V(He) + V(is) + V(teacher) = [0 0 1 1 0 0 1]\r... TF means term-frequency. $$\\text{TF(t, d)}=\\frac{\\text{number of term t occurs in the document d}}{\\text{number of terms in the document d}}$$\nIDF means inverse document-frequency.\n$$\\text{IDF(t)}=\\log\\frac{\\text{number of documents in the corpus}}{\\text{number of document where the term t occurs}}$$\nTF-IDF means term-frequency times inverse document-frequency.\n$$\\text{TF-IDF(t, d)}=\\text{TF(t, d)} \\times \\text{IDF(t)}$$\nExamples of TF-IDF Verctorization with scikit-learn:\nfrom sklearn.feature_extraction.text import TfidfVectorizer  vectorizer = TfidfVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0. 0. 0.62276601 0.4736296 0. 0. 0.62276601]\r[0. 0.79596054 0. 0. 0. 0.60534851 0. ]\r[0.5628291 0. 0. 0.42804604 0.5628291 0.42804604 0. ]]  Note: The calculational formula of TF-IDF has some changes in the TfidfVectorizer.\n Word2vec is an algorithm that uses a shallow feedforward neural network to learn word embedding from a large corpus of text. There have two methods for producing a distributed representation of words.\n Continuous Bag-of-Words which was usually called CBOW predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). Continuous Skip-gram uses the current word to predict the surrounding window of context words.  See also: https://www.tensorflow.org/tutorials/text/word2vec\nExamples of Word2vec with gensim:\nimport gensim.downloader from gensim.models import Word2Vec  word2vec = gensim.downloader.load(\u0026#39;word2vec-google-news-300\u0026#39;) print(word2vec.most_similar(\u0026#39;car\u0026#39;)) print(word2vec.word_vec(\u0026#39;car\u0026#39;))  output:\n [(\u0026#39;vehicle\u0026#39;, 0.7821096181869507),\r(\u0026#39;cars\u0026#39;, 0.7423830032348633),\r(\u0026#39;SUV\u0026#39;, 0.7160962820053101),\r(\u0026#39;minivan\u0026#39;, 0.6907036304473877),\r(\u0026#39;truck\u0026#39;, 0.6735789775848389),\r(\u0026#39;Car\u0026#39;, 0.6677608489990234),\r(\u0026#39;Ford_Focus\u0026#39;, 0.667320191860199),\r(\u0026#39;Honda_Civic\u0026#39;, 0.662684977054596),\r(\u0026#39;Jeep\u0026#39;, 0.6511331796646118),\r(\u0026#39;pickup_truck\u0026#39;, 0.64414381980896)]\r[ 0.13085938 0.00842285 0.03344727 -0.05883789 0.04003906 -0.14257812\r0.04931641 -0.16894531 0.20898438 0.11962891 0.18066406 -0.25\r...\r0.04248047 0.12792969 -0.27539062 0.28515625 -0.04736328 0.06494141\r-0.11230469 -0.02575684 -0.04125977 0.22851562 -0.14941406 -0.15039062] See also: https://radimrehurek.com/gensim/models/word2vec.html\nLSTM network LSTM is an artificial recurrent neural network architecture. Its full name is long short-term memory, it is well-suited to classifying, processing and making predictions based on time series data.\nA common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\nSee also: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe following is an explanation of some parameters that were being used in the model.\nThe metrics parameter is used to evaluate the model during training and testing. It uses F1-score as the final evaluation metric in this task,. $$\\text{F1-score}=2\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}$$ Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.\nRecall is the number of true positive results divided by the number of all samples that should have been identified as positive.\nSee also: https://en.wikipedia.org/wiki/F-score\nThe bias_initializer parameter can influence the initial output of the layer. E.g. You have an imbalanced dataset of the ratio of positives to negatives is 1: 9. The model will predict 50% positive and 50% negative at initialization (PS: before training) if you don\u0026rsquo;t set the bias_initializer parameter. But if you have set the parameter properly, it will predict 10% positive and 90% negative at initialization, it is more reasonable. In addition, setting it correctly will speed up convergence.\nThe calculation of the bias_initializer parameter depends on what activation function the layer used. The correct bias to set can be derived from below if the activation function is sigmoid. $$p_0=\\frac{\\text{positive}}{\\text{positive}+\\text{negative}}=\\frac{1}{1+\\exp(-b_0)}$$ then $$b_0=-\\log(\\frac{1}{p_0}-1)=\\log(\\frac{\\text{positive}}{\\text{negative}})$$ See also: Classification on imbalanced data\nYou can refer to this question if the activation function is softmax.\nThe class weight parameter is also important for a model during training if the dataset is imbalanced. Giving a heavier weight to an under-represented class will cause the model to pay more attention to it.\nFinal Thoughts When I finish the article, I learned more than when I begin to write. I found some approaches more convenient to preprocess text, such as TextVectorization in tensorflow. I hava a more in-depth understand of Word2Vec. I just knew it was a distributed representation of words. Now I know there have two methods to implement it, CBOW and skip-gram. The skip-gram model uses the technique called negative sampling to maximize the probability of predicting context words when given the target word.\nReferences  Stemming and Lemmatization Basic NLP with NLTK NLP | How tokenizing text, sentence, words works Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text Word2Vec - Tensorflow Long short-term memory - Wikipedia  ","permalink":"https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/","summary":"The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.","title":"[Kaggle Summary] Quora Insincere Questions Classification"}]