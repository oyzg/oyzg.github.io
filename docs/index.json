[{"content":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.\nFree Courses  Machine Learning Crash Course It\u0026rsquo;s a Google\u0026rsquo;s fast-paced, practical introduction to Machine Learning. NLP series Course In Chinese Its content includes text preprocessing, word embedding, RNN/LSTM, seq2seq model, and attention/self-attention mechanism. Audio Signal Processing for Machine Learning An awesome course about audio processing for machine learning.  Discussions and Questions  Kaggle is the world\u0026rsquo;s largest data science community with powerful tools and resources. Learn AI Together is a group chat about artificial intelligence in Discord. Machine Learning , Learn Machine Learning are communities about machine learning in Reddit.  Libraries and Tools  Scikit-learn is a free machine learning library. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN. NLTK is a library for building Python programs to work with human language data. It has many useful tools like text tokenization, word lemmatization, and so on. Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.  Online Labs  Embedding Projector is a tool for visualizing high-dimensional data. The website has an example that shows how word2vec looks like.  Datasets  Tatoeba is a large database of sentences and translations. Its content is ever-growing and results from the voluntary contributions of thousands of members.  Others  Papers With Code is a navigation of Machine Learning papers, Code, Dataset, and their leaderboards.  ","permalink":"https://oyzg.github.io/archives/some-useful-website-or-resources-for-learning-ai/","summary":"I gathered some valuable resources or websites from the Internet, including courses, websites, and anything about Machine Learning or Data Science that I had read or used.","title":"Some Valuable Websites or Resources for Learning AI"},{"content":"计算机网络分层 七层网络模型 ","permalink":"https://oyzg.github.io/archives/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/","summary":"计算机网络面试相关内容.","title":"计算机网络"},{"content":"","permalink":"https://oyzg.github.io/archives/all-about-fourier-transform/","summary":"This is a detailed tutorial about Fourier Transform and related topics. It intelligibly explained what Fourier Transform is and how it works.","title":"All about Fourier Transform"},{"content":"","permalink":"https://oyzg.github.io/archives/keras-multiple-outputs-and-multiple-losses/","summary":"A multiple outputs model has several fully connected layers for output. Each layer is responsible for performing a specific task and has a respective loss function and activation function.","title":"Keras: Multiple outputs and multiple losses"},{"content":"","permalink":"https://oyzg.github.io/archives/multi-label-classification-with-keras/","summary":"The post introduces how to do multi-label classification with Keras and implements a simple model with custom SmallerVGGNet can predict both color and clothing type of input image.","title":"Multi-label classification with Keras"},{"content":"","permalink":"https://oyzg.github.io/archives/exploring-spatiotemporal-features-for-activity-classifications-in-films/","summary":"The post introduces several experiments about activity classification based on three main architectures: 3D CNN, ConvLSTM2D, and a pipeline of pre-trained CNN-LSTM.","title":"Exploring Spatiotemporal Features for Activity Classifications in Films"},{"content":"","permalink":"https://oyzg.github.io/archives/video-classification-keras-convlstm/","summary":"This article will explain the Deep Learning based solution of the Video Classification task in Keras using ConvLSTM layers.","title":"Video Classification in Keras using ConvLSTM"},{"content":"","permalink":"https://oyzg.github.io/archives/introduction-to-video-classification-and-human-activity-recognition/","summary":"In this post, we will learn about Video Classification. We will go over a number of approaches to make a video classifier for Human Activity Recognition. Basically, you will learn video classification and human activity recognition.","title":"Introduction to Video Classification and Human Activity Recognition"},{"content":"","permalink":"https://oyzg.github.io/archives/pca-using-python-scikit-learn/","summary":"The post introduced the principal component analysis through the two most commonly used applications, speeding up a Machine Learning algorithm and data visualization.","title":"PCA using Python (Scikit-Learn)"},{"content":"","permalink":"https://oyzg.github.io/archives/map-mean-average-precision-for-object-detection/","summary":"AP (Average precision) is a popular metric in measuring the accuracy of object detectors like Faster R-CNN, SSD, etc. Average precision computes the average precision value for recall value over 0 to 1.","title":"mAP (mean Average Precision) for Object Detection"},{"content":"","permalink":"https://oyzg.github.io/archives/understanding-pytorch-with-an-example-a-step-by-step-tutorial/","summary":"In this post, The author will guide you through the main reasons why PyTorch makes it much easier and more intuitive to build a Deep Learning model in Python — autograd, dynamic computation graph, model classes and more - and He will also show you how to avoid some common pitfalls and errors along the way.","title":"Understanding PyTorch With an Example: A step by step Tutorial"},{"content":"","permalink":"https://oyzg.github.io/archives/implementing-a-lstm-from-scratch-with-numpy/","summary":"In this post, The author implement a simple character-level LSTM using Numpy. It is trained in batches with the Adam optimiser and learns basic words after just a few training iterations.","title":"Implementing a LSTM From Scratch With Numpy"},{"content":"","permalink":"https://oyzg.github.io/archives/deriving-the-backpropagation-equations-for-a-lstm/","summary":"In this post, The author derive the backpropagation equations for a LSTM cell in vectorised form.","title":"Deriving the Backpropagation Equations for a LSTM"},{"content":"","permalink":"https://oyzg.github.io/archives/the-illustrated-transformer/","summary":"A pretty detailed illustration about the transformers: I recommend it for you if you are looking for an explained article about the paper Attention is all you need, or you want to learn the attention mechanism.","title":"The Illustrated Transformer"},{"content":"","permalink":"https://oyzg.github.io/archives/attention-attention/","summary":"Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.","title":"Attention? Attention!"},{"content":"","permalink":"https://oyzg.github.io/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/","summary":"Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained.","title":"A Simple Overview of RNN, LSTM and Attention Mechanism"},{"content":"","permalink":"https://oyzg.github.io/archives/understanding-gru-networks/","summary":"In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network.","title":"Understanding GRU Networks"},{"content":"","permalink":"https://oyzg.github.io/archives/understanding-input-and-output-shapes-in-lstm/","summary":"Even if we understand LSTMs theoretically, still many of us are confused about its input and output shapes while fitting the data to the network. This guide will help you understand the Input and Output shapes of the LSTM.","title":"Understanding Input and Output Shapes in LSTM"},{"content":"","permalink":"https://oyzg.github.io/archives/understanding-the-number-of-parameter-used-in-lstm-network/","summary":"It explained clearly how to calculate the number of trainable parameters used in the LSTM network.","title":"Understanding the Number of Parameter Used in LSTM Network"},{"content":"","permalink":"https://oyzg.github.io/archives/an-introduction-of-word2vec-and-simple-implementation-with-tensorflow/","summary":"Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.","title":"An Introduction of Word2Vec and implementation using Tensorflow"},{"content":"Introduction This article is a summary of the notebook about Quora insincere questions classification in Kaggle.\nThe link of kaggle notebook is below.\n https://www.kaggle.com/ouyangwenguang/quora-insincere-questions-classification\n The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.\n Important: The complete code of examples below can be found in Github or Colab.\n Key Points Text Preprocessing Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\nExamples of tokenization with NLTK:\nfrom nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize  corpus = \u0026#39;\u0026#39;\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;\u0026#39;\u0026#39;  print(sent_tokenize(corpus)) print(word_tokenize(corpus))  output:\n [\u0026#39;Tokenization is the process of tokenizing or splitting a string, text into a list of tokens.\u0026#39;, \u0026#39;One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\u0026#39;]\r[\u0026#39;Tokenization\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;the\u0026#39;, \u0026#39;process\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokenizing\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;splitting\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;text\u0026#39;, \u0026#39;into\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;tokens\u0026#39;, \u0026#39;.\u0026#39;, \u0026#39;One\u0026#39;, \u0026#39;can\u0026#39;, \u0026#39;think\u0026#39;, \u0026#39;of\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;parts\u0026#39;, \u0026#39;like\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;word\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;token\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;paragraph\u0026#39;, \u0026#39;.\u0026#39;] See also: https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\nLemmatization is a normalization technique in the field of natural language processing. There have a related technique called stemming. In some languages, the word has difference forms in difference contexts. The goal of both stemming and lemmatization is to reduce various forms to a common base form.\nExamples of lemmatization with NLTK:\nfrom nltk.stem import WordNetLemmatizer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] lemmatizer = WordNetLemmatizer()  print([lemmatizer.lemmatize(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;gone\u0026#39;, \u0026#39;better\u0026#39;] The result confused me. Why the output of gone and better isn\u0026rsquo;t the base form of them? Because we don\u0026rsquo;t provide the second parameter parts-of-speech to lemmatize. The parameter default value is n means a noun.\n Lemmatization needs the context to know what the parts-of-speech of the word is, a noun, verb or adjective so that can work well.\n Usage:\ndef lemmatize_sent(text):  pos_dict = {\u0026#39;NN\u0026#39;:\u0026#39;n\u0026#39;, \u0026#39;JJ\u0026#39;:\u0026#39;a\u0026#39;, \u0026#39;VB\u0026#39;:\u0026#39;v\u0026#39;, \u0026#39;RB\u0026#39;:\u0026#39;r\u0026#39;}  word_list = []  for word, tag in pos_tag(word_tokenize(text)):  pos = pos_dict[tag[0:2]] if tag[0:2] in pos_dict else \u0026#39;n\u0026#39;  word_list.append(lemmatizer.lemmatize(word, pos=pos))  return word_list  sentence = \u0026#39;He is walking to school\u0026#39; lemmatization_words = [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)] print(\u0026#39;lemmatize word by word: \u0026#39;, lemmatization_words) print(\u0026#39;lemmatize with context: \u0026#39;, lemmatize_sent(sentence))  output:\n lemmatize word by word: [\u0026#39;He\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;walking\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;]\rlemmatize with context: [\u0026#39;He\u0026#39;, \u0026#39;be\u0026#39;, \u0026#39;walk\u0026#39;, \u0026#39;to\u0026#39;, \u0026#39;school\u0026#39;] Stemming shortens words with some rules and can be used without context, so it\u0026rsquo;s more efficient than Lemmatization.\nExamples of Stemming with NLTK:\nfrom nltk.stem import PorterStemmer  corpus = [\u0026#39;rocks\u0026#39;, \u0026#39;going\u0026#39;, \u0026#39;history\u0026#39;] stemmer = PorterStemmer() print([stemmer.stem(w) for w in corpus])  output:\n [\u0026#39;rock\u0026#39;, \u0026#39;go\u0026#39;, \u0026#39;histori\u0026#39;] As you can see, the stemming outputs may not be actual words.\n Time-consuming Test:\n from nltk.corpus import gutenberg  @timing def stemming(text):  [stemmer.stem(w) for w in word_tokenize(sentence)]  @timing def lemmatize(text):  lemmatize_sent(text)  @timing def lemmatize_without_context(text):  [lemmatizer.lemmatize(w) for w in word_tokenize(sentence)]  book = gutenberg.raw(\u0026#34;austen-sense.txt\u0026#34;)  stemming(book) lemmatize(book) lemmatize_without_context(book)  output:\n stemming : 0.22 ms\rlemmatize : 5980.39 ms\rlemmatize_without_context : 0.17 ms See also: https://www.nltk.org/api/nltk.stem.html#module-nltk.stem\nStop words usually refers to the most common words in a language, such as the, is, at, which and a in English. We remove the stop words before processing the text data because these words usually are no enough valuable.\nExamples of Stop words with NLTK:\nfrom nltk.corpus import stopwords  corpus = [\u0026#39;I\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;boy\u0026#39;] print([w for w in corpus if w not in set(stopwords.words(\u0026#39;english\u0026#39;))])  output:\n [\u0026#39;I\u0026#39;, \u0026#39;boy\u0026#39;] See also: https://www.nltk.org/book/ch02.html#stopwords_index_term\nText Vectorization is the process of converting text into a numerical vector that can be fed into a neural network or machine learning model. The most used methods to do that including Bag of Words, TF-IDF vectorization and Word2vec.\nExamples of Bag of Words with scikit-learn:\nfrom sklearn.feature_extraction.text import CountVectorizer  vectorizer = CountVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0 0 1 1 0 0 1]\r[0 1 0 0 0 1 0]\r[1 0 0 1 1 1 0]] The default configuration tokenizes the string by extracting words of at least 2 letters. so the word a has been discarded. The vector representation table of words is below.\nV(also) : [1 0 0 0 0 0 0]\rV(am) : [0 1 0 0 0 0 0]\rV(he) : [0 0 1 0 0 0 0]\rV(is) : [0 0 0 1 0 0 0]\rV(she) : [0 0 0 0 1 0 0]\rV(student) : [0 0 0 0 0 1 0]\rV(teacher) : [0 0 0 0 0 0 1]\rV(He is a teacher) = V(He) + V(is) + V(teacher) = [0 0 1 1 0 0 1]\r... TF means term-frequency. $$\\text{TF(t, d)}=\\frac{\\text{number of term t occurs in the document d}}{\\text{number of terms in the document d}}$$\nIDF means inverse document-frequency.\n$$\\text{IDF(t)}=\\log\\frac{\\text{number of documents in the corpus}}{\\text{number of document where the term t occurs}}$$\nTF-IDF means term-frequency times inverse document-frequency.\n$$\\text{TF-IDF(t, d)}=\\text{TF(t, d)} \\times \\text{IDF(t)}$$\nExamples of TF-IDF Verctorization with scikit-learn:\nfrom sklearn.feature_extraction.text import TfidfVectorizer  vectorizer = TfidfVectorizer() corpus = [  \u0026#39;He is a teacher\u0026#39;,  \u0026#39;I am student\u0026#39;,  \u0026#39;She is also a student\u0026#39;, ] X = vectorizer.fit_transform(corpus)  print(vectorizer.get_feature_names()) print(X.toarray())  output:\n [\u0026#39;also\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;he\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;she\u0026#39;, \u0026#39;student\u0026#39;, \u0026#39;teacher\u0026#39;]\r[[0. 0. 0.62276601 0.4736296 0. 0. 0.62276601]\r[0. 0.79596054 0. 0. 0. 0.60534851 0. ]\r[0.5628291 0. 0. 0.42804604 0.5628291 0.42804604 0. ]]  Note: The calculational formula of TF-IDF has some changes in the TfidfVectorizer.\n Word2vec is an algorithm that uses a shallow feedforward neural network to learn word embedding from a large corpus of text. There have two methods for producing a distributed representation of words.\n Continuous Bag-of-Words which was usually called CBOW predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). Continuous Skip-gram uses the current word to predict the surrounding window of context words.  See also: https://www.tensorflow.org/tutorials/text/word2vec\nExamples of Word2vec with gensim:\nimport gensim.downloader from gensim.models import Word2Vec  word2vec = gensim.downloader.load(\u0026#39;word2vec-google-news-300\u0026#39;) print(word2vec.most_similar(\u0026#39;car\u0026#39;)) print(word2vec.word_vec(\u0026#39;car\u0026#39;))  output:\n [(\u0026#39;vehicle\u0026#39;, 0.7821096181869507),\r(\u0026#39;cars\u0026#39;, 0.7423830032348633),\r(\u0026#39;SUV\u0026#39;, 0.7160962820053101),\r(\u0026#39;minivan\u0026#39;, 0.6907036304473877),\r(\u0026#39;truck\u0026#39;, 0.6735789775848389),\r(\u0026#39;Car\u0026#39;, 0.6677608489990234),\r(\u0026#39;Ford_Focus\u0026#39;, 0.667320191860199),\r(\u0026#39;Honda_Civic\u0026#39;, 0.662684977054596),\r(\u0026#39;Jeep\u0026#39;, 0.6511331796646118),\r(\u0026#39;pickup_truck\u0026#39;, 0.64414381980896)]\r[ 0.13085938 0.00842285 0.03344727 -0.05883789 0.04003906 -0.14257812\r0.04931641 -0.16894531 0.20898438 0.11962891 0.18066406 -0.25\r...\r0.04248047 0.12792969 -0.27539062 0.28515625 -0.04736328 0.06494141\r-0.11230469 -0.02575684 -0.04125977 0.22851562 -0.14941406 -0.15039062] See also: https://radimrehurek.com/gensim/models/word2vec.html\nLSTM network LSTM is an artificial recurrent neural network architecture. Its full name is long short-term memory, it is well-suited to classifying, processing and making predictions based on time series data.\nA common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\nSee also: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\nThe following is an explanation of some parameters that were being used in the model.\nThe metrics parameter is used to evaluate the model during training and testing. It uses F1-score as the final evaluation metric in this task,. $$\\text{F1-score}=2\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}$$ Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly.\nRecall is the number of true positive results divided by the number of all samples that should have been identified as positive.\nSee also: https://en.wikipedia.org/wiki/F-score\nThe bias_initializer parameter can influence the initial output of the layer. E.g. You have an imbalanced dataset of the ratio of positives to negatives is 1: 9. The model will predict 50% positive and 50% negative at initialization (PS: before training) if you don\u0026rsquo;t set the bias_initializer parameter. But if you have set the parameter properly, it will predict 10% positive and 90% negative at initialization, it is more reasonable. In addition, setting it correctly will speed up convergence.\nThe calculation of the bias_initializer parameter depends on what activation function the layer used. The correct bias to set can be derived from below if the activation function is sigmoid. $$p_0=\\frac{\\text{positive}}{\\text{positive}+\\text{negative}}=\\frac{1}{1+\\exp(-b_0)}$$ then $$b_0=-\\log(\\frac{1}{p_0}-1)=\\log(\\frac{\\text{positive}}{\\text{negative}})$$ See also: Classification on imbalanced data\nYou can refer to this question if the activation function is softmax.\nThe class weight parameter is also important for a model during training if the dataset is imbalanced. Giving a heavier weight to an under-represented class will cause the model to pay more attention to it.\nFinal Thoughts When I finish the article, I learned more than when I begin to write. I found some approaches more convenient to preprocess text, such as TextVectorization in tensorflow. I hava a more in-depth understand of Word2Vec. I just knew it was a distributed representation of words. Now I know there have two methods to implement it, CBOW and skip-gram. The skip-gram model uses the technique called negative sampling to maximize the probability of predicting context words when given the target word.\nReferences  Stemming and Lemmatization Basic NLP with NLTK NLP | How tokenizing text, sentence, words works Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text Word2Vec - Tensorflow Long short-term memory - Wikipedia  ","permalink":"https://oyzg.github.io/archives/kaggle-summary-quora-insincere-questions-classification/","summary":"The summary of the notebook about Quora insincere questions classification in Kaggle. The main idea of the notebook is how to solve an imbalanced text classification problem with LSTM networks and Word2vec.","title":"[Kaggle Summary] Quora Insincere Questions Classification"},{"content":"","permalink":"https://oyzg.github.io/archives/an-example-of-classification-on-imbalanced-data-with-tensorflow/","summary":"This tutorial demonstrates how to classify a highly imbalanced dataset in which the number of examples in one class greatly outnumbers the examples in another. You will work with the Credit Card Fraud Detection dataset hosted on Kaggle. The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total. You will use Keras to define the model and class weights to help the model learn from the imbalanced data.","title":"An Example of Classification on Imbalanced Data with Tensorflow"},{"content":"","permalink":"https://oyzg.github.io/archives/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients/","summary":"LSTMs solve the problem using a unique additive gradient structure that includes direct access to the forget gate’s activations, enabling the network to encourage desired behaviour from the error gradient using frequent gates update on every time step of the learning process.","title":"How do LSTM Networks Solve the Problem of Vanishing Gradients"},{"content":"","permalink":"https://oyzg.github.io/archives/understanding-lstm-networks/","summary":"Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter \u0026amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.","title":"Understanding LSTM Networks"},{"content":"","permalink":"https://oyzg.github.io/archives/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation/","summary":"In this post, we’ll start with the intuition behind LSTM ’s and GRU’s. Then I’ll explain the internal mechanisms that allow LSTM’s and GRU’s to perform so well. If you want to understand what’s happening under the hood for these two networks, then this post is for you.","title":"Illustrated Guide to LSTM's and GRU's: A step by step Explanation"},{"content":"","permalink":"https://oyzg.github.io/archives/what-is-a-transformer/","summary":"An introduction to transformers and sequence-to-sequence learning for machine learning","title":"What is a Transformer?"},{"content":"","permalink":"https://oyzg.github.io/archives/an-explanation-of-bleu-score-by-andrew-ng/","summary":"BLEU: A method for automatic evaluation of machine translation","title":"An Explanation of Bleu Score by Andrew Ng"},{"content":"","permalink":"https://oyzg.github.io/archives/an-explanation-about-three-of-the-most-important-metrics-we-use-accuracy-precision-and-recall/","summary":"More specifically, this article shows what happens when we focus on the wrong metric using an imbalanced classification problem.","title":"An Explanation about Three of the Most Important Metrics We Use: Accuracy Precision and Recall"},{"content":"","permalink":"https://oyzg.github.io/archives/analyzing-handwritten-digits-model-with-tensorflow-line-by-line/","summary":"Let\u0026rsquo;s do a line-by-line analysis of this deep learning model and truly understand what\u0026rsquo;s going on. This model identifies handwritten digits. It\u0026rsquo;s one of the classic examples of machine learning applied to computer vision","title":"Analyzing Handwritten Digit Model with Tensorflow Line by Line"}]