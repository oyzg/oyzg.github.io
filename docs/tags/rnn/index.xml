<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RNN on My Favorite</title><link>https://wglog.net/tags/rnn/</link><description>Recent content in RNN on My Favorite</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 22 Mar 2021 14:57:23 +0800</lastBuildDate><atom:link href="https://wglog.net/tags/rnn/index.xml" rel="self" type="application/rss+xml"/><item><title>A Simple Overview of RNN, LSTM and Attention Mechanism</title><link>https://wglog.net/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</link><pubDate>Mon, 22 Mar 2021 14:57:23 +0800</pubDate><guid>https://wglog.net/archives/a-simple-overview-of-rnn-lstm-and-attention-mechanism/</guid><description>Recurrent Neural Networks, Long Short Term Memory and the famous Attention based approach explained.</description></item><item><title>Understanding GRU Networks</title><link>https://wglog.net/archives/understanding-gru-networks/</link><pubDate>Thu, 18 Mar 2021 17:24:49 +0800</pubDate><guid>https://wglog.net/archives/understanding-gru-networks/</guid><description>In this article, I will try to give a fairly simple and understandable explanation of one really fascinating type of neural network. Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network.</description></item><item><title>Understanding LSTM Networks</title><link>https://wglog.net/archives/understanding-lstm-networks/</link><pubDate>Mon, 08 Mar 2021 13:02:42 +0800</pubDate><guid>https://wglog.net/archives/understanding-lstm-networks/</guid><description>Long Short Term Memory networks – usually just called LSTMs – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp;amp; Schmidhuber (1997), and were refined and popularized by many people in following work. They work tremendously well on a large variety of problems, and are now widely used.</description></item></channel></rss>